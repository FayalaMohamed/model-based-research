{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X249tZbHLFed"
      },
      "outputs": [],
      "source": [
        "episodes_dataset = tf.data.TFRecordDataset(\n",
        "    \"gs://rl_unplugged/atari_episodes_ordered/Pong/run_1-00000-of-00050\",\n",
        "    compression_type=\"GZIP\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34QJ1V4eLOLT"
      },
      "outputs": [],
      "source": [
        "raw_example = next(iter(episodes_dataset))\n",
        "\n",
        "example = tf.train.Example()\n",
        "example.ParseFromString(raw_example.numpy())\n",
        "\n",
        "print(\"Features in the TFRecord example:\")\n",
        "for key in example.features.feature.keys():\n",
        "    feature = example.features.feature[key]\n",
        "    # Print the type of data stored in this feature\n",
        "    if feature.bytes_list.value:\n",
        "        dtype = \"bytes\"\n",
        "    elif feature.int64_list.value:\n",
        "        dtype = \"int64\"\n",
        "    elif feature.float_list.value:\n",
        "        dtype = \"float\"\n",
        "    else:\n",
        "        dtype = \"unknown\"\n",
        "    print(f\"{key}: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPG91OhkLQR-"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_description = {\n",
        "    \"observations\": tf.io.VarLenFeature(tf.string),\n",
        "    \"actions\": tf.io.VarLenFeature(tf.int64),\n",
        "    \"clipped_rewards\": tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n",
        "def _parse_function(example_proto):\n",
        "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    obs = tf.sparse.to_dense(parsed[\"observations\"])\n",
        "    actions = tf.sparse.to_dense(parsed[\"actions\"])\n",
        "    rewards = tf.sparse.to_dense(parsed[\"clipped_rewards\"])\n",
        "\n",
        "    img = tf.io.decode_image(obs[0], channels=3)\n",
        "    return img, actions[0], rewards[0]\n",
        "\n",
        "dataset_parsed = episodes_dataset.map(_parse_function)\n",
        "\n",
        "# Plot first 10 frames with actions and rewards\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i, (frame, action, reward) in enumerate(dataset_parsed.take(10)):\n",
        "    plt.subplot(2, 10, i+1)\n",
        "    plt.imshow(frame.numpy())       # now .numpy() is fine outside map\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"A:{action.numpy()} R:{reward.numpy():.1f}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzqU1lwmLTXC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Coluding/world-models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_GJPrq3LWDB"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/world-models')\n",
        "from models.rssm import RSSM\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlEzmMlzLYSY"
      },
      "outputs": [],
      "source": [
        "feature_description = {\n",
        "  \"observations\": tf.io.VarLenFeature(tf.string),\n",
        "  \"actions\": tf.io.VarLenFeature(tf.int64),\n",
        "  \"clipped_rewards\": tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n",
        "def parse_sequence(example_proto):\n",
        "  parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
        "  obs = tf.sparse.to_dense(parsed[\"observations\"])\n",
        "  actions = tf.sparse.to_dense(parsed[\"actions\"])\n",
        "  rewards = tf.sparse.to_dense(parsed[\"clipped_rewards\"])\n",
        "  imgs = tf.map_fn(lambda x: tf.io.decode_image(x, channels=3), obs, dtype=tf.uint8)\n",
        "  #imgs.set_shape([None, 84, 84, 3])\n",
        "  #imgs = tf.image.resize(imgs, [64, 64])\n",
        "  imgs = tf.cast(imgs, tf.float32) / 255.0\n",
        "  actions = tf.cast(actions, tf.int64)\n",
        "  rewards = tf.cast(rewards, tf.float32)\n",
        "  return imgs, actions, rewards\n",
        "\n",
        "sequence_length = 50\n",
        "batch_size = 16\n",
        "\n",
        "episodes_dataset = tf.data.TFRecordDataset(\n",
        "    \"gs://rl_unplugged/atari_episodes_ordered/Pong/run_1-00000-of-00050\",\n",
        "    compression_type=\"GZIP\"\n",
        ")\n",
        "dataset = episodes_dataset.map(parse_sequence)\n",
        "dataset = dataset.filter(lambda imgs, actions, rewards: tf.shape(imgs)[0] >= sequence_length)\n",
        "dataset = dataset.map(lambda imgs, actions, rewards: (\n",
        "    imgs[:sequence_length], actions[:sequence_length], rewards[:sequence_length]\n",
        "))\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCfR4ulPLZw6"
      },
      "outputs": [],
      "source": [
        "\n",
        "example = next(iter(episodes_dataset.take(1)))\n",
        "imgs, actions, rewards = parse_sequence(example)\n",
        "\n",
        "print(imgs.shape)\n",
        "print(actions.shape)\n",
        "print(rewards.shape)\n",
        "\n",
        "last_ten_imgs = imgs[-10:]\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "  ax.imshow(last_ten_imgs[i], vmin=0, vmax=1)\n",
        "  ax.set_title(f\"Image {i+1}\")\n",
        "  ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCFkgWnDLbmM"
      },
      "outputs": [],
      "source": [
        "for imgs, actions, rewards in dataset.take(1):\n",
        "  print(\"imgs:\", imgs.shape)\n",
        "  print(\"actions:\", actions.shape)\n",
        "  print(\"rewards:\", rewards.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19boZkHPLdvt"
      },
      "outputs": [],
      "source": [
        "for imgs, actions, rewards in dataset.take(1):\n",
        "  seq_imgs = imgs[0]\n",
        "  seq_actions = actions[0]\n",
        "  seq_rewards = rewards[0]\n",
        "\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  for i in range(10):\n",
        "    plt.subplot(2, 10, i + 1)\n",
        "    plt.imshow(seq_imgs[i].numpy())\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"A:{seq_actions[i].numpy()} R:{seq_rewards[i].numpy():.1f}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIhUAc7tLfQW"
      },
      "outputs": [],
      "source": [
        "def parse_sequence_for_inspection(example_proto):\n",
        "    parsed = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    obs = tf.sparse.to_dense(parsed[\"observations\"])\n",
        "    imgs = tf.map_fn(lambda x: tf.io.decode_image(x, channels=3), obs, dtype=tf.uint8)\n",
        "    print(\"Original image shape (before resizing):\", imgs.shape)\n",
        "    return imgs\n",
        "\n",
        "example = next(iter(episodes_dataset.take(1)))\n",
        "imgs = parse_sequence_for_inspection(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVZ_A7mwLig8"
      },
      "outputs": [],
      "source": [
        "class AtariSequenceDataset(Dataset):\n",
        "  def __init__(self, tf_dataset, num_batches):\n",
        "    self.data = []\n",
        "    for i, (imgs, actions, rewards) in enumerate(tf_dataset):\n",
        "      if i >= num_batches:\n",
        "        break\n",
        "      self.data.append((\n",
        "        imgs.numpy(), actions.numpy(), rewards.numpy()\n",
        "      ))\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):\n",
        "    imgs, actions, rewards = self.data[idx]\n",
        "    imgs = torch.tensor(imgs, dtype=torch.float32)  # [seq, 64, 64, 3]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)  # [seq]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # [seq]\n",
        "    return imgs, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laOKF1lzLk5N"
      },
      "outputs": [],
      "source": [
        "num_batches = 100\n",
        "atari_dataset = AtariSequenceDataset(dataset, num_batches)\n",
        "dataloader = DataLoader(atari_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lapqUgMLnLY"
      },
      "outputs": [],
      "source": [
        "from models.models import EncoderCNN, DecoderCNN, RewardModel\n",
        "from models.rssm import RSSM\n",
        "from models.dynamics import DynamicsModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "action_dim = 6\n",
        "embedding_dim = 1024\n",
        "hidden_size = 1024\n",
        "state_size = 30\n",
        "embedding_size = 16384\n",
        "\n",
        "encoder = EncoderCNN(in_channels=3, embedding_dim=embedding_size, input_shape=(3, 64, 64)).to(device)\n",
        "decoder = DecoderCNN(hidden_size, state_size, embedding_size, use_bn=True, output_shape=(3, 64, 64)).to(device)\n",
        "reward_model = RewardModel(hidden_size, state_size).to(device)\n",
        "dynamics_model = DynamicsModel(hidden_size, state_size, action_dim, embedding_size).to(device)\n",
        "\n",
        "rssm = RSSM(encoder, decoder, reward_model, dynamics_model, hidden_size, state_size, action_dim, embedding_size, device=device)\n",
        "optimizer = torch.optim.Adam(rssm.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXEaIQyULouG"
      },
      "outputs": [],
      "source": [
        "import torch.distributions as dist\n",
        "\n",
        "def compute_loss(prior_states, posterior_states, imgs, decoder, device):\n",
        "  # KL loss + reconstruction loss\n",
        "  kl_loss = torch.mean(torch.sum(\n",
        "    torch.distributions.kl_divergence(\n",
        "      dist.Normal(posterior_states[0], posterior_states[1]),\n",
        "      dist.Normal(prior_states[0], prior_states[1])\n",
        "    ), dim=-1\n",
        "  ))\n",
        "  recon_imgs = decoder(posterior_states[0])\n",
        "  recon_loss = torch.nn.functional.mse_loss(recon_imgs, imgs.to(device))\n",
        "  return kl_loss + recon_loss\n",
        "\n",
        "rssm.train()\n",
        "for epoch in range(10):\n",
        "  for imgs, actions, rewards in dataloader:\n",
        "    imgs = imgs.squeeze(0).to(device)  # [seq, 64, 64, 3]\n",
        "    actions = actions.squeeze(0).to(device)  # [seq]\n",
        "    embedded_obs = rssm.encode(imgs)\n",
        "    hiddens, prior_states, posterior_states, prior_means, prior_logvars, posterior_means, posterior_logvars = \\\n",
        "        rssm.generate_rollout(actions, obs=embedded_obs)\n",
        "    loss = compute_loss((prior_means, prior_logvars), (posterior_means, posterior_logvars), imgs, rssm.decoder, device)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
