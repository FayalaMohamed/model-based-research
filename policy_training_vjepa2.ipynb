{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUSJBJP_Bq4"
      },
      "source": [
        "# V-JEPA2 Policy Training Pipeline\n",
        "\n",
        "This notebook implements policy training using the V-JEPA2 encoder, following the same structure as the RSSM-based policy training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eccwVYDn_Bq4",
        "outputId": "61bbd6b4-d571-4602-ba17-ee9e36f7cb5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import ale_py\n",
        "import imageio\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Experiment Configuration ---\n",
        "import os\n",
        "\n",
        "GAME_ID = os.environ.get(\"ATARI_GAME\", \"PongNoFrameskip-v4\")\n",
        "USE_VJEPA_ENCODER = bool(int(os.environ.get(\"USE_VJEPA_ENCODER\", \"0\")))\n",
        "# Temporal context (number of frames provided to encoder / policy)\n",
        "T_CONTEXT = int(os.environ.get(\"T_CONTEXT\", \"4\"))\n",
        "# Action embedding only applies when using V-JEPA encoder\n",
        "EMBED_ACTIONS = bool(int(os.environ.get(\"VJEPA_EMBED_ACTIONS\", \"1\"))) if USE_VJEPA_ENCODER else False\n",
        "NUM_EPISODES = int(os.environ.get(\"NUM_TRAIN_EPISODES\", \"100\"))\n",
        "EVAL_EPISODES = int(os.environ.get(\"NUM_EVAL_EPISODES\", \"5\"))\n",
        "GAMMA = float(os.environ.get(\"DISCOUNT_GAMMA\", \"0.99\"))\n",
        "LEARNING_RATE = float(os.environ.get(\"POLICY_LR\", \"1e-4\"))\n",
        "\n",
        "print(f\"Game: {GAME_ID}\")\n",
        "print(f\"Use V-JEPA encoder: {USE_VJEPA_ENCODER}\")\n",
        "print(f\"Temporal context (frames): {T_CONTEXT}\")\n",
        "print(f\"Action embedding (V-JEPA only): {EMBED_ACTIONS}\")\n",
        "print(f\"Training episodes: {NUM_EPISODES}\")\n",
        "print(f\"Evaluation episodes: {EVAL_EPISODES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQsbW_VA_Bq5"
      },
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YOfiYxE_Bq5",
        "outputId": "baa5cee3-f425-4451-84e7-142e59dfaf2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment initialized successfully\n"
          ]
        }
      ],
      "source": [
        "from gymnasium import spaces\n",
        "\n",
        "# Create environment\n",
        "env = gym.make(GAME_ID)\n",
        "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "def transform_obs(obs):\n",
        "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "\n",
        "env = gym.wrappers.TransformObservation(\n",
        "    env,\n",
        "    func=transform_obs,\n",
        "    observation_space=new_obs_space,\n",
        ")\n",
        "\n",
        "obs, info = env.reset()\n",
        "assert obs.shape == (3, 84, 84), f\"Expected (3, 84, 84), got {obs.shape}\"\n",
        "print(f\"Environment initialized successfully for {GAME_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDxKSxpr_Bq5"
      },
      "source": [
        "## Load V-JEPA2 Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YElwTBeF_Bq5",
        "outputId": "f926cc6e-c4cf-4206-8e2b-c94d07e7bc26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/vjepa2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_vjepa2_main\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/vjepa2/vitl.pt\" to /root/.cache/torch/hub/checkpoints/vitl.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.78G/4.78G [00:17<00:00, 294MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded V-JEPA 2 (ViT-L) from hub.\n",
            "V-JEPA2 encoder loaded successfully (AC mode: False)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# V-JEPA2 configuration (only used when USE_VJEPA_ENCODER=True)\n",
        "VJEPA_BACKEND = os.environ.get(\"VJEPA_BACKEND\", \"hub\")\n",
        "# Note: V-JEPA2-AC was designed for robot control with explicit states (end-effector, joints)\n",
        "# For Atari (visual-only), regular V-JEPA2 may be more appropriate\n",
        "# Set USE_AC=1 to try AC model (uses visual tokens as states), USE_AC=0 for regular model\n",
        "USE_AC = bool(int(os.environ.get(\"VJEPA_USE_AC\", \"0\")))\n",
        "HF_REPO = os.environ.get(\"VJEPA_HF_REPO\", \"facebook/vjepa2-vitl-fpc64-256\")\n",
        "\n",
        "preprocessor = None\n",
        "vjepa_model = None\n",
        "vjepa_ac_predictor = None\n",
        "\n",
        "if not USE_VJEPA_ENCODER:\n",
        "    print(\"Skipping V-JEPA encoder load (baseline CNN mode).\")\n",
        "else:\n",
        "    if VJEPA_BACKEND == \"hub\":\n",
        "        preprocessor = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_preprocessor')\n",
        "        if USE_AC:\n",
        "            obj = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_ac_vit_giant')\n",
        "            if isinstance(obj, tuple):\n",
        "                vjepa_model, vjepa_ac_predictor = obj[0], obj[1]\n",
        "            else:\n",
        "                vjepa_model = obj\n",
        "                vjepa_ac_predictor = None\n",
        "            print(\"Loaded V-JEPA 2-AC (ViT-g) from hub.\")\n",
        "            if vjepa_ac_predictor is not None:\n",
        "                vjepa_ac_predictor.to(device).eval()\n",
        "                for p in vjepa_ac_predictor.parameters():\n",
        "                    p.requires_grad_(False)\n",
        "                print(\"Action-conditioned predictor loaded and frozen.\")\n",
        "        else:\n",
        "            obj = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_large')\n",
        "            vjepa_model = obj[0] if isinstance(obj, tuple) else obj\n",
        "            print(\"Loaded V-JEPA 2 (ViT-L) from hub.\")\n",
        "        \n",
        "        vjepa_model.to(device).eval()\n",
        "        for p in vjepa_model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    elif VJEPA_BACKEND == \"hf\":\n",
        "        from transformers import AutoVideoProcessor, AutoModel\n",
        "        vjepa_model = AutoModel.from_pretrained(HF_REPO).to(device).eval()\n",
        "        preprocessor = AutoVideoProcessor.from_pretrained(HF_REPO)\n",
        "        for p in vjepa_model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        print(f\"Loaded {HF_REPO} from Hugging Face.\")\n",
        "        if USE_AC:\n",
        "            print(\"Warning: Action-conditioned model not available via HuggingFace. Use hub backend.\")\n",
        "    else:\n",
        "        raise ValueError(\"VJEPA_BACKEND must be 'hub' or 'hf'.\")\n",
        "\n",
        "    print(f\"V-JEPA2 encoder loaded successfully (AC mode: {USE_AC})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkD8h3hN_Bq5"
      },
      "source": [
        "## V-JEPA2 Feature Extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzKxCp9J_Bq5"
      },
      "outputs": [],
      "source": [
        "class VJEPA2FeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts features from a single observation using V-JEPA2 encoder.\n",
        "\n",
        "    V-JEPA2 expects 256x256 images, so we need to:\n",
        "    1. Resize observations to 256x256 (we use 84×84 → 256×256)\n",
        "    2. Create a temporal context (V-JEPA expects sequences)\n",
        "    3. Extract spatial features from center frame tokens\n",
        "\n",
        "    If action-conditioned predictor is available, actions are used to condition features.\n",
        "    \"\"\"\n",
        "    def __init__(self, vjepa_model, preprocessor, device, ac_predictor=None, action_dim=None, embed_actions_in_encoder=False):\n",
        "        super().__init__()\n",
        "        self.vjepa_model = vjepa_model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.device = device\n",
        "        self.ac_predictor = ac_predictor\n",
        "        self.use_ac = ac_predictor is not None\n",
        "        self.action_dim = action_dim\n",
        "        self.embed_actions_in_encoder = embed_actions_in_encoder\n",
        "\n",
        "        # Create action embedding if we want to encode actions into visual tokens\n",
        "        if self.embed_actions_in_encoder and action_dim is not None:\n",
        "            # Action embedding: map action indices to embedding space\n",
        "            # Embedding dimension should match token dimension (typically 1024)\n",
        "            self.action_embedding = nn.Embedding(action_dim, 1024).to(device)\n",
        "            print(f\"Created action embedding layer: {action_dim} actions -> 1024 dim\")\n",
        "        else:\n",
        "            self.action_embedding = None\n",
        "\n",
        "        # Infer output dimensions from the encoder\n",
        "        self._infer_output_dims()\n",
        "\n",
        "    def _infer_output_dims(self):\n",
        "        \"\"\"Probe the encoder to determine output dimensions\"\"\"\n",
        "        # Create dummy input: single frame resized to 256x256, then create temporal context\n",
        "        # Use a simpler approach: create a dummy observation similar to what we'll get from env\n",
        "        dummy_obs = np.random.rand(3, 64, 64).astype(np.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Convert to tensor and resize (matching forward() method)\n",
        "            obs_tensor = torch.tensor(dummy_obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            obs_resized = F.interpolate(\n",
        "                obs_tensor,\n",
        "                size=(256, 256),\n",
        "                mode='nearest'\n",
        "            )  # (1, 3, 256, 256)\n",
        "\n",
        "            # Create temporal context (for inference, just repeat frame since we don't have history)\n",
        "            # In actual usage, obs_history will provide real sequences\n",
        "            context = max(T_CONTEXT, 1)\n",
        "            obs_seq = obs_resized.repeat(context, 1, 1, 1).unsqueeze(0)  # (1, T, C, H, W)\n",
        "            obs_seq = obs_seq.permute(0, 2, 1, 3, 4)  # (1, C, T, H, W)\n",
        "\n",
        "            # Process through preprocessor (matching forward() method)\n",
        "            proc_list = []\n",
        "            for t_idx in range(context):\n",
        "                frame = obs_seq[0, :, t_idx]  # Get (C, H, W) frame\n",
        "                if self.preprocessor is not None:\n",
        "                    # Convert to (H, W, C) numpy array for preprocessor\n",
        "                    frame_np = frame.cpu().permute(1, 2, 0).numpy()  # (H, W, C)\n",
        "                    # Ensure values are in valid range [0, 1] or [0, 255]\n",
        "                    frame_np = np.clip(frame_np, 0, 1)\n",
        "                    out = self.preprocessor([frame_np])\n",
        "                    out = out[0] if isinstance(out, (list, tuple)) else out\n",
        "                    if isinstance(out, torch.Tensor):\n",
        "                        # Ensure output is (C, H, W)\n",
        "                        if out.dim() == 4:  # (C, T, H, W) or (B, C, H, W)\n",
        "                            if out.shape[1] == 3 or out.shape[0] == 3:\n",
        "                                # Likely (C, T, H, W) or (B, C, H, W)\n",
        "                                if out.shape[0] == 3:  # (C, T, H, W)\n",
        "                                    out = out[:, 0]  # Take first temporal slice -> (C, H, W)\n",
        "                                else:  # (B, C, H, W)\n",
        "                                    out = out[0]  # Take first batch -> (C, H, W)\n",
        "                        elif out.dim() == 3:\n",
        "                            # Could be (C, H, W) or (H, W, C)\n",
        "                            if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                        proc_list.append(out)\n",
        "                    else:\n",
        "                        # Convert numpy to tensor if needed\n",
        "                        if isinstance(out, np.ndarray):\n",
        "                            out = torch.from_numpy(out)\n",
        "                            # Ensure (C, H, W) format\n",
        "                            if out.dim() == 3:\n",
        "                                if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                    out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                        proc_list.append(out)\n",
        "                else:\n",
        "                    proc_list.append(frame)\n",
        "\n",
        "            # Stack processed frames: (T, C, H, W) -> (1, C, T, H, W)\n",
        "            if len(proc_list) > 0:\n",
        "                # Verify all frames have shape (C=3, H, W)\n",
        "                for i, frame in enumerate(proc_list):\n",
        "                    if frame.dim() != 3 or frame.shape[0] != 3:\n",
        "                        raise RuntimeError(f\"Frame {i} has unexpected shape: {frame.shape}, expected (3, H, W)\")\n",
        "\n",
        "                # Each item in proc_list is (C, H, W), stack to (T, C, H, W)\n",
        "                stacked = torch.stack(proc_list, dim=0)  # (T, C, H, W)\n",
        "                # Rearrange to (1, C, T, H, W)\n",
        "                proc = stacked.permute(1, 0, 2, 3).unsqueeze(0).contiguous()  # (1, C, T, H, W)\n",
        "                proc = proc.to(self.device)  # (1, C, T, 256, 256)\n",
        "\n",
        "                # Encode\n",
        "                out = self.vjepa_model(proc)\n",
        "\n",
        "                # Extract token features\n",
        "                if isinstance(out, (list, tuple)) and len(out) > 0:\n",
        "                    tokens = out[0]\n",
        "                elif isinstance(out, dict):\n",
        "                    tokens = out.get('x', list(out.values())[0])\n",
        "                else:\n",
        "                    tokens = out\n",
        "\n",
        "                # Get center temporal slice and spatial dimension\n",
        "                if tokens.dim() == 4:  # (B, T', N, D)\n",
        "                    b, T, N, D = tokens.shape\n",
        "                    tokens = tokens[:, T // 2]  # Center frame\n",
        "                elif tokens.dim() == 3:  # (B, N, D)\n",
        "                    b, N, D = tokens.shape\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Unexpected V-JEPA output shape: {tokens.shape}\")\n",
        "\n",
        "                # Compute spatial grid\n",
        "                Ht = Wt = int(np.sqrt(N))\n",
        "                self.token_dim = D\n",
        "                self.spatial_tokens = Ht * Wt\n",
        "                self.flat_feature_dim = D  # Will pool spatially for policy\n",
        "\n",
        "                print(f\"V-JEPA2 feature dims: token_dim={D}, spatial_grid={Ht}x{Wt}\")\n",
        "            else:\n",
        "                # Fallback dimensions\n",
        "                self.token_dim = 1024\n",
        "                self.spatial_tokens = 256\n",
        "                self.flat_feature_dim = 1024\n",
        "                print(\"Using default V-JEPA2 feature dims\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, obs, obs_history=None, actions=None):\n",
        "        \"\"\"\n",
        "        obs: (3, 84, 84) normalized to [0,1] in C,H,W format (current frame)\n",
        "        obs_history: Optional list of previous observations. If provided, should be a list\n",
        "                    of T_context-1 frames. Current obs will be appended to make T_context total.\n",
        "                    If None, will use current frame repeated (fallback).\n",
        "        actions: Optional action history for AC predictor.\n",
        "                If use_ac=True, should be tensor of shape (T_context,) or (T_context, action_dim)\n",
        "                If None and use_ac=True, uses zero actions (no-op)\n",
        "        Returns: flattened features suitable for policy network\n",
        "        \"\"\"\n",
        "        context = max(T_CONTEXT, 1)  # Match temporal context used in training\n",
        "\n",
        "        # 1. Prepare observation sequence\n",
        "        if obs_history is not None and len(obs_history) > 0:\n",
        "            # Use actual temporal history: [oldest, ..., newest]\n",
        "            # Append current observation\n",
        "            obs_list = obs_history + [obs]\n",
        "\n",
        "            # Ensure we have exactly context frames\n",
        "            if len(obs_list) > context:\n",
        "                obs_list = obs_list[-context:]  # Take most recent frames\n",
        "            elif len(obs_list) < context:\n",
        "                # Pad from the left with oldest frame\n",
        "                oldest = obs_list[0]\n",
        "                obs_list = [oldest] * (context - len(obs_list)) + obs_list\n",
        "\n",
        "            # Convert list to tensor and resize each frame\n",
        "            obs_tensors = []\n",
        "            for frame in obs_list:\n",
        "                frame_tensor = torch.tensor(frame, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                frame_resized = F.interpolate(frame_tensor, size=(256, 256), mode='nearest')\n",
        "                obs_tensors.append(frame_resized.squeeze(0))\n",
        "\n",
        "            # Stack to (T, C, H, W)\n",
        "            obs_resized_seq = torch.stack(obs_tensors, dim=0)\n",
        "        else:\n",
        "            # Fallback: resize current frame and repeat (for backward compatibility)\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            obs_resized = F.interpolate(obs_tensor, size=(256, 256), mode='nearest').squeeze(0)\n",
        "            obs_resized_seq = obs_resized.repeat(context, 1, 1, 1)  # (T, C, H, W)\n",
        "\n",
        "        # Rearrange to (1, C, T, H, W) for V-JEPA\n",
        "        obs_seq = obs_resized_seq.unsqueeze(0).permute(0, 2, 1, 3, 4)  # (1, C, T, H, W)\n",
        "\n",
        "        # Store context for later use in AC predictor\n",
        "        self._current_T_context = context\n",
        "\n",
        "        # 3. Preprocess through V-JEPA preprocessor\n",
        "        proc_list = []\n",
        "        for t_idx in range(context):\n",
        "            frame = obs_seq[0, :, t_idx]  # Get (C, H, W) frame\n",
        "            if self.preprocessor is not None:\n",
        "                # Convert to (H, W, C) numpy array for preprocessor\n",
        "                frame_np = frame.cpu().permute(1, 2, 0).numpy()  # (H, W, C)\n",
        "                # Ensure values are in valid range [0, 1]\n",
        "                frame_np = np.clip(frame_np, 0, 1)\n",
        "                out = self.preprocessor([frame_np])\n",
        "                out = out[0] if isinstance(out, (list, tuple)) else out\n",
        "                if isinstance(out, torch.Tensor):\n",
        "                    # Ensure output is (C, H, W)\n",
        "                    if out.dim() == 4:  # (C, T, H, W) or (B, C, H, W)\n",
        "                        if out.shape[1] == 3 or out.shape[0] == 3:\n",
        "                            # Likely (C, T, H, W) or (B, C, H, W)\n",
        "                            if out.shape[0] == 3:  # (C, T, H, W)\n",
        "                                out = out[:, 0]  # Take first temporal slice -> (C, H, W)\n",
        "                            else:  # (B, C, H, W)\n",
        "                                out = out[0]  # Take first batch -> (C, H, W)\n",
        "                    elif out.dim() == 3:\n",
        "                        # Could be (C, H, W) or (H, W, C)\n",
        "                        if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                            out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                    proc_list.append(out)\n",
        "                else:\n",
        "                    # Convert numpy to tensor if needed\n",
        "                    if isinstance(out, np.ndarray):\n",
        "                        out = torch.from_numpy(out)\n",
        "                        # Ensure (C, H, W) format\n",
        "                        if out.dim() == 3:\n",
        "                            if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                    proc_list.append(out)\n",
        "            else:\n",
        "                proc_list.append(frame)\n",
        "\n",
        "        # Stack processed frames: (T, C, H, W) -> (1, C, T, H, W)\n",
        "        # Verify all frames have shape (C=3, H, W)\n",
        "        for i, frame in enumerate(proc_list):\n",
        "            if frame.dim() != 3 or frame.shape[0] != 3:\n",
        "                raise RuntimeError(f\"Frame {i} has unexpected shape: {frame.shape}, expected (3, H, W)\")\n",
        "\n",
        "        # Each item in proc_list is (C, H, W), stack to (T, C, H, W)\n",
        "        stacked = torch.stack(proc_list, dim=0)  # (T, C, H, W)\n",
        "        # Rearrange to (1, C, T, H, W)\n",
        "        proc = stacked.permute(1, 0, 2, 3).unsqueeze(0).contiguous()  # (1, C, T, H, W)\n",
        "        proc = proc.to(self.device)  # (1, C, T, 256, 256)\n",
        "\n",
        "        # 4. Encode through V-JEPA (with optional action conditioning)\n",
        "        if self.embed_actions_in_encoder and actions is not None and self.action_embedding is not None:\n",
        "            # Option 1: Embed actions and add to visual features BEFORE encoding\n",
        "            # This makes the encoder action-aware from the start\n",
        "            # Convert actions to embeddings\n",
        "            actions_tensor = torch.tensor(actions, device=self.device, dtype=torch.long)\n",
        "            if actions_tensor.dim() == 1:\n",
        "                # (T,) -> embed -> (T, D)\n",
        "                action_embeds = self.action_embedding(actions_tensor)  # (T, 1024)\n",
        "                # Broadcast to match spatial tokens: we'll add this after encoding\n",
        "                # For now, we encode normally and add action info after\n",
        "                out = self.vjepa_model(proc)\n",
        "            else:\n",
        "                # Already embedded or one-hot\n",
        "                out = self.vjepa_model(proc)\n",
        "        else:\n",
        "            # Standard encoding without action embedding\n",
        "            out = self.vjepa_model(proc)\n",
        "\n",
        "        # 5. Extract features\n",
        "        if isinstance(out, (list, tuple)) and len(out) > 0:\n",
        "            tokens = out[0]\n",
        "        elif isinstance(out, dict):\n",
        "            tokens = out.get('x', list(out.values())[0])\n",
        "        else:\n",
        "            tokens = out\n",
        "\n",
        "        # Get center temporal slice\n",
        "        if tokens.dim() == 4:  # (B, T', N, D)\n",
        "            tokens = tokens[:, tokens.size(1) // 2]  # Center frame\n",
        "\n",
        "        # tokens is now (B, N, D) where N = Ht * Wt\n",
        "\n",
        "        # 5b. Optionally inject action information into tokens after encoding\n",
        "        # This adds action context to visual tokens (alternative to AC predictor)\n",
        "        if self.embed_actions_in_encoder and actions is not None and self.action_embedding is not None:\n",
        "            # Get action embedding for most recent action\n",
        "            if isinstance(actions, (list, tuple)):\n",
        "                recent_action = actions[-1] if len(actions) > 0 else 0\n",
        "            else:\n",
        "                recent_action = actions[-1].item() if hasattr(actions[-1], 'item') else actions[-1]\n",
        "\n",
        "            action_embed = self.action_embedding(torch.tensor([recent_action], device=self.device))  # (1, D)\n",
        "            # Add action embedding to all spatial tokens (broadcast)\n",
        "            # This makes tokens action-aware: tokens += action_embed\n",
        "            tokens = tokens + action_embed.unsqueeze(1)  # (B, N, D) + (1, 1, D) -> (B, N, D)\n",
        "\n",
        "        # 6. Apply action-conditioned predictor if available\n",
        "        if self.use_ac and self.ac_predictor is not None:\n",
        "                        # Get context from the current processing\n",
        "            context = getattr(self, '_current_T_context', max(T_CONTEXT, 1))\n",
        "            \n",
        "            # Prepare actions for predictor\n",
        "            if actions is None:\n",
        "                # Use zero/no-op action for each timestep\n",
        "                if self.action_dim is not None:\n",
        "                    actions = torch.zeros(context, self.action_dim, device=self.device)\n",
        "                else:\n",
        "                    actions = torch.zeros(context, dtype=torch.long, device=self.device)\n",
        "            else:\n",
        "                # Ensure actions are on correct device and have correct shape\n",
        "                actions = torch.tensor(actions, device=self.device)\n",
        "                if actions.dim() == 1:\n",
        "                    # Convert to one-hot if needed\n",
        "                    if self.action_dim is not None and actions.dtype != torch.float32:\n",
        "                        actions_onehot = torch.zeros(len(actions), self.action_dim, device=self.device)\n",
        "                        actions_onehot.scatter_(1, actions.long().unsqueeze(1), 1.0)\n",
        "                        actions = actions_onehot\n",
        "                elif actions.dim() == 2 and actions.shape[1] != self.action_dim:\n",
        "                    # Might be one-hot already, check shape matches\n",
        "                    if actions.shape[1] == self.action_dim:\n",
        "                        pass  # Already correct\n",
        "                    else:\n",
        "                        # Assume it's action indices, convert to one-hot\n",
        "                        if self.action_dim is not None:\n",
        "                            actions_onehot = torch.zeros(len(actions), self.action_dim, device=self.device)\n",
        "                            actions_onehot.scatter_(1, actions.long().unsqueeze(1), 1.0)\n",
        "                            actions = actions_onehot\n",
        "\n",
        "            # Apply AC predictor: requires tokens, states, and actions\n",
        "            # For Atari, we don't have explicit robot states, so we use tokens as states\n",
        "            # Based on V-JEPA2-AC paper, states typically represent robot state (end-effector, joints)\n",
        "            # For visual-only environments like Atari, visual tokens serve as the state representation\n",
        "\n",
        "            try:\n",
        "                # Prepare tokens with temporal dimension if needed\n",
        "                if tokens.dim() == 3:  # (B, N, D)\n",
        "                    tokens_with_time = tokens.unsqueeze(1)  # (B, 1, N, D)\n",
        "                else:\n",
        "                    tokens_with_time = tokens\n",
        "\n",
        "                # Prepare actions with batch dimension if needed\n",
        "                if actions.dim() == 2:  # (T, action_dim) or (T,)\n",
        "                    actions_with_batch = actions.unsqueeze(0)  # (1, T, action_dim) or (1, T)\n",
        "                else:\n",
        "                    actions_with_batch = actions\n",
        "\n",
        "                # For Atari: use tokens as states (visual state representation)\n",
        "                # In robot tasks, states would be end-effector positions, joint angles, etc.\n",
        "                # Here, the spatial tokens encode the visual state of the game\n",
        "                states = tokens_with_time\n",
        "\n",
        "                # Try signature: predictor(tokens, states, actions) - common order in robotics\n",
        "                try:\n",
        "                    conditioned_tokens = self.ac_predictor(tokens_with_time, states, actions_with_batch)\n",
        "                except TypeError:\n",
        "                    # Try alternative order: predictor(tokens, actions, states)\n",
        "                    conditioned_tokens = self.ac_predictor(tokens_with_time, actions_with_batch, states)\n",
        "\n",
        "                # Handle output dimensions\n",
        "                if conditioned_tokens.dim() == 4:  # (B, T, N, D)\n",
        "                    conditioned_tokens = conditioned_tokens[:, 0]  # Take first timestep -> (B, N, D)\n",
        "                elif conditioned_tokens.dim() == 3:  # (B, N, D)\n",
        "                    pass  # Already correct\n",
        "\n",
        "                tokens = conditioned_tokens\n",
        "            except Exception as e:\n",
        "                # Fallback: try different argument patterns\n",
        "                try:\n",
        "                    # Try: predictor(states, actions) - some implementations combine tokens+states\n",
        "                    if tokens.dim() == 3:\n",
        "                        tokens_with_time = tokens.unsqueeze(1)\n",
        "                    else:\n",
        "                        tokens_with_time = tokens\n",
        "                    if actions.dim() == 2:\n",
        "                        actions_with_batch = actions.unsqueeze(0)\n",
        "                    else:\n",
        "                        actions_with_batch = actions\n",
        "                    states = tokens_with_time\n",
        "                    conditioned_tokens = self.ac_predictor(states, actions_with_batch)\n",
        "                    if conditioned_tokens.dim() == 4:\n",
        "                        conditioned_tokens = conditioned_tokens[:, 0]\n",
        "                    tokens = conditioned_tokens\n",
        "                except Exception as e2:\n",
        "                    # Final fallback: try minimal signature or disable AC\n",
        "                    try:\n",
        "                        conditioned_tokens = self.ac_predictor(tokens_with_time, actions_with_batch)\n",
        "                        if conditioned_tokens.dim() == 4:\n",
        "                            conditioned_tokens = conditioned_tokens[:, 0]\n",
        "                        tokens = conditioned_tokens\n",
        "                    except Exception as e3:\n",
        "                        print(f\"Warning: Could not apply AC predictor: {e3}\")\n",
        "                        print(f\"Note: V-JEPA2-AC was designed for robot control with explicit states.\")\n",
        "                        print(f\"For Atari, consider disabling AC (USE_AC=0) or use visual tokens as states.\")\n",
        "                        print(f\"Continuing with unconditioned features.\")\n",
        "\n",
        "        # 7. Pool spatially for policy: average pool or flatten\n",
        "        features = tokens.mean(dim=1)  # (B, D) - average pooling over spatial tokens\n",
        "\n",
        "        return features.squeeze(0)  # (D,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNFeatureExtractor(nn.Module):\n",
        "    \"\"\"Simple CNN feature extractor for Atari frames (baseline without V-JEPA).\"\"\"\n",
        "\n",
        "    def __init__(self, context_frames: int, device: str, output_dim: int = 256):\n",
        "        super().__init__()\n",
        "        self.context = max(context_frames, 1)\n",
        "        in_channels = self.context * 3  # RGB frames stacked along channel dimension\n",
        "        self.device = device\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, output_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.flat_feature_dim = output_dim\n",
        "\n",
        "    def _stack_frames(self, obs, obs_history):\n",
        "        frames = []\n",
        "        if obs_history is not None and len(obs_history) > 0:\n",
        "            frames.extend(obs_history)\n",
        "        frames.append(obs)\n",
        "\n",
        "        if len(frames) > self.context:\n",
        "            frames = frames[-self.context:]\n",
        "        elif len(frames) < self.context:\n",
        "            frames = [frames[0]] * (self.context - len(frames)) + frames\n",
        "\n",
        "        frame_array = np.stack(frames, axis=0)\n",
        "        frame_tensor = torch.from_numpy(frame_array).to(self.device, dtype=torch.float32)\n",
        "        # (context, C, H, W) -> (1, context*C, H, W)\n",
        "        frame_tensor = frame_tensor.reshape(1, self.context * 3, frame_tensor.size(-2), frame_tensor.size(-1))\n",
        "        return frame_tensor\n",
        "\n",
        "    def forward(self, obs, obs_history=None, actions=None):\n",
        "        x = self._stack_frames(obs, obs_history)\n",
        "        feats = self.conv(x)\n",
        "        feats = feats.view(feats.size(0), -1)\n",
        "        feats = self.fc(feats)\n",
        "        return feats.squeeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNIA-d8Y_Bq6"
      },
      "source": [
        "## Policy Network and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kauVQ7sK_Bq6",
        "outputId": "1cb35be4-86dc-4b22-c1de-76ee567c37d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created action embedding layer: 18 actions -> 1024 dim\n",
            "V-JEPA2 feature dims: token_dim=1024, spatial_grid=22x22\n",
            "Optimizer includes 18432 action embedding parameters\n",
            "Encoder feature dim: 1024\n",
            "Action dim: 18\n",
            "Using action-conditioned predictor (AC): False\n",
            "Using action embedding in encoder: True\n",
            "Action embedding parameters will be learned during policy training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "class LinearPolicy(nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# Initialize feature extractor and policy\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "if USE_VJEPA_ENCODER:\n",
        "    print(\"Using V-JEPA2 feature extractor (frozen).\")\n",
        "    encoder = VJEPA2FeatureExtractor(\n",
        "        vjepa_model,\n",
        "        preprocessor,\n",
        "        device,\n",
        "        ac_predictor=vjepa_ac_predictor,\n",
        "        action_dim=action_dim,\n",
        "        embed_actions_in_encoder=EMBED_ACTIONS\n",
        "    ).to(device)\n",
        "    latent_dim = encoder.flat_feature_dim\n",
        "    policy = LinearPolicy(latent_dim, action_dim).to(device)\n",
        "\n",
        "    params = list(policy.parameters())\n",
        "    if encoder.embed_actions_in_encoder and encoder.action_embedding is not None:\n",
        "        params += list(encoder.action_embedding.parameters())\n",
        "        print(f\"Optimizer will update {sum(p.numel() for p in encoder.action_embedding.parameters())} action-embedding parameters\")\n",
        "    optimizer = torch.optim.Adam(params, lr=LEARNING_RATE)\n",
        "else:\n",
        "    print(\"Using simple CNN feature extractor (trainable baseline).\")\n",
        "    encoder = CNNFeatureExtractor(T_CONTEXT, device).to(device)\n",
        "    latent_dim = encoder.flat_feature_dim\n",
        "    policy = LinearPolicy(latent_dim, action_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(list(policy.parameters()) + list(encoder.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "print(f\"Encoder feature dim: {latent_dim}\")\n",
        "print(f\"Action dim: {action_dim}\")\n",
        "if USE_VJEPA_ENCODER:\n",
        "    print(f\"Using action-conditioned predictor (AC): {encoder.use_ac}\")\n",
        "    print(f\"Using action embedding in encoder: {encoder.embed_actions_in_encoder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity check on feature extractor\n",
        "sample_obs, _ = env.reset()\n",
        "with torch.no_grad():\n",
        "    sample_features = encoder(sample_obs, obs_history=None, actions=None)\n",
        "print(f\"Sample feature vector shape: {sample_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEJaWrDq_Bq6"
      },
      "source": [
        "## Policy Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiERAlJd_Bq6"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, encoder, policy, gamma=GAMMA, seed=None, save_video=False):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        env.reset(seed=seed)\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    log_probs, rewards = [], []\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    raw_frames = []\n",
        "\n",
        "    # Track action and observation history for temporal context\n",
        "    context = getattr(encoder, \"context\", max(T_CONTEXT, 1))\n",
        "    action_history = []\n",
        "    obs_history = []  # Store previous observations for temporal sequence\n",
        "\n",
        "    while not done:\n",
        "        frame = (obs * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "        raw_frames.append(frame)\n",
        "\n",
        "        # Prepare action history for AC predictor / action embedding\n",
        "        uses_ac = getattr(encoder, \"use_ac\", False)\n",
        "        if uses_ac:\n",
        "            if len(action_history) == 0:\n",
        "                # First step: use no-op actions (action 0 typically)\n",
        "                actions_for_encoder = [0] * context\n",
        "            else:\n",
        "                recent_actions = action_history[-context:]\n",
        "                actions_for_encoder = [0] * (context - len(recent_actions)) + recent_actions\n",
        "        else:\n",
        "            actions_for_encoder = None\n",
        "\n",
        "        # Extract features using the configured encoder\n",
        "        z = encoder(obs, obs_history=obs_history, actions=actions_for_encoder)\n",
        "        logits = policy(z)\n",
        "\n",
        "        # Mask invalid actions if needed\n",
        "        valid = env.action_space.n\n",
        "        logits[valid:] = -1e9\n",
        "\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "\n",
        "        log_probs.append(dist.log_prob(action))\n",
        "\n",
        "        # Extract action as scalar (handle both batched and unbatched cases)\n",
        "        if action.dim() > 0:\n",
        "            action_scalar = action.squeeze().item()\n",
        "        else:\n",
        "            action_scalar = action.item()\n",
        "\n",
        "        # Store action and observation for next step's history\n",
        "        action_history.append(action_scalar)\n",
        "        obs_history.append(obs.copy())  # Store current observation before step\n",
        "\n",
        "        # Keep only last context-1 observations (current obs will be added next step)\n",
        "        if len(obs_history) >= context:\n",
        "            obs_history = obs_history[-(context-1):]\n",
        "\n",
        "        obs, reward, terminated, truncated, _ = env.step(action_scalar)\n",
        "        done = terminated or truncated\n",
        "        rewards.append(reward)\n",
        "        total_reward += reward\n",
        "\n",
        "    if save_video:\n",
        "        imageio.mimsave(\"vjepa2_raw.gif\", raw_frames, fps=15)\n",
        "        print(\"Saved: vjepa2_raw.gif\")\n",
        "\n",
        "    returns, G = [], 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
        "    return loss, float(total_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqxkrIyq_Bq6"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRqSCB2K_Bq6",
        "outputId": "9df597e4-0500-4eed-cf49-4e72dc496e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/200 | Reward: 130.0\n",
            "Episode 2/200 | Reward: 150.0\n",
            "Episode 3/200 | Reward: 230.0\n",
            "Episode 4/200 | Reward: 110.0\n",
            "Episode 5/200 | Reward: 140.0\n",
            "Episode 6/200 | Reward: 110.0\n",
            "Episode 7/200 | Reward: 110.0\n",
            "Episode 8/200 | Reward: 180.0\n",
            "Episode 9/200 | Reward: 260.0\n",
            "Episode 10/200 | Reward: 280.0\n",
            "Episode 11/200 | Reward: 260.0\n",
            "Episode 12/200 | Reward: 230.0\n",
            "Episode 13/200 | Reward: 220.0\n",
            "Episode 14/200 | Reward: 140.0\n",
            "Episode 15/200 | Reward: 340.0\n",
            "Episode 16/200 | Reward: 270.0\n",
            "Episode 17/200 | Reward: 250.0\n",
            "Episode 18/200 | Reward: 140.0\n",
            "Episode 19/200 | Reward: 260.0\n",
            "Episode 20/200 | Reward: 140.0\n",
            "Episode 21/200 | Reward: 140.0\n",
            "Episode 22/200 | Reward: 250.0\n",
            "Episode 23/200 | Reward: 140.0\n",
            "Episode 24/200 | Reward: 140.0\n",
            "Episode 25/200 | Reward: 250.0\n",
            "Episode 26/200 | Reward: 230.0\n",
            "Episode 27/200 | Reward: 230.0\n",
            "Episode 28/200 | Reward: 140.0\n",
            "Episode 29/200 | Reward: 230.0\n",
            "Episode 30/200 | Reward: 140.0\n",
            "Episode 31/200 | Reward: 140.0\n",
            "Episode 32/200 | Reward: 140.0\n",
            "Episode 33/200 | Reward: 140.0\n",
            "Episode 34/200 | Reward: 140.0\n",
            "Episode 35/200 | Reward: 140.0\n",
            "Episode 36/200 | Reward: 140.0\n",
            "Episode 37/200 | Reward: 140.0\n",
            "Episode 38/200 | Reward: 230.0\n",
            "Episode 39/200 | Reward: 140.0\n",
            "Episode 40/200 | Reward: 140.0\n",
            "Episode 41/200 | Reward: 140.0\n",
            "Episode 42/200 | Reward: 250.0\n",
            "Episode 43/200 | Reward: 140.0\n",
            "Episode 44/200 | Reward: 140.0\n",
            "Episode 45/200 | Reward: 140.0\n",
            "Episode 46/200 | Reward: 140.0\n",
            "Episode 47/200 | Reward: 140.0\n",
            "Episode 48/200 | Reward: 140.0\n",
            "Episode 49/200 | Reward: 140.0\n",
            "Episode 50/200 | Reward: 140.0\n",
            "Episode 51/200 | Reward: 140.0\n",
            "Episode 52/200 | Reward: 140.0\n",
            "Episode 53/200 | Reward: 140.0\n",
            "Episode 54/200 | Reward: 140.0\n",
            "Episode 55/200 | Reward: 140.0\n",
            "Episode 56/200 | Reward: 140.0\n",
            "Episode 57/200 | Reward: 140.0\n",
            "Episode 58/200 | Reward: 230.0\n",
            "Episode 59/200 | Reward: 140.0\n",
            "Episode 60/200 | Reward: 140.0\n",
            "Episode 61/200 | Reward: 140.0\n",
            "Episode 62/200 | Reward: 140.0\n",
            "Episode 63/200 | Reward: 140.0\n",
            "Episode 64/200 | Reward: 140.0\n",
            "Episode 65/200 | Reward: 140.0\n",
            "Episode 66/200 | Reward: 140.0\n",
            "Episode 67/200 | Reward: 140.0\n",
            "Episode 68/200 | Reward: 140.0\n",
            "Episode 69/200 | Reward: 140.0\n",
            "Episode 70/200 | Reward: 140.0\n",
            "Episode 71/200 | Reward: 140.0\n",
            "Episode 72/200 | Reward: 140.0\n",
            "Episode 73/200 | Reward: 250.0\n",
            "Episode 74/200 | Reward: 140.0\n",
            "Episode 75/200 | Reward: 140.0\n",
            "Episode 76/200 | Reward: 140.0\n",
            "Episode 77/200 | Reward: 140.0\n",
            "Episode 78/200 | Reward: 140.0\n",
            "Episode 79/200 | Reward: 140.0\n",
            "Episode 80/200 | Reward: 140.0\n",
            "Episode 81/200 | Reward: 140.0\n",
            "Episode 82/200 | Reward: 140.0\n",
            "Episode 83/200 | Reward: 140.0\n",
            "Episode 84/200 | Reward: 140.0\n",
            "Episode 85/200 | Reward: 140.0\n",
            "Episode 86/200 | Reward: 140.0\n",
            "Episode 87/200 | Reward: 140.0\n",
            "Episode 88/200 | Reward: 140.0\n",
            "Episode 89/200 | Reward: 140.0\n",
            "Episode 90/200 | Reward: 140.0\n",
            "Episode 91/200 | Reward: 140.0\n",
            "Episode 92/200 | Reward: 140.0\n",
            "Episode 93/200 | Reward: 140.0\n",
            "Episode 94/200 | Reward: 250.0\n",
            "Episode 95/200 | Reward: 140.0\n",
            "Episode 96/200 | Reward: 140.0\n",
            "Episode 97/200 | Reward: 140.0\n",
            "Episode 98/200 | Reward: 140.0\n",
            "Episode 99/200 | Reward: 140.0\n",
            "Episode 100/200 | Reward: 140.0\n",
            "Episode 101/200 | Reward: 140.0\n",
            "Episode 102/200 | Reward: 140.0\n",
            "Episode 103/200 | Reward: 140.0\n",
            "Episode 104/200 | Reward: 140.0\n",
            "Episode 105/200 | Reward: 140.0\n",
            "Episode 106/200 | Reward: 140.0\n",
            "Episode 107/200 | Reward: 140.0\n",
            "Episode 108/200 | Reward: 140.0\n",
            "Episode 109/200 | Reward: 140.0\n",
            "Episode 110/200 | Reward: 140.0\n",
            "Episode 111/200 | Reward: 140.0\n",
            "Episode 112/200 | Reward: 140.0\n",
            "Episode 113/200 | Reward: 140.0\n",
            "Episode 114/200 | Reward: 140.0\n",
            "Episode 115/200 | Reward: 140.0\n",
            "Episode 116/200 | Reward: 140.0\n",
            "Episode 117/200 | Reward: 140.0\n",
            "Episode 118/200 | Reward: 140.0\n",
            "Episode 119/200 | Reward: 140.0\n",
            "Episode 120/200 | Reward: 140.0\n",
            "Episode 121/200 | Reward: 140.0\n",
            "Episode 122/200 | Reward: 140.0\n",
            "Episode 123/200 | Reward: 140.0\n",
            "Episode 124/200 | Reward: 140.0\n",
            "Episode 125/200 | Reward: 140.0\n",
            "Episode 126/200 | Reward: 140.0\n",
            "Episode 127/200 | Reward: 140.0\n",
            "Episode 128/200 | Reward: 140.0\n",
            "Episode 129/200 | Reward: 140.0\n",
            "Episode 130/200 | Reward: 140.0\n",
            "Episode 131/200 | Reward: 140.0\n",
            "Episode 132/200 | Reward: 140.0\n",
            "Episode 133/200 | Reward: 140.0\n",
            "Episode 134/200 | Reward: 140.0\n",
            "Episode 135/200 | Reward: 140.0\n",
            "Episode 136/200 | Reward: 140.0\n",
            "Episode 137/200 | Reward: 140.0\n",
            "Episode 138/200 | Reward: 140.0\n",
            "Episode 139/200 | Reward: 140.0\n",
            "Episode 140/200 | Reward: 140.0\n",
            "Episode 141/200 | Reward: 140.0\n",
            "Episode 142/200 | Reward: 140.0\n",
            "Episode 143/200 | Reward: 140.0\n",
            "Episode 144/200 | Reward: 140.0\n",
            "Episode 145/200 | Reward: 140.0\n",
            "Episode 146/200 | Reward: 140.0\n",
            "Episode 147/200 | Reward: 140.0\n",
            "Episode 148/200 | Reward: 140.0\n",
            "Episode 149/200 | Reward: 140.0\n",
            "Episode 150/200 | Reward: 140.0\n",
            "Episode 151/200 | Reward: 140.0\n",
            "Episode 152/200 | Reward: 140.0\n",
            "Episode 153/200 | Reward: 140.0\n",
            "Episode 154/200 | Reward: 140.0\n",
            "Episode 155/200 | Reward: 140.0\n",
            "Episode 156/200 | Reward: 140.0\n",
            "Episode 157/200 | Reward: 140.0\n",
            "Episode 158/200 | Reward: 140.0\n",
            "Episode 159/200 | Reward: 140.0\n",
            "Episode 160/200 | Reward: 140.0\n",
            "Episode 161/200 | Reward: 140.0\n",
            "Episode 162/200 | Reward: 140.0\n",
            "Episode 163/200 | Reward: 140.0\n",
            "Episode 164/200 | Reward: 140.0\n",
            "Episode 165/200 | Reward: 140.0\n",
            "Episode 166/200 | Reward: 140.0\n",
            "Episode 167/200 | Reward: 140.0\n",
            "Episode 168/200 | Reward: 140.0\n",
            "Episode 169/200 | Reward: 140.0\n",
            "Episode 170/200 | Reward: 140.0\n",
            "Episode 171/200 | Reward: 140.0\n",
            "Episode 172/200 | Reward: 140.0\n",
            "Episode 173/200 | Reward: 140.0\n",
            "Episode 174/200 | Reward: 140.0\n",
            "Episode 175/200 | Reward: 140.0\n",
            "Episode 176/200 | Reward: 140.0\n",
            "Episode 177/200 | Reward: 140.0\n",
            "Episode 178/200 | Reward: 140.0\n",
            "Episode 179/200 | Reward: 140.0\n",
            "Episode 180/200 | Reward: 140.0\n",
            "Episode 181/200 | Reward: 140.0\n",
            "Episode 182/200 | Reward: 140.0\n",
            "Episode 183/200 | Reward: 140.0\n",
            "Episode 184/200 | Reward: 140.0\n",
            "Episode 185/200 | Reward: 140.0\n",
            "Episode 186/200 | Reward: 140.0\n",
            "Episode 187/200 | Reward: 140.0\n",
            "Episode 188/200 | Reward: 140.0\n",
            "Episode 189/200 | Reward: 140.0\n",
            "Episode 190/200 | Reward: 140.0\n",
            "Episode 191/200 | Reward: 140.0\n",
            "Episode 192/200 | Reward: 140.0\n",
            "Episode 193/200 | Reward: 140.0\n",
            "Episode 194/200 | Reward: 140.0\n",
            "Episode 195/200 | Reward: 140.0\n",
            "Episode 196/200 | Reward: 140.0\n",
            "Episode 197/200 | Reward: 140.0\n",
            "Episode 198/200 | Reward: 140.0\n",
            "Episode 199/200 | Reward: 140.0\n",
            "Episode 200/200 | Reward: 140.0\n"
          ]
        }
      ],
      "source": [
        "if \"Pong\" in GAME_ID:\n",
        "    print(\"Pong theoretical max episodic reward: 21 (win 21-0).\")\n",
        "\n",
        "reward_history = []\n",
        "best_reward = float(\"-inf\")\n",
        "\n",
        "for ep in range(NUM_EPISODES):\n",
        "    policy.train()\n",
        "    if USE_VJEPA_ENCODER:\n",
        "        encoder.eval()\n",
        "    else:\n",
        "        encoder.train()\n",
        "\n",
        "    loss, total_reward = run_episode(env, encoder, policy, seed=ep)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    reward_history.append(float(total_reward))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    best_reward = max(best_reward, total_reward)\n",
        "    print(f\"Episode {ep+1}/{NUM_EPISODES} | Reward: {total_reward:.1f} | Best so far: {best_reward:.1f}\")\n",
        "\n",
        "print(f\"Training complete. Best episodic reward achieved: {best_reward:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfcnrnCE_Bq6"
      },
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2TrYjGF_Bq6",
        "outputId": "7630b88d-2931-4de3-bb1e-0040f8ac9e3e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: # or: %config InlineBackend.figure_format = 'retina'\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Policy Training Reward Curve\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm8QpLo6_Bq7",
        "outputId": "576a8aa5-7284-465e-a2d1-a8e747c6f05b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (84, 84) to (96, 96) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved video to vjepa2_eval.mp4 | Episode return: 250.0\n"
          ]
        }
      ],
      "source": [
        "# === EVALUATE AND RECORD A GAME RUN ===\n",
        "import numpy as np, torch, imageio\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_and_record(env, encoder, policy, seed=0, save_path=\"vjepa2_eval.mp4\",\n",
        "                        fps=30, overlay_actions=True):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    policy.eval()\n",
        "\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "\n",
        "    # Match training-time context tracking\n",
        "    context = getattr(encoder, \"context\", max(T_CONTEXT, 1))\n",
        "    action_history, obs_history = [], []\n",
        "    frames = []\n",
        "\n",
        "    # Helper to render RGB HxWxC uint8 (from normalized 84×84 obs)\n",
        "    def obs_to_rgb(o):\n",
        "        return (np.clip(o, 0, 1) * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "\n",
        "    while not done:\n",
        "        uses_ac = getattr(encoder, \"use_ac\", False)\n",
        "        if uses_ac:\n",
        "            if len(action_history) == 0:\n",
        "                actions_for_encoder = [0] * context\n",
        "            else:\n",
        "                recent = action_history[-context:]\n",
        "                actions_for_encoder = [0] * (context - len(recent)) + recent\n",
        "        else:\n",
        "            actions_for_encoder = None\n",
        "\n",
        "        # Encode (with temporal obs history)\n",
        "        z = encoder(obs, obs_history=obs_history, actions=actions_for_encoder)\n",
        "        logits = policy(z)\n",
        "\n",
        "        # Sample action\n",
        "        valid = env.action_space.n\n",
        "        logits[valid:] = -1e9\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        action_scalar = action.item()\n",
        "\n",
        "        # Render current frame (before step) with optional overlay\n",
        "        if save_path or overlay_actions:\n",
        "            frame = obs_to_rgb(obs)\n",
        "            if overlay_actions:\n",
        "                img = Image.fromarray(frame)\n",
        "                draw = ImageDraw.Draw(img)\n",
        "                txt = f\"action={action_scalar}\"\n",
        "                draw.rectangle([0, 0, 140, 20], fill=(0, 0, 0, 160))\n",
        "                draw.text((5, 3), txt, fill=(255, 255, 255))\n",
        "                frame = np.array(img)\n",
        "            frames.append(frame)\n",
        "\n",
        "        # Update histories for next step\n",
        "        action_history.append(action_scalar)\n",
        "        obs_history.append(obs.copy())\n",
        "        if len(obs_history) >= context:\n",
        "            obs_history = obs_history[-(context-1):]\n",
        "\n",
        "        # Step env\n",
        "        obs, reward, terminated, truncated, _ = env.step(action_scalar)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    # Save video (optional)\n",
        "    if save_path:\n",
        "        with imageio.get_writer(save_path, fps=fps, macro_block_size=1) as w:\n",
        "            for f in frames:\n",
        "                w.append_data(f)\n",
        "        print(f\"Saved video to {save_path} | Episode return: {total_reward:.1f}\")\n",
        "    else:\n",
        "        print(f\"Episode return: {total_reward:.1f}\")\n",
        "    return total_reward\n",
        "\n",
        "# Usage (re-use your existing env/encoder/policy from training)\n",
        "_ = evaluate_and_record(env, encoder, policy, seed=123, save_path=\"vjepa2_eval.mp4\", fps=30, overlay_actions=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_eval_env(seed=None):\n",
        "    env = gym.make(GAME_ID)\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    env = gym.wrappers.TransformObservation(env, func=transform_obs, observation_space=new_obs_space)\n",
        "    if seed is not None:\n",
        "        env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "\n",
        "def evaluate_policy(encoder, policy, episodes=EVAL_EPISODES, seed_offset=1000):\n",
        "    policy.eval()\n",
        "    returns = []\n",
        "    for idx in range(episodes):\n",
        "        eval_env = make_eval_env(seed=seed_offset + idx)\n",
        "        reward = evaluate_and_record(eval_env, encoder, policy, seed=seed_offset + idx,\n",
        "                                     save_path=None, overlay_actions=False)\n",
        "        returns.append(reward)\n",
        "        eval_env.close()\n",
        "    returns = np.array(returns)\n",
        "    print(f\"Evaluation over {episodes} episode(s): mean={returns.mean():.2f}, std={returns.std():.2f}, max={returns.max():.2f}\")\n",
        "    return returns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8khgXka_Bq7"
      },
      "source": [
        "## Notes\n",
        "\n",
        "### Pipeline Overview\n",
        "\n",
        "This notebook implements a policy training pipeline using V-JEPA2 encoder as a frozen feature extractor:\n",
        "\n",
        "1. **Environment**: Atari Alien with 84×84 RGB observations\n",
        "2. **Encoder**: V-JEPA2 (ViT-L) - pre-trained, frozen\n",
        "3. **Feature Extraction**:\n",
        "   - Resize observations from 84×84 → 256×256 (V-JEPA expects 256×256)\n",
        "   - Create temporal context using the last T=4 frames (real history)\n",
        "   - Extract spatial tokens from the center temporal slice\n",
        "   - Average pool over spatial tokens to get flat features\n",
        "4. **Policy**: 2-layer MLP (linear → ReLU → linear)\n",
        "5. **Training**: REINFORCE with returns normalization\n",
        "\n",
        "### Key Differences from RSSM Pipeline\n",
        "\n",
        "- **Image Resolution**: RSSM uses 64×64; V-JEPA uses 84×84 → 256×256\n",
        "- **Temporal Context**: V-JEPA uses explicit multi-frame input; RSSM uses an RNN state\n",
        "- **Feature Extraction**: V-JEPA outputs spatial tokens that we pool; RSSM learns a global embedding\n",
        "- **Dynamics**: No learned dynamics here; encoder features are used directly\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "- Consider using the V-JEPA-AC variant for action-conditioned features\n",
        "- Explore different spatial pooling strategies (attention-weighted, max pooling, etc.)\n",
        "- Add a dynamics model on top of V-JEPA features (as done in RSSM)\n",
        "- Implement imagination-based training using predicted futures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "try:\n",
        "    env.close()\n",
        "except NameError:\n",
        "    pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txdTHhGXAxD6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
