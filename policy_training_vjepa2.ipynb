{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdUSJBJP_Bq4"
      },
      "source": [
        "# V-JEPA2 Policy Training Pipeline\n",
        "\n",
        "This notebook implements policy training using the V-JEPA2 encoder, following the same structure as the RSSM-based policy training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eccwVYDn_Bq4",
        "outputId": "c9f2b49f-1cb4-4a7a-a4d5-a2e868b17d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import ale_py\n",
        "import imageio\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQsbW_VA_Bq5"
      },
      "source": [
        "## Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YOfiYxE_Bq5",
        "outputId": "77bf1549-bb39-4480-b329-f4ab6b1e4bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment initialized successfully\n"
          ]
        }
      ],
      "source": [
        "from gymnasium import spaces\n",
        "\n",
        "# Create environment - using Alien for consistency with RSSM notebook\n",
        "env = gym.make(\"AlienNoFrameskip-v4\")\n",
        "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "def transform_obs(obs):\n",
        "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "\n",
        "env = gym.wrappers.TransformObservation(\n",
        "    env,\n",
        "    func=transform_obs,\n",
        "    observation_space=new_obs_space,\n",
        ")\n",
        "\n",
        "obs, info = env.reset()\n",
        "assert obs.shape == (3, 84, 84), f\"Expected (3, 84, 84), got {obs.shape}\"\n",
        "print(\"Environment initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDxKSxpr_Bq5"
      },
      "source": [
        "## Load V-JEPA2 Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YElwTBeF_Bq5",
        "outputId": "ea0cf76d-be63-4c9f-c6d4-2cb512d050e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/vjepa2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_vjepa2_main\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/vjepa2/vitl.pt\" to /root/.cache/torch/hub/checkpoints/vitl.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.78G/4.78G [00:24<00:00, 210MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded V-JEPA 2 (ViT-L) from hub.\n",
            "V-JEPA2 encoder loaded successfully (AC mode: False)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# V-JEPA2 configuration\n",
        "VJEPA_BACKEND = os.environ.get(\"VJEPA_BACKEND\", \"hub\")\n",
        "# Note: V-JEPA2-AC was designed for robot control with explicit states (end-effector, joints)\n",
        "# For Atari (visual-only), regular V-JEPA2 may be more appropriate\n",
        "# Set USE_AC=1 to try AC model (uses visual tokens as states), USE_AC=0 for regular model\n",
        "USE_AC = bool(int(os.environ.get(\"VJEPA_USE_AC\", \"0\")))  # Default to regular V-JEPA2 for Atari\n",
        "HF_REPO = os.environ.get(\"VJEPA_HF_REPO\", \"facebook/vjepa2-vitl-fpc64-256\")\n",
        "\n",
        "# Load V-JEPA2 model and preprocessor\n",
        "preprocessor = None\n",
        "vjepa_model = None\n",
        "vjepa_ac_predictor = None\n",
        "\n",
        "if VJEPA_BACKEND == \"hub\":\n",
        "    preprocessor = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_preprocessor')\n",
        "    if USE_AC:\n",
        "        obj = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_ac_vit_giant')\n",
        "        if isinstance(obj, tuple):\n",
        "            vjepa_model, vjepa_ac_predictor = obj[0], obj[1]\n",
        "        else:\n",
        "            vjepa_model = obj\n",
        "            vjepa_ac_predictor = None\n",
        "        print(\"Loaded V-JEPA 2-AC (ViT-g) from hub.\")\n",
        "        if vjepa_ac_predictor is not None:\n",
        "            vjepa_ac_predictor.to(device).eval()\n",
        "            for p in vjepa_ac_predictor.parameters():\n",
        "                p.requires_grad_(False)\n",
        "            print(\"Action-conditioned predictor loaded and frozen.\")\n",
        "    else:\n",
        "        obj = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_large')\n",
        "        vjepa_model = obj[0] if isinstance(obj, tuple) else obj\n",
        "        print(\"Loaded V-JEPA 2 (ViT-L) from hub.\")\n",
        "\n",
        "    vjepa_model.to(device).eval()\n",
        "    for p in vjepa_model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "elif VJEPA_BACKEND == \"hf\":\n",
        "    from transformers import AutoVideoProcessor, AutoModel\n",
        "    vjepa_model = AutoModel.from_pretrained(HF_REPO).to(device).eval()\n",
        "    preprocessor = AutoVideoProcessor.from_pretrained(HF_REPO)\n",
        "    for p in vjepa_model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    print(f\"Loaded {HF_REPO} from Hugging Face.\")\n",
        "    if USE_AC:\n",
        "        print(\"Warning: Action-conditioned model not available via HuggingFace. Use hub backend.\")\n",
        "else:\n",
        "    raise ValueError(\"VJEPA_BACKEND must be 'hub' or 'hf'.\")\n",
        "\n",
        "print(f\"V-JEPA2 encoder loaded successfully (AC mode: {USE_AC})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkD8h3hN_Bq5"
      },
      "source": [
        "## V-JEPA2 Feature Extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzKxCp9J_Bq5"
      },
      "outputs": [],
      "source": [
        "class VJEPA2FeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts features from a single observation using V-JEPA2 encoder.\n",
        "\n",
        "    V-JEPA2 expects 256x256 images, so we need to:\n",
        "    1. Resize observations to 256x256 (we use 84×84 → 256×256)\n",
        "    2. Create a temporal context (V-JEPA expects sequences)\n",
        "    3. Extract spatial features from center frame tokens\n",
        "\n",
        "    If action-conditioned predictor is available, actions are used to condition features.\n",
        "    \"\"\"\n",
        "    def __init__(self, vjepa_model, preprocessor, device, ac_predictor=None, action_dim=None, embed_actions_in_encoder=False):\n",
        "        super().__init__()\n",
        "        self.vjepa_model = vjepa_model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.device = device\n",
        "        self.ac_predictor = ac_predictor\n",
        "        self.use_ac = ac_predictor is not None\n",
        "        self.action_dim = action_dim\n",
        "        self.embed_actions_in_encoder = embed_actions_in_encoder\n",
        "\n",
        "        # Create action embedding if we want to encode actions into visual tokens\n",
        "        if self.embed_actions_in_encoder and action_dim is not None:\n",
        "            # Action embedding: map action indices to embedding space\n",
        "            # Embedding dimension should match token dimension (typically 1024)\n",
        "            self.action_embedding = nn.Embedding(action_dim, 1024).to(device)\n",
        "            print(f\"Created action embedding layer: {action_dim} actions -> 1024 dim\")\n",
        "        else:\n",
        "            self.action_embedding = None\n",
        "\n",
        "        # Infer output dimensions from the encoder\n",
        "        self._infer_output_dims()\n",
        "\n",
        "    def _infer_output_dims(self):\n",
        "        \"\"\"Probe the encoder to determine output dimensions\"\"\"\n",
        "        # Create dummy input: single frame resized to 256x256, then create temporal context\n",
        "        # Use a simpler approach: create a dummy observation similar to what we'll get from env\n",
        "        dummy_obs = np.random.rand(3, 64, 64).astype(np.float32)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Convert to tensor and resize (matching forward() method)\n",
        "            obs_tensor = torch.tensor(dummy_obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            obs_resized = F.interpolate(\n",
        "                obs_tensor,\n",
        "                size=(256, 256),\n",
        "                mode='nearest'\n",
        "            )  # (1, 3, 256, 256)\n",
        "\n",
        "            # Create temporal context (for inference, just repeat frame since we don't have history)\n",
        "            # In actual usage, obs_history will provide real sequences\n",
        "            T_context = 4\n",
        "            obs_seq = obs_resized.repeat(T_context, 1, 1, 1).unsqueeze(0)  # (1, T, C, H, W)\n",
        "            obs_seq = obs_seq.permute(0, 2, 1, 3, 4)  # (1, C, T, H, W)\n",
        "\n",
        "            # Process through preprocessor (matching forward() method)\n",
        "            proc_list = []\n",
        "            for t_idx in range(T_context):\n",
        "                frame = obs_seq[0, :, t_idx]  # Get (C, H, W) frame\n",
        "                if self.preprocessor is not None:\n",
        "                    # Convert to (H, W, C) numpy array for preprocessor\n",
        "                    frame_np = frame.cpu().permute(1, 2, 0).numpy()  # (H, W, C)\n",
        "                    # Ensure values are in valid range [0, 1] or [0, 255]\n",
        "                    frame_np = np.clip(frame_np, 0, 1)\n",
        "                    out = self.preprocessor([frame_np])\n",
        "                    out = out[0] if isinstance(out, (list, tuple)) else out\n",
        "                    if isinstance(out, torch.Tensor):\n",
        "                        # Ensure output is (C, H, W)\n",
        "                        if out.dim() == 4:  # (C, T, H, W) or (B, C, H, W)\n",
        "                            if out.shape[1] == 3 or out.shape[0] == 3:\n",
        "                                # Likely (C, T, H, W) or (B, C, H, W)\n",
        "                                if out.shape[0] == 3:  # (C, T, H, W)\n",
        "                                    out = out[:, 0]  # Take first temporal slice -> (C, H, W)\n",
        "                                else:  # (B, C, H, W)\n",
        "                                    out = out[0]  # Take first batch -> (C, H, W)\n",
        "                        elif out.dim() == 3:\n",
        "                            # Could be (C, H, W) or (H, W, C)\n",
        "                            if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                        proc_list.append(out)\n",
        "                    else:\n",
        "                        # Convert numpy to tensor if needed\n",
        "                        if isinstance(out, np.ndarray):\n",
        "                            out = torch.from_numpy(out)\n",
        "                            # Ensure (C, H, W) format\n",
        "                            if out.dim() == 3:\n",
        "                                if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                    out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                        proc_list.append(out)\n",
        "                else:\n",
        "                    proc_list.append(frame)\n",
        "\n",
        "            # Stack processed frames: (T, C, H, W) -> (1, C, T, H, W)\n",
        "            if len(proc_list) > 0:\n",
        "                # Verify all frames have shape (C=3, H, W)\n",
        "                for i, frame in enumerate(proc_list):\n",
        "                    if frame.dim() != 3 or frame.shape[0] != 3:\n",
        "                        raise RuntimeError(f\"Frame {i} has unexpected shape: {frame.shape}, expected (3, H, W)\")\n",
        "\n",
        "                # Each item in proc_list is (C, H, W), stack to (T, C, H, W)\n",
        "                stacked = torch.stack(proc_list, dim=0)  # (T, C, H, W)\n",
        "                # Rearrange to (1, C, T, H, W)\n",
        "                proc = stacked.permute(1, 0, 2, 3).unsqueeze(0).contiguous()  # (1, C, T, H, W)\n",
        "                proc = proc.to(self.device)  # (1, C, T, 256, 256)\n",
        "\n",
        "                # Encode\n",
        "                out = self.vjepa_model(proc)\n",
        "\n",
        "                # Extract token features\n",
        "                if isinstance(out, (list, tuple)) and len(out) > 0:\n",
        "                    tokens = out[0]\n",
        "                elif isinstance(out, dict):\n",
        "                    tokens = out.get('x', list(out.values())[0])\n",
        "                else:\n",
        "                    tokens = out\n",
        "\n",
        "                # Get center temporal slice and spatial dimension\n",
        "                if tokens.dim() == 4:  # (B, T', N, D)\n",
        "                    b, T, N, D = tokens.shape\n",
        "                    tokens = tokens[:, T // 2]  # Center frame\n",
        "                elif tokens.dim() == 3:  # (B, N, D)\n",
        "                    b, N, D = tokens.shape\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Unexpected V-JEPA output shape: {tokens.shape}\")\n",
        "\n",
        "                # Compute spatial grid\n",
        "                Ht = Wt = int(np.sqrt(N))\n",
        "                self.token_dim = D\n",
        "                self.spatial_tokens = Ht * Wt\n",
        "                self.flat_feature_dim = D  # Will pool spatially for policy\n",
        "\n",
        "                print(f\"V-JEPA2 feature dims: token_dim={D}, spatial_grid={Ht}x{Wt}\")\n",
        "            else:\n",
        "                # Fallback dimensions\n",
        "                self.token_dim = 1024\n",
        "                self.spatial_tokens = 256\n",
        "                self.flat_feature_dim = 1024\n",
        "                print(\"Using default V-JEPA2 feature dims\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, obs, obs_history=None, actions=None):\n",
        "        \"\"\"\n",
        "        obs: (3, 64, 64) normalized to [0,1] in C,H,W format (current frame)\n",
        "        obs_history: Optional list of previous observations. If provided, should be a list\n",
        "                    of T_context-1 frames. Current obs will be appended to make T_context total.\n",
        "                    If None, will use current frame repeated (fallback).\n",
        "        actions: Optional action history for AC predictor.\n",
        "                If use_ac=True, should be tensor of shape (T_context,) or (T_context, action_dim)\n",
        "                If None and use_ac=True, uses zero actions (no-op)\n",
        "        Returns: flattened features suitable for policy network\n",
        "        \"\"\"\n",
        "        T_context = 4  # Match temporal context used in training\n",
        "\n",
        "        # 1. Prepare observation sequence\n",
        "        if obs_history is not None and len(obs_history) > 0:\n",
        "            # Use actual temporal history: [oldest, ..., newest]\n",
        "            # Append current observation\n",
        "            obs_list = obs_history + [obs]\n",
        "\n",
        "            # Ensure we have exactly T_context frames\n",
        "            if len(obs_list) > T_context:\n",
        "                obs_list = obs_list[-T_context:]  # Take most recent T_context\n",
        "            elif len(obs_list) < T_context:\n",
        "                # Pad from the left with oldest frame\n",
        "                oldest = obs_list[0]\n",
        "                obs_list = [oldest] * (T_context - len(obs_list)) + obs_list\n",
        "\n",
        "            # Convert list to tensor and resize each frame\n",
        "            obs_tensors = []\n",
        "            for frame in obs_list:\n",
        "                frame_tensor = torch.tensor(frame, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                frame_resized = F.interpolate(frame_tensor, size=(256, 256), mode='nearest')\n",
        "                obs_tensors.append(frame_resized.squeeze(0))\n",
        "\n",
        "            # Stack to (T, C, H, W)\n",
        "            obs_resized_seq = torch.stack(obs_tensors, dim=0)\n",
        "        else:\n",
        "            # Fallback: resize current frame and repeat (for backward compatibility)\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            obs_resized = F.interpolate(obs_tensor, size=(256, 256), mode='nearest').squeeze(0)\n",
        "            obs_resized_seq = obs_resized.repeat(T_context, 1, 1, 1)  # (T, C, H, W)\n",
        "\n",
        "        # Rearrange to (1, C, T, H, W) for V-JEPA\n",
        "        obs_seq = obs_resized_seq.unsqueeze(0).permute(0, 2, 1, 3, 4)  # (1, C, T, H, W)\n",
        "\n",
        "        # Store T_context for later use in AC predictor\n",
        "        self._current_T_context = T_context\n",
        "\n",
        "        # 3. Preprocess through V-JEPA preprocessor\n",
        "        proc_list = []\n",
        "        for t_idx in range(T_context):\n",
        "            frame = obs_seq[0, :, t_idx]  # Get (C, H, W) frame\n",
        "            if self.preprocessor is not None:\n",
        "                # Convert to (H, W, C) numpy array for preprocessor\n",
        "                frame_np = frame.cpu().permute(1, 2, 0).numpy()  # (H, W, C)\n",
        "                # Ensure values are in valid range [0, 1]\n",
        "                frame_np = np.clip(frame_np, 0, 1)\n",
        "                out = self.preprocessor([frame_np])\n",
        "                out = out[0] if isinstance(out, (list, tuple)) else out\n",
        "                if isinstance(out, torch.Tensor):\n",
        "                    # Ensure output is (C, H, W)\n",
        "                    if out.dim() == 4:  # (C, T, H, W) or (B, C, H, W)\n",
        "                        if out.shape[1] == 3 or out.shape[0] == 3:\n",
        "                            # Likely (C, T, H, W) or (B, C, H, W)\n",
        "                            if out.shape[0] == 3:  # (C, T, H, W)\n",
        "                                out = out[:, 0]  # Take first temporal slice -> (C, H, W)\n",
        "                            else:  # (B, C, H, W)\n",
        "                                out = out[0]  # Take first batch -> (C, H, W)\n",
        "                    elif out.dim() == 3:\n",
        "                        # Could be (C, H, W) or (H, W, C)\n",
        "                        if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                            out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                    proc_list.append(out)\n",
        "                else:\n",
        "                    # Convert numpy to tensor if needed\n",
        "                    if isinstance(out, np.ndarray):\n",
        "                        out = torch.from_numpy(out)\n",
        "                        # Ensure (C, H, W) format\n",
        "                        if out.dim() == 3:\n",
        "                            if out.shape[0] != 3 and out.shape[2] == 3:\n",
        "                                out = out.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "                    proc_list.append(out)\n",
        "            else:\n",
        "                proc_list.append(frame)\n",
        "\n",
        "        # Stack processed frames: (T, C, H, W) -> (1, C, T, H, W)\n",
        "        # Verify all frames have shape (C=3, H, W)\n",
        "        for i, frame in enumerate(proc_list):\n",
        "            if frame.dim() != 3 or frame.shape[0] != 3:\n",
        "                raise RuntimeError(f\"Frame {i} has unexpected shape: {frame.shape}, expected (3, H, W)\")\n",
        "\n",
        "        # Each item in proc_list is (C, H, W), stack to (T, C, H, W)\n",
        "        stacked = torch.stack(proc_list, dim=0)  # (T, C, H, W)\n",
        "        # Rearrange to (1, C, T, H, W)\n",
        "        proc = stacked.permute(1, 0, 2, 3).unsqueeze(0).contiguous()  # (1, C, T, H, W)\n",
        "        proc = proc.to(self.device)  # (1, C, T, 256, 256)\n",
        "\n",
        "        # 4. Encode through V-JEPA (with optional action conditioning)\n",
        "        if self.embed_actions_in_encoder and actions is not None and self.action_embedding is not None:\n",
        "            # Option 1: Embed actions and add to visual features BEFORE encoding\n",
        "            # This makes the encoder action-aware from the start\n",
        "            # Convert actions to embeddings\n",
        "            actions_tensor = torch.tensor(actions, device=self.device, dtype=torch.long)\n",
        "            if actions_tensor.dim() == 1:\n",
        "                # (T,) -> embed -> (T, D)\n",
        "                action_embeds = self.action_embedding(actions_tensor)  # (T, 1024)\n",
        "                # Broadcast to match spatial tokens: we'll add this after encoding\n",
        "                # For now, we encode normally and add action info after\n",
        "                out = self.vjepa_model(proc)\n",
        "            else:\n",
        "                # Already embedded or one-hot\n",
        "                out = self.vjepa_model(proc)\n",
        "        else:\n",
        "            # Standard encoding without action embedding\n",
        "            out = self.vjepa_model(proc)\n",
        "\n",
        "        # 5. Extract features\n",
        "        if isinstance(out, (list, tuple)) and len(out) > 0:\n",
        "            tokens = out[0]\n",
        "        elif isinstance(out, dict):\n",
        "            tokens = out.get('x', list(out.values())[0])\n",
        "        else:\n",
        "            tokens = out\n",
        "\n",
        "        # Get center temporal slice\n",
        "        if tokens.dim() == 4:  # (B, T', N, D)\n",
        "            tokens = tokens[:, tokens.size(1) // 2]  # Center frame\n",
        "\n",
        "        # tokens is now (B, N, D) where N = Ht * Wt\n",
        "\n",
        "        # 5b. Optionally inject action information into tokens after encoding\n",
        "        # This adds action context to visual tokens (alternative to AC predictor)\n",
        "        if self.embed_actions_in_encoder and actions is not None and self.action_embedding is not None:\n",
        "            # Get action embedding for most recent action\n",
        "            if isinstance(actions, (list, tuple)):\n",
        "                recent_action = actions[-1] if len(actions) > 0 else 0\n",
        "            else:\n",
        "                recent_action = actions[-1].item() if hasattr(actions[-1], 'item') else actions[-1]\n",
        "\n",
        "            action_embed = self.action_embedding(torch.tensor([recent_action], device=self.device))  # (1, D)\n",
        "            # Add action embedding to all spatial tokens (broadcast)\n",
        "            # This makes tokens action-aware: tokens += action_embed\n",
        "            tokens = tokens + action_embed.unsqueeze(1)  # (B, N, D) + (1, 1, D) -> (B, N, D)\n",
        "\n",
        "        # 6. Apply action-conditioned predictor if available\n",
        "        if self.use_ac and self.ac_predictor is not None:\n",
        "            # Get T_context from the current processing\n",
        "            T_context = getattr(self, '_current_T_context', 4)\n",
        "\n",
        "            # Prepare actions for predictor\n",
        "            if actions is None:\n",
        "                # Use zero/no-op action for each timestep\n",
        "                if self.action_dim is not None:\n",
        "                    actions = torch.zeros(T_context, self.action_dim, device=self.device)\n",
        "                else:\n",
        "                    actions = torch.zeros(T_context, dtype=torch.long, device=self.device)\n",
        "            else:\n",
        "                # Ensure actions are on correct device and have correct shape\n",
        "                actions = torch.tensor(actions, device=self.device)\n",
        "                if actions.dim() == 1:\n",
        "                    # Convert to one-hot if needed\n",
        "                    if self.action_dim is not None and actions.dtype != torch.float32:\n",
        "                        actions_onehot = torch.zeros(len(actions), self.action_dim, device=self.device)\n",
        "                        actions_onehot.scatter_(1, actions.long().unsqueeze(1), 1.0)\n",
        "                        actions = actions_onehot\n",
        "                elif actions.dim() == 2 and actions.shape[1] != self.action_dim:\n",
        "                    # Might be one-hot already, check shape matches\n",
        "                    if actions.shape[1] == self.action_dim:\n",
        "                        pass  # Already correct\n",
        "                    else:\n",
        "                        # Assume it's action indices, convert to one-hot\n",
        "                        if self.action_dim is not None:\n",
        "                            actions_onehot = torch.zeros(len(actions), self.action_dim, device=self.device)\n",
        "                            actions_onehot.scatter_(1, actions.long().unsqueeze(1), 1.0)\n",
        "                            actions = actions_onehot\n",
        "\n",
        "            # Apply AC predictor: requires tokens, states, and actions\n",
        "            # For Atari, we don't have explicit robot states, so we use tokens as states\n",
        "            # Based on V-JEPA2-AC paper, states typically represent robot state (end-effector, joints)\n",
        "            # For visual-only environments like Atari, visual tokens serve as the state representation\n",
        "\n",
        "            try:\n",
        "                # Prepare tokens with temporal dimension if needed\n",
        "                if tokens.dim() == 3:  # (B, N, D)\n",
        "                    tokens_with_time = tokens.unsqueeze(1)  # (B, 1, N, D)\n",
        "                else:\n",
        "                    tokens_with_time = tokens\n",
        "\n",
        "                # Prepare actions with batch dimension if needed\n",
        "                if actions.dim() == 2:  # (T, action_dim) or (T,)\n",
        "                    actions_with_batch = actions.unsqueeze(0)  # (1, T, action_dim) or (1, T)\n",
        "                else:\n",
        "                    actions_with_batch = actions\n",
        "\n",
        "                # For Atari: use tokens as states (visual state representation)\n",
        "                # In robot tasks, states would be end-effector positions, joint angles, etc.\n",
        "                # Here, the spatial tokens encode the visual state of the game\n",
        "                states = tokens_with_time\n",
        "\n",
        "                # Try signature: predictor(tokens, states, actions) - common order in robotics\n",
        "                try:\n",
        "                    conditioned_tokens = self.ac_predictor(tokens_with_time, states, actions_with_batch)\n",
        "                except TypeError:\n",
        "                    # Try alternative order: predictor(tokens, actions, states)\n",
        "                    conditioned_tokens = self.ac_predictor(tokens_with_time, actions_with_batch, states)\n",
        "\n",
        "                # Handle output dimensions\n",
        "                if conditioned_tokens.dim() == 4:  # (B, T, N, D)\n",
        "                    conditioned_tokens = conditioned_tokens[:, 0]  # Take first timestep -> (B, N, D)\n",
        "                elif conditioned_tokens.dim() == 3:  # (B, N, D)\n",
        "                    pass  # Already correct\n",
        "\n",
        "                tokens = conditioned_tokens\n",
        "            except Exception as e:\n",
        "                # Fallback: try different argument patterns\n",
        "                try:\n",
        "                    # Try: predictor(states, actions) - some implementations combine tokens+states\n",
        "                    if tokens.dim() == 3:\n",
        "                        tokens_with_time = tokens.unsqueeze(1)\n",
        "                    else:\n",
        "                        tokens_with_time = tokens\n",
        "                    if actions.dim() == 2:\n",
        "                        actions_with_batch = actions.unsqueeze(0)\n",
        "                    else:\n",
        "                        actions_with_batch = actions\n",
        "                    states = tokens_with_time\n",
        "                    conditioned_tokens = self.ac_predictor(states, actions_with_batch)\n",
        "                    if conditioned_tokens.dim() == 4:\n",
        "                        conditioned_tokens = conditioned_tokens[:, 0]\n",
        "                    tokens = conditioned_tokens\n",
        "                except Exception as e2:\n",
        "                    # Final fallback: try minimal signature or disable AC\n",
        "                    try:\n",
        "                        conditioned_tokens = self.ac_predictor(tokens_with_time, actions_with_batch)\n",
        "                        if conditioned_tokens.dim() == 4:\n",
        "                            conditioned_tokens = conditioned_tokens[:, 0]\n",
        "                        tokens = conditioned_tokens\n",
        "                    except Exception as e3:\n",
        "                        print(f\"Warning: Could not apply AC predictor: {e3}\")\n",
        "                        print(f\"Note: V-JEPA2-AC was designed for robot control with explicit states.\")\n",
        "                        print(f\"For Atari, consider disabling AC (USE_AC=0) or use visual tokens as states.\")\n",
        "                        print(f\"Continuing with unconditioned features.\")\n",
        "\n",
        "        # 7. Pool spatially for policy: average pool or flatten\n",
        "        features = tokens.mean(dim=1)  # (B, D) - average pooling over spatial tokens\n",
        "\n",
        "        return features.squeeze(0)  # (D,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNIA-d8Y_Bq6"
      },
      "source": [
        "## Policy Network and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kauVQ7sK_Bq6",
        "outputId": "d77d4fe8-2f47-43e8-f606-6dec044eef57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V-JEPA2 feature dims: token_dim=1024, spatial_grid=22x22\n",
            "Encoder feature dim: 1024\n",
            "Action dim: 18\n",
            "Using action-conditioned predictor (AC): False\n",
            "Using action embedding in encoder: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        }
      ],
      "source": [
        "class LinearPolicy(nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# Initialize feature extractor and policy\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Option: Embed actions directly into visual tokens (alternative to AC predictor)\n",
        "# This adds action information to tokens after encoding\n",
        "# Set embed_actions_in_encoder=True to enable this\n",
        "embed_actions_in_encoder=True\n",
        "EMBED_ACTIONS = bool(int(os.environ.get(\"VJEPA_EMBED_ACTIONS\", \"1\")))  # Default: False\n",
        "\n",
        "encoder = VJEPA2FeatureExtractor(\n",
        "    vjepa_model,\n",
        "    preprocessor,\n",
        "    device,\n",
        "    ac_predictor=vjepa_ac_predictor,\n",
        "    action_dim=action_dim,\n",
        "    embed_actions_in_encoder=EMBED_ACTIONS\n",
        ").to(device)\n",
        "latent_dim = encoder.flat_feature_dim\n",
        "\n",
        "policy = LinearPolicy(latent_dim, action_dim).to(device)\n",
        "\n",
        "# Optimizer: include action embedding if enabled\n",
        "if encoder.embed_actions_in_encoder and encoder.action_embedding is not None:\n",
        "    # Train both policy and action embedding together\n",
        "    optimizer = torch.optim.Adam(\n",
        "        list(policy.parameters()) + list(encoder.action_embedding.parameters()),\n",
        "        lr=1e-4\n",
        "    )\n",
        "    print(f\"Optimizer includes {sum(p.numel() for p in encoder.action_embedding.parameters())} action embedding parameters\")\n",
        "else:\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)\n",
        "\n",
        "print(f\"Encoder feature dim: {latent_dim}\")\n",
        "print(f\"Action dim: {action_dim}\")\n",
        "print(f\"Using action-conditioned predictor (AC): {encoder.use_ac}\")\n",
        "print(f\"Using action embedding in encoder: {encoder.embed_actions_in_encoder}\")\n",
        "\n",
        "if encoder.embed_actions_in_encoder and encoder.action_embedding is not None:\n",
        "    print(f\"Action embedding parameters will be learned during policy training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEJaWrDq_Bq6"
      },
      "source": [
        "## Policy Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiERAlJd_Bq6"
      },
      "outputs": [],
      "source": [
        "def run_episode(env, encoder, policy, gamma=0.99, seed=None, save_video=False):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        env.reset(seed=seed)\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    log_probs, rewards = [], []\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    raw_frames = []\n",
        "\n",
        "    # Track action and observation history for temporal context\n",
        "    T_context = 8\n",
        "    action_history = []\n",
        "    obs_history = []  # Store previous observations for temporal sequence\n",
        "\n",
        "    while not done:\n",
        "        frame = (obs * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "        raw_frames.append(frame)\n",
        "\n",
        "        # Prepare action history for AC predictor\n",
        "        # Use last T_context actions (pad with no-op if needed)\n",
        "        if encoder.use_ac:\n",
        "            if len(action_history) == 0:\n",
        "                # First step: use no-op actions (action 0 typically)\n",
        "                actions_for_encoder = [0] * T_context\n",
        "            else:\n",
        "                # Use last T_context actions, padding from the left if needed\n",
        "                recent_actions = action_history[-T_context:]\n",
        "                actions_for_encoder = [0] * (T_context - len(recent_actions)) + recent_actions\n",
        "        else:\n",
        "            actions_for_encoder = None\n",
        "\n",
        "        # Extract features using V-JEPA2 encoder\n",
        "        # Pass observation history for actual temporal sequences (not repeated frames)\n",
        "        z = encoder(obs, obs_history=obs_history, actions=actions_for_encoder)\n",
        "        logits = policy(z)\n",
        "\n",
        "        # Mask invalid actions if needed\n",
        "        valid = env.action_space.n\n",
        "        logits[valid:] = -1e9\n",
        "\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "\n",
        "        log_probs.append(dist.log_prob(action))\n",
        "\n",
        "        # Extract action as scalar (handle both batched and unbatched cases)\n",
        "        if action.dim() > 0:\n",
        "            action_scalar = action.squeeze().item()\n",
        "        else:\n",
        "            action_scalar = action.item()\n",
        "\n",
        "        # Store action and observation for next step's history\n",
        "        action_history.append(action_scalar)\n",
        "        obs_history.append(obs.copy())  # Store current observation before step\n",
        "\n",
        "        # Keep only last T_context-1 observations (current obs will be added next step)\n",
        "        if len(obs_history) >= T_context:\n",
        "            obs_history = obs_history[-(T_context-1):]\n",
        "\n",
        "        obs, reward, terminated, truncated, _ = env.step(action_scalar)\n",
        "        done = terminated or truncated\n",
        "        rewards.append(reward)\n",
        "        total_reward += reward\n",
        "\n",
        "    if save_video:\n",
        "        imageio.mimsave(\"vjepa2_raw.gif\", raw_frames, fps=15)\n",
        "        print(\"Saved: vjepa2_raw.gif\")\n",
        "\n",
        "    returns, G = [], 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
        "\n",
        "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
        "    return loss, float(total_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqxkrIyq_Bq6"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRqSCB2K_Bq6",
        "outputId": "004e01c9-75a4-4da9-e0fc-b492ffcc5236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 | Reward: 120.0\n",
            "Episode 2/10 | Reward: 230.0\n",
            "Episode 3/10 | Reward: 370.0\n",
            "Episode 4/10 | Reward: 140.0\n",
            "Episode 5/10 | Reward: 190.0\n",
            "Episode 6/10 | Reward: 170.0\n",
            "Episode 7/10 | Reward: 210.0\n",
            "Episode 8/10 | Reward: 210.0\n",
            "Episode 9/10 | Reward: 210.0\n",
            "Episode 10/10 | Reward: 140.0\n"
          ]
        }
      ],
      "source": [
        "num_episodes =\n",
        "reward_history = []\n",
        "\n",
        "for ep in range(num_episodes):\n",
        "    policy.train()\n",
        "    loss, total_reward = run_episode(env, encoder, policy, seed=ep)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    reward_history.append(float(total_reward))\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Episode {ep+1}/{num_episodes} | Reward: {total_reward:.1f}\")\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfcnrnCE_Bq6"
      },
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2TrYjGF_Bq6",
        "outputId": "4d26d7a4-5028-4b2c-89f0-222b7354c80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: unrecognized arguments: # or: %config InlineBackend.figure_format = 'retina'\n"
          ]
        }
      ],
      "source": [
        "# Remove/disable: matplotlib.use(\"Agg\")\n",
        "%matplotlib inline  # or: %config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel(\"Episode\"); plt.ylabel(\"Reward\")\n",
        "plt.title(\"V-JEPA2-based Linear Policy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm8QpLo6_Bq7",
        "outputId": "576a8aa5-7284-465e-a2d1-a8e747c6f05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (84, 84) to (96, 96) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved video to vjepa2_eval.mp4 | Episode return: 250.0\n"
          ]
        }
      ],
      "source": [
        "# === EVALUATE AND RECORD A GAME RUN ===\n",
        "import numpy as np, torch, imageio\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_and_record(env, encoder, policy, seed=0, save_path=\"vjepa2_eval.mp4\",\n",
        "                        fps=30, overlay_actions=True):\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "    policy.eval()\n",
        "\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "\n",
        "    # Match training-time context tracking\n",
        "    T_context = 4\n",
        "    action_history, obs_history = [], []\n",
        "    frames = []\n",
        "\n",
        "    # Helper to render RGB HxWxC uint8 (from the 64x64 normalized obs)\n",
        "    def obs_to_rgb(o):\n",
        "        return (np.clip(o, 0, 1) * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "\n",
        "    while not done:\n",
        "        # Prepare actions for encoder (if AC or action embedding is used, this is used)\n",
        "        if getattr(encoder, \"use_ac\", False):\n",
        "            if len(action_history) == 0:\n",
        "                actions_for_encoder = [0] * T_context\n",
        "            else:\n",
        "                recent = action_history[-T_context:]\n",
        "                actions_for_encoder = [0] * (T_context - len(recent)) + recent\n",
        "        else:\n",
        "            actions_for_encoder = None\n",
        "\n",
        "        # Encode (with temporal obs history)\n",
        "        z = encoder(obs, obs_history=obs_history, actions=actions_for_encoder)\n",
        "        logits = policy(z)\n",
        "\n",
        "        # Sample action\n",
        "        valid = env.action_space.n\n",
        "        logits[valid:] = -1e9\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        action_scalar = action.item()\n",
        "\n",
        "        # Render current frame (before step) with optional overlay\n",
        "        frame = obs_to_rgb(obs)\n",
        "        if overlay_actions:\n",
        "            img = Image.fromarray(frame)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            txt = f\"action={action_scalar}\"\n",
        "            draw.rectangle([0, 0, 140, 20], fill=(0, 0, 0, 160))\n",
        "            draw.text((5, 3), txt, fill=(255, 255, 255))\n",
        "            frame = np.array(img)\n",
        "        frames.append(frame)\n",
        "\n",
        "        # Update histories for next step\n",
        "        action_history.append(action_scalar)\n",
        "        obs_history.append(obs.copy())\n",
        "        if len(obs_history) >= T_context:\n",
        "            obs_history = obs_history[-(T_context-1):]\n",
        "\n",
        "        # Step env\n",
        "        obs, reward, terminated, truncated, _ = env.step(action_scalar)\n",
        "        total_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "    # Save video\n",
        "    with imageio.get_writer(save_path, fps=fps) as w:\n",
        "        for f in frames:\n",
        "            w.append_data(f)\n",
        "    print(f\"Saved video to {save_path} | Episode return: {total_reward:.1f}\")\n",
        "    return total_reward\n",
        "\n",
        "# Usage (re-use your existing env/encoder/policy from training)\n",
        "_ = evaluate_and_record(env, encoder, policy, seed=123, save_path=\"vjepa2_eval.mp4\", fps=30, overlay_actions=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8khgXka_Bq7"
      },
      "source": [
        "## Notes\n",
        "\n",
        "### Pipeline Overview\n",
        "\n",
        "This notebook implements a policy training pipeline using V-JEPA2 encoder as a frozen feature extractor:\n",
        "\n",
        "1. **Environment**: Atari Alien with 84×84 RGB observations\n",
        "2. **Encoder**: V-JEPA2 (ViT-L) - pre-trained, frozen\n",
        "3. **Feature Extraction**:\n",
        "   - Resize observations from 84×84 → 256×256 (V-JEPA expects 256×256)\n",
        "   - Create temporal context using the last T=4 frames (real history)\n",
        "   - Extract spatial tokens from the center temporal slice\n",
        "   - Average pool over spatial tokens to get flat features\n",
        "4. **Policy**: 2-layer MLP (linear → ReLU → linear)\n",
        "5. **Training**: REINFORCE with returns normalization\n",
        "\n",
        "### Key Differences from RSSM Pipeline\n",
        "\n",
        "- **Image Resolution**: RSSM uses 64×64; V-JEPA uses 84×84 → 256×256\n",
        "- **Temporal Context**: V-JEPA uses explicit multi-frame input; RSSM uses an RNN state\n",
        "- **Feature Extraction**: V-JEPA outputs spatial tokens that we pool; RSSM learns a global embedding\n",
        "- **Dynamics**: No learned dynamics here; encoder features are used directly\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "- Consider using the V-JEPA-AC variant for action-conditioned features\n",
        "- Explore different spatial pooling strategies (attention-weighted, max pooling, etc.)\n",
        "- Add a dynamics model on top of V-JEPA features (as done in RSSM)\n",
        "- Implement imagination-based training using predicted futures\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "txdTHhGXAxD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}