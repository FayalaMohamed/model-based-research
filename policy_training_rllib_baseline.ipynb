{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pong Policy Training Baseline with RLlib\n",
        "\n",
        "This notebook implements a baseline policy training pipeline using Ray RLlib, training directly on visual observations without any encoder. This serves as a comparison baseline for the V-JEPA2 and RSSM approaches.\n",
        "\n",
        "## Environment Notes\n",
        "\n",
        "- **Google Colab**: keep the install cell and default config (2 CPUs, 1 GPU)\n",
        "- **Local Apple Silicon (M1/M2)**: use the optimized configuration below (multi-core CPU, optional MPS acceleration)\n",
        "- Adjust the configuration variables if you run on a different machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'decoder (Python 3.13.2)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n decoder ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Install dependencies for Google Colab\n",
        "# Note: Colab comes with gymnasium, but we need ray[rllib] and ale-py\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game: PongNoFrameskip-v4\n",
            "Training iterations: 100\n",
            "Evaluation episodes: 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Register ALE environments\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Configuration\n",
        "GAME_ID = os.environ.get(\"ATARI_GAME\", \"PongNoFrameskip-v4\")\n",
        "NUM_TRAIN_ITERATIONS = int(os.environ.get(\"NUM_TRAIN_ITERATIONS\", \"100\"))\n",
        "EVAL_EPISODES = int(os.environ.get(\"NUM_EVAL_EPISODES\", \"5\"))\n",
        "\n",
        "print(f\"Game: {GAME_ID}\")\n",
        "print(f\"Training iterations: {NUM_TRAIN_ITERATIONS}\")\n",
        "print(f\"Evaluation episodes: {EVAL_EPISODES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Create the Pong environment with standard preprocessing (resize to 84x84, normalize).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment initialized successfully\n",
            "Observation shape: (3, 84, 84)\n",
            "Action space: Discrete(6)\n",
            "Number of actions: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is -964179207\n"
          ]
        }
      ],
      "source": [
        "def transform_obs(obs):\n",
        "    \"\"\"Transform observation to (C, H, W) format and normalize to [0, 1].\"\"\"\n",
        "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "# Create environment with wrappers\n",
        "env = gym.make(GAME_ID)\n",
        "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "\n",
        "env = gym.wrappers.TransformObservation(\n",
        "    env,\n",
        "    func=transform_obs,\n",
        "    observation_space=new_obs_space,\n",
        ")\n",
        "\n",
        "# Test environment\n",
        "obs, info = env.reset()\n",
        "assert obs.shape == (3, 84, 84), f\"Expected (3, 84, 84), got {obs.shape}\"\n",
        "print(f\"Environment initialized successfully\")\n",
        "print(f\"Observation shape: {obs.shape}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RLlib Training Setup\n",
        "\n",
        "Configure Ray RLlib to train a policy using PPO (Proximal Policy Optimization), which works well for Atari games.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected accelerator: mps\n",
            "Using up to 10 CPU cores for Ray\n",
            "Ray already initialized\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "# Check available accelerators (CUDA or Apple MPS)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    accelerator = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    accelerator = \"mps\"\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "\n",
        "print(f\"Detected accelerator: {accelerator}\")\n",
        "has_cuda = accelerator == \"cuda\"\n",
        "has_mps = accelerator == \"mps\"\n",
        "\n",
        "# Initialize Ray based on local resources\n",
        "available_cpus = max(2, os.cpu_count() or 2)\n",
        "print(f\"Using up to {available_cpus} CPU cores for Ray\")\n",
        "\n",
        "if not ray.is_initialized():\n",
        "    ray.init(\n",
        "        ignore_reinit_error=True,\n",
        "        num_cpus=available_cpus,\n",
        "        num_gpus=1 if has_cuda else 0,\n",
        "        object_store_memory=2_000_000_000,  # 2GB object store (macOS recommended limit)\n",
        "    )\n",
        "    print(\"Ray initialized\")\n",
        "else:\n",
        "    print(\"Ray already initialized\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment registered with RLlib\n"
          ]
        }
      ],
      "source": [
        "# Create a function to register and return the environment\n",
        "def env_creator(env_config):\n",
        "    \"\"\"Create and return the Pong environment.\"\"\"\n",
        "    import gymnasium as gym\n",
        "    import ale_py\n",
        "    from gymnasium import spaces\n",
        "    \n",
        "    gym.register_envs(ale_py)\n",
        "    \n",
        "    env = gym.make(GAME_ID)\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    \n",
        "    def transform_obs(obs):\n",
        "        obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "        return obs_t\n",
        "    \n",
        "    new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "    env = gym.wrappers.TransformObservation(\n",
        "        env,\n",
        "        func=transform_obs,\n",
        "        observation_space=new_obs_space,\n",
        "    )\n",
        "    \n",
        "    return env\n",
        "\n",
        "# Register the environment\n",
        "tune.register_env(\"pong_env\", env_creator)\n",
        "print(\"Environment registered with RLlib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO configuration (local-optimized):\n",
            "  Learning rate: 0.0003\n",
            "  Training batch size: 3000\n",
            "  Number of epochs: 10\n",
            "  Number of env runners: 4\n",
            "  Env per runner: 2\n",
            "  Total env processes: 8\n",
            "  Evaluation env runners: 0\n",
            "  CUDA available: False\n",
            "  MPS available: True\n",
            "  Gamma (discount): 0.99\n",
            "  Note: Minibatch size is auto-calculated by RLlib\n",
            "  Note: Evaluation disabled during training for speed\n"
          ]
        }
      ],
      "source": [
        "# Configure PPO algorithm\n",
        "# Local-friendly defaults: leverage multi-core CPU + optional GPU/MPS\n",
        "num_env_runners = int(os.environ.get(\"NUM_ENV_RUNNERS\", \"1\"))\n",
        "num_envs_per_env_runner = int(os.environ.get(\"NUM_ENVS_PER_RUNNER\", \"2\"))\n",
        "train_batch_size = int(os.environ.get(\"TRAIN_BATCH_SIZE\", \"6000\"))\n",
        "use_gpu = has_cuda  # RLlib only counts CUDA as GPU; MPS is handled within PyTorch\n",
        "\n",
        "# Note: minibatch_size is automatically calculated by RLlib from train_batch_size\n",
        "num_env_runners = 4\n",
        "num_envs_per_env_runner = 2\n",
        "train_batch_size = 3000\n",
        "num_epochs = 10\n",
        "\n",
        "# Guard: cap env runners if requested processes exceed CPU cores\n",
        "requested_env_processes = num_env_runners * num_envs_per_env_runner\n",
        "if requested_env_processes > available_cpus:\n",
        "    print(f\"⚠️ Requested {requested_env_processes} env processes but only {available_cpus} CPUs available.\")\n",
        "    num_env_runners = max(1, available_cpus // num_envs_per_env_runner)\n",
        "    requested_env_processes = num_env_runners * num_envs_per_env_runner\n",
        "    print(f\"  Capping num_env_runners to {num_env_runners} (total env processes = {requested_env_processes}).\")\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\"pong_env\")\n",
        "    .framework(\"torch\")\n",
        "    .training(\n",
        "        lr=3e-4,\n",
        "        train_batch_size=train_batch_size,\n",
        "        num_epochs=num_epochs,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.95,\n",
        "        clip_param=0.2,\n",
        "        entropy_coeff=0.01,\n",
        "        vf_loss_coeff=0.5,\n",
        "    )\n",
        "    .resources(num_gpus=0)\n",
        "    .env_runners(\n",
        "        num_env_runners=num_env_runners,\n",
        "        num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "        num_cpus_per_env_runner=1,\n",
        "        rollout_fragment_length=400,\n",
        "        batch_mode=\"complete_episodes\",\n",
        "    )\n",
        "    .evaluation(evaluation_interval=None)\n",
        ")\n",
        "\n",
        "print(\"PPO configuration (local-optimized):\")\n",
        "print(f\"  Learning rate: {config.lr}\")\n",
        "print(f\"  Training batch size: {config.train_batch_size}\")\n",
        "print(f\"  Number of epochs: {config.num_epochs}\")\n",
        "print(f\"  Number of env runners: {config.num_env_runners}\")\n",
        "print(f\"  Env per runner: {num_envs_per_env_runner}\")\n",
        "print(f\"  Total env processes: {config.num_env_runners * num_envs_per_env_runner}\")\n",
        "print(f\"  Evaluation env runners: {config.evaluation_num_env_runners}\")\n",
        "print(f\"  CUDA available: {use_gpu}\")\n",
        "print(f\"  MPS available: {has_mps}\")\n",
        "print(f\"  Gamma (discount): {config.gamma}\")\n",
        "print(\"  Note: Minibatch size is auto-calculated by RLlib\")\n",
        "print(\"  Note: Evaluation disabled during training for speed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics\n",
        "\n",
        "Check Ray cluster status and algorithm readiness before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ray cluster status:\n",
            "  Total CPUs: 10.0\n",
            "  Available CPUs: 6.0\n",
            "  Total GPUs: 0\n",
            "  Available GPUs: 0\n",
            "  Used CPUs: 4.0\n",
            "\n",
            "Algorithm configuration check:\n",
            "  Required CPUs (training): 4\n",
            "  Required CPUs (evaluation): 0\n",
            "  Total required: 4\n",
            "  ✓ Resource requirements are within available resources\n"
          ]
        }
      ],
      "source": [
        "# Check Ray cluster status\n",
        "import ray\n",
        "print(\"Ray cluster status:\")\n",
        "cluster_resources = {}\n",
        "available_resources = {}\n",
        "try:\n",
        "    cluster_resources = ray.cluster_resources()\n",
        "    available_resources = ray.available_resources()\n",
        "    print(f\"  Total CPUs: {cluster_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Available CPUs: {available_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Total GPUs: {cluster_resources.get('GPU', 0)}\")\n",
        "    print(f\"  Available GPUs: {available_resources.get('GPU', 0)}\")\n",
        "    used_cpus = cluster_resources.get('CPU', 0) - available_resources.get('CPU', 0)\n",
        "    print(f\"  Used CPUs: {used_cpus:.1f}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not get cluster resources: {e}\")\n",
        "\n",
        "print(\"\\nAlgorithm configuration check:\")\n",
        "try:\n",
        "    num_train_cpus = config.num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    num_eval_cpus = config.evaluation_num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    total_required = num_train_cpus + num_eval_cpus\n",
        "    print(f\"  Required CPUs (training): {num_train_cpus}\")\n",
        "    print(f\"  Required CPUs (evaluation): {num_eval_cpus}\")\n",
        "    print(f\"  Total required: {total_required}\")\n",
        "    \n",
        "    if cluster_resources:\n",
        "        available_cpus = cluster_resources.get('CPU', 0)\n",
        "        if total_required > available_cpus:\n",
        "            print(f\"  ⚠️  WARNING: Required CPUs ({total_required}) > Available CPUs ({available_cpus})\")\n",
        "            print(f\"     This may cause training to hang. Consider reducing num_env_runners or setting evaluation_num_env_runners=0\")\n",
        "        else:\n",
        "            print(f\"  ✓ Resource requirements are within available resources\")\n",
        "    else:\n",
        "        print(f\"  (Could not verify against available resources)\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not check configuration: {e}\")\n",
        "    print(f\"  Config attributes: num_env_runners={config.num_env_runners}, evaluation_num_env_runners={config.evaluation_num_env_runners}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Algorithm\n",
        "\n",
        "**Expected time: 1-3 minutes**\n",
        "\n",
        "If this takes longer than 5 minutes, it may be stuck. Check:\n",
        "1. Ray cluster status (diagnostics cell above)\n",
        "2. No CPU resource warnings\n",
        "3. If stuck, restart Ray and rebuild\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Train the policy using RLlib's PPO algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building algorithm (this may take 1-3 minutes on first run)...\n",
            "Components being initialized:\n",
            "  - Neural network models\n",
            "  - Environment runners\n",
            "  - Learner group\n",
            "  - ALE environment (Pong ROM)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:526: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Game console created:\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Cart Name: Video Olympics (1978) (Atari)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Display Format:  AUTO-DETECT ==> NTSC\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   ROM Size:        2048\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Running ROM file...\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Random seed is 367494594\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Game console created:\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Cart Name: Video Olympics (1978) (Atari)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Display Format:  AUTO-DETECT ==> NTSC\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   ROM Size:        2048\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m   Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Running ROM file...\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m Random seed is 1196766518\n",
            "\u001b[36m(SingleAgentEnvRunner pid=83531)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83531)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83534)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83534)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83539)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83539)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=83536)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
            "2025-11-15 01:56:08,669\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Algorithm built successfully in 3.8 seconds (0.1 minutes)\n",
            "Note: Could not access model directly: 'LearnerGroup' object has no attribute 'get_module'\n",
            "Algorithm is ready for training\n"
          ]
        }
      ],
      "source": [
        "from ray.rllib.algorithms.ppo import PPO\n",
        "import time\n",
        "\n",
        "# Build the algorithm (using new API method)\n",
        "print(\"Building algorithm (this may take 1-3 minutes on first run)...\")\n",
        "print(\"Components being initialized:\")\n",
        "print(\"  - Neural network models\")\n",
        "print(\"  - Environment runners\")\n",
        "print(\"  - Learner group\")\n",
        "print(\"  - ALE environment (Pong ROM)\")\n",
        "print()\n",
        "\n",
        "build_start = time.time()\n",
        "try:\n",
        "    algo = config.build_algo()\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✓ Algorithm built successfully in {build_time:.1f} seconds ({build_time/60:.1f} minutes)\")\n",
        "except Exception as e:\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✗ Algorithm build failed after {build_time:.1f} seconds\")\n",
        "    print(f\"Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Access the model - new API stack uses modules differently\n",
        "try:\n",
        "    # Try new API stack access\n",
        "    if hasattr(algo, 'learner_group'):\n",
        "        module = algo.learner_group.get_module()\n",
        "        print(f\"Policy module type: {type(module)}\")\n",
        "    elif hasattr(algo, 'get_policy'):\n",
        "        # Old API stack\n",
        "        policy = algo.get_policy()\n",
        "        print(f\"Policy network: {policy.model}\")\n",
        "    else:\n",
        "        print(\"Algorithm built, but model access method not found\")\n",
        "        print(f\"Algorithm type: {type(algo)}\")\n",
        "        print(f\"Available attributes: {[attr for attr in dir(algo) if not attr.startswith('_')]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not access model directly: {e}\")\n",
        "    print(\"Algorithm is ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 100 iterations...\n",
            "============================================================\n",
            "Note: First iteration may take longer to initialize environments\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(raylet)\u001b[0m Spilled 2291 MiB, 5 objects, write throughput 523 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First iteration diagnostics:\n",
            "  Result type: <class 'dict'>\n",
            "  Result keys (first 20): ['timers', 'env_runners', 'learners', 'num_training_step_calls_per_iteration', 'num_env_steps_sampled_lifetime', 'fault_tolerance', 'env_runner_group', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore']\n",
            "  Env runners metrics type: <class 'dict'>\n",
            "  Env runners keys: ['episode_duration_sec_mean', 'episode_return_max', 'env_to_module_connector', 'episode_len_mean', 'agent_episode_return_mean', 'module_to_env_connector', 'num_agent_steps_sampled_lifetime', 'env_step_timer', 'num_env_steps_sampled_lifetime', 'sample', 'env_to_module_sum_episodes_length_in', 'env_reset_timer', 'weights_seq_no', 'num_module_steps_sampled', 'episode_len_max', 'num_module_steps_sampled_lifetime', 'module_episode_return_mean', 'num_episodes', 'num_episodes_lifetime', 'rlmodule_inference_timer']\n",
            "    episode_duration_sec_mean: 15.932068713613262\n",
            "    episode_return_max: -18.0\n",
            "    env_to_module_connector: {'timers': {'connectors': {'add_time_dim_to_batch_and_zero_pad': np.float64(3.724617844645349e-06), 'add_observations_from_episodes_to_batch': np.float64(9.849046575292424e-06), 'batch_individual_items': np.float64(2.8624604921397955e-05), 'add_states_from_episodes_to_batch': np.float64(1.7793391734988105e-06), 'numpy_to_tensor': np.float64(3.4031231639228333e-05)}}, 'connector_pipeline_timer': np.float64(0.00011840391664274982)}\n",
            "    episode_len_mean: 3538.875\n",
            "    agent_episode_return_mean: {'default_agent': -20.625}\n",
            "    module_to_env_connector: {'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(1.6439149270901032e-06), 'tensor_to_numpy': np.float64(4.956268931200378e-05), 'listify_data_for_vector_env': np.float64(2.8396612529394682e-05), 'get_actions': np.float64(0.00016895691203348682), 'un_batch_to_individual_items': np.float64(1.7323441784216377e-05), 'normalize_and_clip_actions': np.float64(3.125031447239517e-05)}}, 'connector_pipeline_timer': np.float64(0.00034964212984977085)}\n",
            "    num_agent_steps_sampled_lifetime: {'default_agent': 30774.0}\n",
            "    env_step_timer: 0.0018795904369902274\n",
            "    num_env_steps_sampled_lifetime: 30774.0\n",
            "    sample: 17.468838645727374\n",
            "  Learners metrics keys: ['default_policy', '__all_modules__']\n",
            "\n",
            "  Searching for episode/reward metrics:\n",
            "\n",
            "  env_runners metrics:\n",
            "    episode_duration_sec_mean: 15.932068713613262\n",
            "    episode_return_max: -18.0\n",
            "    episode_len_mean: 3538.875\n",
            "    agent_episode_return_mean: {'default_agent': -20.625}\n",
            "    num_agent_steps_sampled_lifetime: {'default_agent': 30774.0}\n",
            "    env_step_timer: 0.0018795904369902274\n",
            "    num_env_steps_sampled_lifetime: 30774.0\n",
            "    env_to_module_sum_episodes_length_in: 4366.024099457759\n",
            "    num_module_steps_sampled: {'default_policy': 30774.0}\n",
            "    episode_len_max: 4287\n",
            "    num_module_steps_sampled_lifetime: {'default_policy': 30774.0}\n",
            "    module_episode_return_mean: {'default_policy': -20.625}\n",
            "    num_episodes: 8.0\n",
            "    num_episodes_lifetime: 8.0\n",
            "    env_to_module_sum_episodes_length_out: 4366.024099457759\n",
            "    num_agent_steps_sampled: {'default_agent': 30774.0}\n",
            "    episode_return_min: -21.0\n",
            "    episode_len_min: 3056\n",
            "    num_env_steps_sampled: 30774.0\n",
            "    episode_return_mean: -20.625\n",
            "    num_env_steps_sampled_lifetime_throughput: nan\n",
            "\n",
            "  Total steps sampled: 30774.0\n",
            "\n",
            "Iteration 1/100 | Mean Reward: -20.62 | Min: -21.00 | Max: -18.00 | Episodes: 8.0 | Mean Len: 3538.9 | Time: 828.0s\n",
            "  Available metrics keys: []\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(raylet)\u001b[0m Spilled 4602 MiB, 9 objects, write throughput 673 MiB/s.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train for one iteration\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     result = \u001b[43malgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mERROR at iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:328\u001b[39m, in \u001b[36mTrainable.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    326\u001b[39m start = time.time()\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    330\u001b[39m     skipped = skip_exceptions(e)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:1241\u001b[39m, in \u001b[36mAlgorithm.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[32m   1237\u001b[39m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[32m   1238\u001b[39m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.enable_env_runner_and_connector_v2:\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m         train_results, train_iter_ctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1243\u001b[39m         (\n\u001b[32m   1244\u001b[39m             train_results,\n\u001b[32m   1245\u001b[39m             train_iter_ctx,\n\u001b[32m   1246\u001b[39m         ) = \u001b[38;5;28mself\u001b[39m._run_one_training_iteration_old_api_stack()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:3630\u001b[39m, in \u001b[36mAlgorithm._run_one_training_iteration\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3626\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics.log_time((TIMERS, TRAINING_STEP_TIMER)):\n\u001b[32m   3627\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m TimerAndPrometheusLogger(\n\u001b[32m   3628\u001b[39m         \u001b[38;5;28mself\u001b[39m._metrics_training_step_time\n\u001b[32m   3629\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m3630\u001b[39m         training_step_return_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3631\u001b[39m     has_run_once = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3633\u001b[39m \u001b[38;5;66;03m# On the new API stack, results should NOT be returned anymore as\u001b[39;00m\n\u001b[32m   3634\u001b[39m \u001b[38;5;66;03m# a dict, but purely logged through the `MetricsLogger` API. This\u001b[39;00m\n\u001b[32m   3635\u001b[39m \u001b[38;5;66;03m# way, we make sure to never miss a single stats/counter/timer\u001b[39;00m\n\u001b[32m   3636\u001b[39m \u001b[38;5;66;03m# when calling `self.training_step()` more than once within the same\u001b[39;00m\n\u001b[32m   3637\u001b[39m \u001b[38;5;66;03m# iteration.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:426\u001b[39m, in \u001b[36mPPO.training_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# Perform a learner update step on the collected episodes.\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics.log_time((TIMERS, LEARNER_UPDATE_TIMER)):\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     learner_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearner_group\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mNUM_ENV_STEPS_SAMPLED_LIFETIME\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpeek\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mENV_RUNNER_RESULTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ENV_STEPS_SAMPLED_LIFETIME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle_batch_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshuffle_batch_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m.metrics.aggregate(learner_results, key=LEARNER_RESULTS)\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Update weights - after learning on the local worker - on all remote\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/learner/learner_group.py:350\u001b[39m, in \u001b[36mLearnerGroup.update\u001b[39m\u001b[34m(self, batch, batches, batch_refs, episodes, episodes_refs, data_iterators, training_data, timesteps, async_update, return_state, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_state\u001b[39m\u001b[33m\"\u001b[39m] = return_state\n\u001b[32m    348\u001b[39m     \u001b[38;5;66;03m# Return the single Learner's update results.\u001b[39;00m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m     ]\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# Remote Learner actors' kwargs.\u001b[39;00m\n\u001b[32m    358\u001b[39m remote_call_kwargs = [\n\u001b[32m    359\u001b[39m     \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    360\u001b[39m         training_data=td_shard,\n\u001b[32m   (...)\u001b[39m\u001b[32m    374\u001b[39m     )\n\u001b[32m    375\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/learner/learner.py:1112\u001b[39m, in \u001b[36mLearner.update\u001b[39m\u001b[34m(self, batch, batches, batch_refs, episodes, episodes_refs, data_iterators, training_data, timesteps, num_total_minibatches, num_epochs, minibatch_size, shuffle_batch_per_epoch, _no_metrics_reduce, **kwargs)\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;66;03m# Make the actual in-graph/traced `_update` call. This should return\u001b[39;00m\n\u001b[32m   1110\u001b[39m \u001b[38;5;66;03m# all tensor values (no numpy).\u001b[39;00m\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TimerAndPrometheusLogger(\u001b[38;5;28mself\u001b[39m._metrics_learner_inner_update):\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     fwd_out, loss_per_module, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensor_minibatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy_batches\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# Ray metrics\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[38;5;28mself\u001b[39m._log_metrics(batch=tensor_minibatch)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/learner/torch/torch_learner.py:520\u001b[39m, in \u001b[36mTorchLearner._update\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._uncompiled_update(batch)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_possibly_compiled_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/learner/torch/torch_learner.py:154\u001b[39m, in \u001b[36mTorchLearner._uncompiled_update\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# TODO (sven): Causes weird cuda error when WandB is used.\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m#  Diagnosis thus far:\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m#  - All peek values during metrics.reduce are non-tensors.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[38;5;66;03m#    possible (learner, which performs the reduce() and learner thread, which\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m#    performs the logging of tensors into metrics logger).\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mself\u001b[39m._compute_off_policyness(batch)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m fwd_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m loss_per_module = \u001b[38;5;28mself\u001b[39m.compute_losses(fwd_out=fwd_out, batch=batch)\n\u001b[32m    156\u001b[39m gradients = \u001b[38;5;28mself\u001b[39m.compute_gradients(loss_per_module)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/rl_module/rl_module.py:649\u001b[39m, in \u001b[36mRLModule.forward_train\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.inference_only:\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    645\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCalling `forward_train` on an inference_only module is not allowed! \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    646\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSet the `inference_only=False` flag in the RLModuleSpec (or the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRLModule\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms constructor).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/rl_module/multi_rl_module.py:230\u001b[39m, in \u001b[36mMultiRLModule._forward_train\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;129m@override\u001b[39m(RLModule)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_train\u001b[39m(\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m, batch: Dict[\u001b[38;5;28mstr\u001b[39m, Any], **kwargs\n\u001b[32m    221\u001b[39m ) -> Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Dict[ModuleID, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[32m    222\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward-pass used before the loss computation (training).\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03m    Override this method only, if you need specific behavior and outputs for your\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m    By default, this calls the generic `self._forward()` method.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rl_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_forward_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/rl_module/multi_rl_module.py:231\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;129m@override\u001b[39m(RLModule)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_train\u001b[39m(\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m, batch: Dict[\u001b[38;5;28mstr\u001b[39m, Any], **kwargs\n\u001b[32m    221\u001b[39m ) -> Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Dict[ModuleID, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[32m    222\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward-pass used before the loss computation (training).\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03m    Override this method only, if you need specific behavior and outputs for your\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m    By default, this calls the generic `self._forward()` method.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m         mid: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rl_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_forward_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m mid \u001b[38;5;129;01min\u001b[39;00m batch.keys()\n\u001b[32m    233\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m    234\u001b[39m     }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py:43\u001b[39m, in \u001b[36mDefaultPPOTorchRLModule._forward_train\u001b[39m\u001b[34m(self, batch, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Train forward pass (keep embeddings for possible shared value func. call).\"\"\"\u001b[39;00m\n\u001b[32m     42\u001b[39m output = {}\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m encoder_outs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m output[Columns.EMBEDDINGS] = encoder_outs[ENCODER_OUT][CRITIC]\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Columns.STATE_OUT \u001b[38;5;129;01min\u001b[39;00m encoder_outs:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/models/torch/base.py:75\u001b[39m, in \u001b[36mTorchModel.forward\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m, inputs: Union[\u001b[38;5;28mdict\u001b[39m, TensorType], **kwargs\n\u001b[32m     63\u001b[39m ) -> Union[\u001b[38;5;28mdict\u001b[39m, TensorType]:\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the output of this model for the given input.\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33;03m    This method only makes sure that we have a spec-checked _forward() method.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m \u001b[33;03m        dict: The output tensors.\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/models/base.py:334\u001b[39m, in \u001b[36mActorCriticEncoder._forward\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    323\u001b[39m         ENCODER_OUT: {\n\u001b[32m    324\u001b[39m             ACTOR: encoder_outs[ENCODER_OUT],\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         }\n\u001b[32m    331\u001b[39m     }\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# Encoders should not modify inputs, so we can pass the same inputs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     actor_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.critic_encoder:\n\u001b[32m    336\u001b[39m         critic_out = \u001b[38;5;28mself\u001b[39m.critic_encoder(inputs, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/models/torch/base.py:75\u001b[39m, in \u001b[36mTorchModel.forward\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m, inputs: Union[\u001b[38;5;28mdict\u001b[39m, TensorType], **kwargs\n\u001b[32m     63\u001b[39m ) -> Union[\u001b[38;5;28mdict\u001b[39m, TensorType]:\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the output of this model for the given input.\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33;03m    This method only makes sure that we have a spec-checked _forward() method.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m \u001b[33;03m        dict: The output tensors.\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/models/torch/encoder.py:111\u001b[39m, in \u001b[36mTorchCNNEncoder._forward\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;129m@override\u001b[39m(Model)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mdict\u001b[39m, **kwargs) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {ENCODER_OUT: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mColumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOBS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ray/rllib/core/models/torch/primitives.py:312\u001b[39m, in \u001b[36mTorchCNN.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[32m    309\u001b[39m     \u001b[38;5;66;03m# Permute b/c data comes in as channels_last ([B, dim, dim, channels]) ->\u001b[39;00m\n\u001b[32m    310\u001b[39m     \u001b[38;5;66;03m# Convert to `channels_first` for torch:\u001b[39;00m\n\u001b[32m    311\u001b[39m     inputs = inputs.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     \u001b[38;5;66;03m# Permute back to `channels_last`.\u001b[39;00m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/modules/activation.py:133\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "training_history = {\n",
        "    \"episode_reward_mean\": [],\n",
        "    \"episode_reward_min\": [],\n",
        "    \"episode_reward_max\": [],\n",
        "    \"episode_len_mean\": [],\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {NUM_TRAIN_ITERATIONS} iterations...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Note: First iteration may take longer to initialize environments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(NUM_TRAIN_ITERATIONS):\n",
        "    iteration_start = time.time()\n",
        "    \n",
        "    # Train for one iteration\n",
        "    try:\n",
        "        result = algo.train()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR at iteration {i+1}: {e}\")\n",
        "        print(\"Training failed. Check resource constraints and Ray status.\")\n",
        "        break\n",
        "    \n",
        "    # Diagnostic check on first iteration: print all available metrics\n",
        "    if i == 0:\n",
        "        print(\"\\nFirst iteration diagnostics:\")\n",
        "        print(f\"  Result type: {type(result)}\")\n",
        "        print(f\"  Result keys (first 20): {list(result.keys())[:20]}\")\n",
        "        \n",
        "        # Check for environment runner metrics (new API - note: plural \"env_runners\")\n",
        "        if \"env_runners\" in result:\n",
        "            env_metrics = result[\"env_runners\"]\n",
        "            print(f\"  Env runners metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runners keys: {list(env_metrics.keys())[:20]}\")\n",
        "                # Print some values\n",
        "                for k in list(env_metrics.keys())[:10]:\n",
        "                    print(f\"    {k}: {env_metrics[k]}\")\n",
        "        elif \"env_runner\" in result:\n",
        "            env_metrics = result[\"env_runner\"]\n",
        "            print(f\"  Env runner (singular) metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runner keys: {list(env_metrics.keys())[:15]}\")\n",
        "        \n",
        "        # Check for learner metrics\n",
        "        if \"learners\" in result:\n",
        "            learner_metrics = result[\"learners\"]\n",
        "            print(f\"  Learners metrics keys: {list(learner_metrics.keys())[:15] if isinstance(learner_metrics, dict) else 'N/A'}\")\n",
        "        \n",
        "        # Look for episode metrics in different places\n",
        "        print(\"\\n  Searching for episode/reward metrics:\")\n",
        "        for key in result.keys():\n",
        "            if \"episode\" in key.lower() or \"reward\" in key.lower():\n",
        "                val = result[key]\n",
        "                print(f\"    {key}: {val}\")\n",
        "        \n",
        "        # Check env_runners dict for episode metrics\n",
        "        if \"env_runners\" in result and isinstance(result[\"env_runners\"], dict):\n",
        "            print(\"\\n  env_runners metrics:\")\n",
        "            for key in result[\"env_runners\"].keys():\n",
        "                if \"episode\" in key.lower() or \"reward\" in key.lower() or \"step\" in key.lower():\n",
        "                    val = result[\"env_runners\"][key]\n",
        "                    print(f\"    {key}: {val}\")\n",
        "            \n",
        "            # Also check num_env_steps_sampled to see if steps are being collected\n",
        "            if \"num_env_steps_sampled\" in result.get(\"env_runners\", {}):\n",
        "                print(f\"\\n  Total steps sampled: {result['env_runners']['num_env_steps_sampled']}\")\n",
        "        print()\n",
        "    \n",
        "    # Store metrics - new RLlib API uses different key names\n",
        "    # In env_runners: episode_return_* (not episode_reward_*)\n",
        "    # Also check agent_episode_return_mean for agent-specific rewards\n",
        "    env_runners_metrics = result.get(\"env_runners\", {})\n",
        "    env_runner_metrics = result.get(\"env_runner\", {})  # Also check singular for compatibility\n",
        "    \n",
        "    # Try env_runners first (new API), then env_runner, then top-level\n",
        "    if isinstance(env_runners_metrics, dict) and len(env_runners_metrics) > 0:\n",
        "        # New API uses \"episode_return\" not \"episode_reward\"\n",
        "        episode_reward_mean = env_runners_metrics.get(\"episode_return_mean\",\n",
        "                                                      env_runners_metrics.get(\"episode_reward_mean\",\n",
        "                                                      result.get(\"episode_return_mean\",\n",
        "                                                                 result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runners_metrics.get(\"episode_return_min\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_min\",\n",
        "                                                     result.get(\"episode_return_min\",\n",
        "                                                                result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runners_metrics.get(\"episode_return_max\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_max\",\n",
        "                                                     result.get(\"episode_return_max\",\n",
        "                                                                result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runners_metrics.get(\"episode_len_mean\",\n",
        "                                                   result.get(\"episode_len_mean\", 0))\n",
        "        \n",
        "        # If we got rewards from agent_episode_return_mean, use that (more accurate)\n",
        "        agent_returns = env_runners_metrics.get(\"agent_episode_return_mean\", {})\n",
        "        if isinstance(agent_returns, dict) and len(agent_returns) > 0:\n",
        "            # Use the first agent's return (typically \"default_agent\")\n",
        "            first_agent_return = list(agent_returns.values())[0]\n",
        "            if first_agent_return != 0 or episode_reward_mean == 0:\n",
        "                episode_reward_mean = first_agent_return\n",
        "    elif isinstance(env_runner_metrics, dict) and len(env_runner_metrics) > 0:\n",
        "        episode_reward_mean = env_runner_metrics.get(\"episode_return_mean\",\n",
        "                                                     env_runner_metrics.get(\"episode_reward_mean\",\n",
        "                                                     result.get(\"episode_return_mean\",\n",
        "                                                                result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runner_metrics.get(\"episode_return_min\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_min\",\n",
        "                                                    result.get(\"episode_return_min\",\n",
        "                                                               result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runner_metrics.get(\"episode_return_max\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_max\",\n",
        "                                                    result.get(\"episode_return_max\",\n",
        "                                                               result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runner_metrics.get(\"episode_len_mean\",\n",
        "                                                  result.get(\"episode_len_mean\", 0))\n",
        "    else:\n",
        "        # Fallback to top-level keys\n",
        "        episode_reward_mean = result.get(\"episode_return_mean\", result.get(\"episode_reward_mean\", 0))\n",
        "        episode_reward_min = result.get(\"episode_return_min\", result.get(\"episode_reward_min\", 0))\n",
        "        episode_reward_max = result.get(\"episode_return_max\", result.get(\"episode_reward_max\", 0))\n",
        "        episode_len_mean = result.get(\"episode_len_mean\", 0)\n",
        "    \n",
        "    training_history[\"episode_reward_mean\"].append(episode_reward_mean)\n",
        "    training_history[\"episode_reward_min\"].append(episode_reward_min)\n",
        "    training_history[\"episode_reward_max\"].append(episode_reward_max)\n",
        "    training_history[\"episode_len_mean\"].append(episode_len_mean)\n",
        "    \n",
        "    iteration_time = time.time() - iteration_start\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print progress (always print first iteration, then every 10)\n",
        "    if (i + 1) % 10 == 0 or i == 0:\n",
        "        # Use the same values we stored (already extracted from correct location)\n",
        "        mean_reward = episode_reward_mean\n",
        "        min_reward = episode_reward_min\n",
        "        max_reward = episode_reward_max\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        \n",
        "        # Check if we have other reward metrics\n",
        "        env_runner_metrics = result.get(\"env_runner\", {})\n",
        "        if isinstance(env_runner_metrics, dict):\n",
        "            actual_rewards = env_runner_metrics.get(\"episode_reward_mean\", mean_reward)\n",
        "        else:\n",
        "            actual_rewards = mean_reward\n",
        "        \n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Min: {min_reward:.2f} | Max: {max_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "        \n",
        "        # Diagnostic: Print all result keys on first iteration\n",
        "        if i == 0:\n",
        "            print(f\"  Available metrics keys: {[k for k in result.keys() if 'reward' in k.lower() or 'episode' in k.lower()][:10]}\")\n",
        "        \n",
        "    elif i < 5:  # Also print first 5 iterations to show it's working\n",
        "        mean_reward = episode_reward_mean\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Training complete! Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Final mean reward: {training_history['episode_reward_mean'][-1] if training_history['episode_reward_mean'] else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Episode rewards\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_history[\"episode_reward_mean\"], label=\"Mean Reward\", linewidth=2)\n",
        "plt.fill_between(\n",
        "    range(len(training_history[\"episode_reward_mean\"])),\n",
        "    training_history[\"episode_reward_min\"],\n",
        "    training_history[\"episode_reward_max\"],\n",
        "    alpha=0.3,\n",
        "    label=\"Min-Max Range\"\n",
        ")\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Reward\")\n",
        "plt.title(\"Training Progress - Episode Rewards\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Episode length\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_history[\"episode_len_mean\"], label=\"Mean Episode Length\", color=\"green\", linewidth=2)\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.title(\"Training Progress - Episode Length\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final statistics\n",
        "final_mean = training_history[\"episode_reward_mean\"][-1] if training_history[\"episode_reward_mean\"] else 0\n",
        "best_mean = max(training_history[\"episode_reward_mean\"]) if training_history[\"episode_reward_mean\"] else 0\n",
        "print(f\"\\nFinal mean reward: {final_mean:.2f}\")\n",
        "print(f\"Best mean reward: {best_mean:.2f}\")\n",
        "if \"Pong\" in GAME_ID:\n",
        "    print(f\"Pong theoretical max: 21 (winning 21-0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Run evaluation episodes to see how well the trained policy performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "eval_results = algo.evaluate()\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Mean episode reward: {eval_results.get('evaluation/episode_reward_mean', 'N/A'):.2f}\")\n",
        "print(f\"  Min episode reward: {eval_results.get('evaluation/episode_reward_min', 'N/A'):.2f}\")\n",
        "print(f\"  Max episode reward: {eval_results.get('evaluation/episode_reward_max', 'N/A'):.2f}\")\n",
        "print(f\"  Mean episode length: {eval_results.get('evaluation/episode_len_mean', 'N/A'):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual evaluation: run a few episodes and record video\n",
        "import imageio\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def evaluate_and_record(algo, env, num_episodes=3, save_video=True):\n",
        "    \"\"\"Run evaluation episodes and optionally save videos.\"\"\"\n",
        "    # Handle both old and new API stacks\n",
        "    def get_action(obs):\n",
        "        try:\n",
        "            # Try new API stack first (algo.compute_single_action)\n",
        "            if hasattr(algo, 'compute_single_action'):\n",
        "                result = algo.compute_single_action(obs, explore=False)\n",
        "                # Result might be action directly or tuple\n",
        "                return result[0] if isinstance(result, (list, tuple)) else result\n",
        "            # Fallback to old API stack\n",
        "            elif hasattr(algo, 'get_policy'):\n",
        "                policy = algo.get_policy()\n",
        "                return policy.compute_single_action(obs, explore=False)[0]\n",
        "            else:\n",
        "                raise AttributeError(\"No method found to compute actions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing action: {e}\")\n",
        "            # Return a random action as fallback\n",
        "            return env.action_space.sample()\n",
        "    \n",
        "    all_rewards = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        obs, info = env.reset(seed=1000 + ep)\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        frames = []\n",
        "        \n",
        "        while not done:\n",
        "            # Get action from policy (works with both API stacks)\n",
        "            action = get_action(obs)\n",
        "            \n",
        "            # Render frame\n",
        "            frame = (np.clip(obs, 0, 1) * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "            \n",
        "            # Add action overlay\n",
        "            img = Image.fromarray(frame)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            txt = f\"action={action}, reward={total_reward:.1f}\"\n",
        "            draw.rectangle([0, 0, 200, 20], fill=(0, 0, 0, 200))\n",
        "            draw.text((5, 3), txt, fill=(255, 255, 255))\n",
        "            frame = np.array(img)\n",
        "            frames.append(frame)\n",
        "            \n",
        "            # Step environment\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        \n",
        "        all_rewards.append(total_reward)\n",
        "        \n",
        "        # Save video\n",
        "        if save_video:\n",
        "            video_path = f\"rllib_eval_episode_{ep+1}.mp4\"\n",
        "            with imageio.get_writer(video_path, fps=30, macro_block_size=1) as writer:\n",
        "                for f in frames:\n",
        "                    writer.append_data(f)\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}, Saved to {video_path}\")\n",
        "        else:\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}\")\n",
        "    \n",
        "    print(f\"\\nEvaluation Summary:\")\n",
        "    print(f\"  Mean reward: {np.mean(all_rewards):.2f}\")\n",
        "    print(f\"  Std reward: {np.std(all_rewards):.2f}\")\n",
        "    print(f\"  Min reward: {np.min(all_rewards):.2f}\")\n",
        "    print(f\"  Max reward: {np.max(all_rewards):.2f}\")\n",
        "    \n",
        "    return all_rewards\n",
        "\n",
        "# Run evaluation\n",
        "eval_rewards = evaluate_and_record(algo, env, num_episodes=EVAL_EPISODES, save_video=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Load Model\n",
        "\n",
        "Save the trained model for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "checkpoint_path = algo.save(\"./rllib_pong_checkpoint\")\n",
        "print(f\"Model saved to: {checkpoint_path}\")\n",
        "\n",
        "# Example: Load the model later\n",
        "# from ray.rllib.algorithms.ppo import PPO\n",
        "# algo_loaded = PPO.from_checkpoint(checkpoint_path)\n",
        "# print(\"Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Google Colab Considerations\n",
        "\n",
        "- **GPU Runtime**: Enable GPU for faster training (Runtime > Change runtime type > GPU)\n",
        "- **Resource Limits**: Configuration uses 1 worker and reduced batch sizes to fit Colab's constraints\n",
        "- **Memory**: If you encounter OOM errors, reduce `train_batch_size` further\n",
        "- **Ray Shutdown**: Colab may require restarting the runtime if Ray doesn't shut down cleanly\n",
        "\n",
        "### Baseline Comparison\n",
        "\n",
        "This notebook provides a baseline for comparing against:\n",
        "- **V-JEPA2 encoder approach**: Uses pre-trained frozen encoder\n",
        "- **RSSM approach**: Uses learned dynamics model\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **No encoder**: Policy learns directly from raw visual observations\n",
        "- **RLlib framework**: Uses well-tested RL algorithms (PPO) with built-in optimizations\n",
        "- **Standard preprocessing**: 84x84 RGB frames, normalized to [0, 1]\n",
        "- **Colab-optimized**: Reduced workers and batch sizes for Colab's resource constraints\n",
        "\n",
        "### Configuration\n",
        "\n",
        "You can adjust training parameters via environment variables:\n",
        "- `NUM_TRAIN_ITERATIONS`: Number of training iterations (default: 100)\n",
        "- `NUM_EVAL_EPISODES`: Number of evaluation episodes (default: 5)\n",
        "- `ATARI_GAME`: Game environment (default: PongNoFrameskip-v4)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Compare training curves with V-JEPA2 and RSSM approaches\n",
        "- Experiment with different RLlib algorithms (IMPALA, A3C, etc.)\n",
        "- Tune hyperparameters for better performance\n",
        "- Add frame stacking for temporal information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "try:\n",
        "    env.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Shutdown Ray (recommended for Colab to free resources)\n",
        "# Note: In Colab, you may need to restart the runtime if Ray doesn't shut down cleanly\n",
        "try:\n",
        "    ray.shutdown()\n",
        "    print(\"Ray shut down successfully\")\n",
        "except:\n",
        "    print(\"Ray shutdown encountered an issue. You may need to restart the Colab runtime.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl-pong",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
