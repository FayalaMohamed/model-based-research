{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pong Policy Training Baseline with RLlib\n",
        "\n",
        "This notebook implements a baseline policy training pipeline using Ray RLlib, training directly on visual observations without any encoder. This serves as a comparison baseline for the V-JEPA2 and RSSM approaches.\n",
        "\n",
        "## Environment Notes\n",
        "\n",
        "- **Google Colab**: keep the install cell and default config (2 CPUs, 1 GPU)\n",
        "- **Local Apple Silicon (M1/M2)**: use the optimized configuration below (multi-core CPU, optional MPS acceleration)\n",
        "- Adjust the configuration variables if you run on a different machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'decoder (Python 3.13.2)' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n decoder ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Install dependencies for Google Colab\n",
        "# Note: Colab comes with gymnasium, but we need ray[rllib] and ale-py\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game: PongNoFrameskip-v4\n",
            "Training iterations: 1200\n",
            "Evaluation episodes: 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Register ALE environments\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Configuration\n",
        "GAME_ID = os.environ.get(\"ATARI_GAME\", \"PongNoFrameskip-v4\")\n",
        "NUM_TRAIN_ITERATIONS = int(os.environ.get(\"NUM_TRAIN_ITERATIONS\", \"1200\"))\n",
        "EVAL_EPISODES = int(os.environ.get(\"NUM_EVAL_EPISODES\", \"5\"))\n",
        "\n",
        "print(f\"Game: {GAME_ID}\")\n",
        "print(f\"Training iterations: {NUM_TRAIN_ITERATIONS}\")\n",
        "print(f\"Evaluation episodes: {EVAL_EPISODES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Create the Pong environment with standard preprocessing (resize to 84x84, normalize).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment initialized successfully\n",
            "Observation shape: (3, 84, 84)\n",
            "Action space: Discrete(6)\n",
            "Number of actions: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is 1768350623\n"
          ]
        }
      ],
      "source": [
        "def transform_obs(obs):\n",
        "    \"\"\"Transform observation to (C, H, W) format and normalize to [0, 1].\"\"\"\n",
        "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "# Create environment with wrappers\n",
        "env = gym.make(GAME_ID)\n",
        "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "\n",
        "env = gym.wrappers.TransformObservation(\n",
        "    env,\n",
        "    func=transform_obs,\n",
        "    observation_space=new_obs_space,\n",
        ")\n",
        "\n",
        "# Test environment\n",
        "obs, info = env.reset()\n",
        "assert obs.shape == (3, 84, 84), f\"Expected (3, 84, 84), got {obs.shape}\"\n",
        "print(f\"Environment initialized successfully\")\n",
        "print(f\"Observation shape: {obs.shape}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RLlib Training Setup\n",
        "\n",
        "Configure Ray RLlib to train a policy using PPO (Proximal Policy Optimization), which works well for Atari games.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected accelerator: mps\n",
            "Using up to 10 CPU cores for Ray\n",
            "Ray already initialized\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "# Check available accelerators (CUDA or Apple MPS)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    accelerator = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    accelerator = \"mps\"\n",
        "else:\n",
        "    accelerator = \"cpu\"\n",
        "\n",
        "print(f\"Detected accelerator: {accelerator}\")\n",
        "has_cuda = accelerator == \"cuda\"\n",
        "has_mps = accelerator == \"mps\"\n",
        "\n",
        "# Initialize Ray based on local resources\n",
        "available_cpus = max(2, os.cpu_count() or 2)\n",
        "print(f\"Using up to {available_cpus} CPU cores for Ray\")\n",
        "\n",
        "if not ray.is_initialized():\n",
        "    ray.init(\n",
        "        ignore_reinit_error=True,\n",
        "        num_cpus=available_cpus,\n",
        "        num_gpus=1 if has_cuda else 0,\n",
        "        object_store_memory=2_000_000_000,  # 2GB object store (macOS recommended limit)\n",
        "    )\n",
        "    print(\"Ray initialized\")\n",
        "else:\n",
        "    print(\"Ray already initialized\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment registered with RLlib\n"
          ]
        }
      ],
      "source": [
        "# Create a function to register and return the environment\n",
        "def env_creator(env_config):\n",
        "    \"\"\"Create and return the Pong environment.\"\"\"\n",
        "    import gymnasium as gym\n",
        "    import ale_py\n",
        "    from gymnasium import spaces\n",
        "    \n",
        "    gym.register_envs(ale_py)\n",
        "    \n",
        "    env = gym.make(GAME_ID)\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    \n",
        "    def transform_obs(obs):\n",
        "        obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "        return obs_t\n",
        "    \n",
        "    new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "    env = gym.wrappers.TransformObservation(\n",
        "        env,\n",
        "        func=transform_obs,\n",
        "        observation_space=new_obs_space,\n",
        "    )\n",
        "    \n",
        "    return env\n",
        "\n",
        "# Register the environment\n",
        "tune.register_env(\"pong_env\", env_creator)\n",
        "print(\"Environment registered with RLlib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPO configuration (local-optimized):\n",
            "  Learning rate: 0.0003\n",
            "  Training batch size: 2000\n",
            "  Number of epochs: 6\n",
            "  Number of env runners: 4\n",
            "  Env per runner: 2\n",
            "  Total env processes: 8\n",
            "  Evaluation env runners: 0\n",
            "  CUDA available: False\n",
            "  MPS available: True\n",
            "  Gamma (discount): 0.99\n",
            "  Note: Minibatch size is auto-calculated by RLlib\n",
            "  Note: Evaluation disabled during training for speed\n"
          ]
        }
      ],
      "source": [
        "# Configure PPO algorithm\n",
        "# Local-friendly defaults: leverage multi-core CPU + optional GPU/MPS\n",
        "num_env_runners = int(os.environ.get(\"NUM_ENV_RUNNERS\", \"1\"))\n",
        "num_envs_per_env_runner = int(os.environ.get(\"NUM_ENVS_PER_RUNNER\", \"2\"))\n",
        "train_batch_size = int(os.environ.get(\"TRAIN_BATCH_SIZE\", \"2000\"))\n",
        "use_gpu = has_cuda  # RLlib only counts CUDA as GPU; MPS is handled within PyTorch\n",
        "\n",
        "# Local defaults (can be overridden via env vars)\n",
        "num_env_runners = 4\n",
        "num_envs_per_env_runner = 2\n",
        "train_batch_size = 2000\n",
        "num_epochs = 6\n",
        "\n",
        "# Guard: cap env runners if requested processes exceed CPU cores\n",
        "requested_env_processes = num_env_runners * num_envs_per_env_runner\n",
        "max_env_processes = max(1, available_cpus - 2)  # leave headroom for learner/driver\n",
        "if requested_env_processes > max_env_processes:\n",
        "    print(f\"⚠️ Requested {requested_env_processes} env processes but only {available_cpus} CPUs available (reserving 2 for learner/driver).\")\n",
        "    num_env_runners = max(1, max_env_processes // num_envs_per_env_runner)\n",
        "    requested_env_processes = num_env_runners * num_envs_per_env_runner\n",
        "    print(f\"  Capping num_env_runners to {num_env_runners} (total env processes = {requested_env_processes}).\")\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\"pong_env\")\n",
        "    .framework(\"torch\")\n",
        "    .training(\n",
        "        lr=3e-4,\n",
        "        train_batch_size=train_batch_size,\n",
        "        num_epochs=num_epochs,\n",
        "        gamma=0.99,\n",
        "        lambda_=0.95,\n",
        "        clip_param=0.2,\n",
        "        entropy_coeff=0.01,\n",
        "        vf_loss_coeff=0.5,\n",
        "    )\n",
        "    .resources(num_gpus=0)\n",
        "    .env_runners(\n",
        "        num_env_runners=num_env_runners,\n",
        "        num_envs_per_env_runner=num_envs_per_env_runner,\n",
        "        num_cpus_per_env_runner=1,\n",
        "        rollout_fragment_length=\"auto\",\n",
        "        batch_mode=\"truncate_episodes\",\n",
        "    )\n",
        "    .evaluation(evaluation_interval=None)\n",
        ")\n",
        "\n",
        "print(\"PPO configuration (local-optimized):\")\n",
        "print(f\"  Learning rate: {config.lr}\")\n",
        "print(f\"  Training batch size: {config.train_batch_size}\")\n",
        "print(f\"  Number of epochs: {config.num_epochs}\")\n",
        "print(f\"  Number of env runners: {config.num_env_runners}\")\n",
        "print(f\"  Env per runner: {num_envs_per_env_runner}\")\n",
        "print(f\"  Total env processes: {config.num_env_runners * num_envs_per_env_runner}\")\n",
        "print(f\"  Evaluation env runners: {config.evaluation_num_env_runners}\")\n",
        "print(f\"  CUDA available: {use_gpu}\")\n",
        "print(f\"  MPS available: {has_mps}\")\n",
        "print(f\"  Gamma (discount): {config.gamma}\")\n",
        "print(\"  Note: Minibatch size is auto-calculated by RLlib\")\n",
        "print(\"  Note: Evaluation disabled during training for speed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics\n",
        "\n",
        "Check Ray cluster status and algorithm readiness before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ray cluster status:\n",
            "  Total CPUs: 10.0\n",
            "  Available CPUs: 6.0\n",
            "  Total GPUs: 0\n",
            "  Available GPUs: 0\n",
            "  Used CPUs: 4.0\n",
            "\n",
            "Algorithm configuration check:\n",
            "  Required CPUs (training): 4\n",
            "  Required CPUs (evaluation): 0\n",
            "  Total required: 4\n",
            "  ✓ Resource requirements are within available resources\n"
          ]
        }
      ],
      "source": [
        "# Check Ray cluster status\n",
        "import ray\n",
        "print(\"Ray cluster status:\")\n",
        "cluster_resources = {}\n",
        "available_resources = {}\n",
        "try:\n",
        "    cluster_resources = ray.cluster_resources()\n",
        "    available_resources = ray.available_resources()\n",
        "    print(f\"  Total CPUs: {cluster_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Available CPUs: {available_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Total GPUs: {cluster_resources.get('GPU', 0)}\")\n",
        "    print(f\"  Available GPUs: {available_resources.get('GPU', 0)}\")\n",
        "    used_cpus = cluster_resources.get('CPU', 0) - available_resources.get('CPU', 0)\n",
        "    print(f\"  Used CPUs: {used_cpus:.1f}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not get cluster resources: {e}\")\n",
        "\n",
        "print(\"\\nAlgorithm configuration check:\")\n",
        "try:\n",
        "    num_train_cpus = config.num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    num_eval_cpus = config.evaluation_num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    total_required = num_train_cpus + num_eval_cpus\n",
        "    print(f\"  Required CPUs (training): {num_train_cpus}\")\n",
        "    print(f\"  Required CPUs (evaluation): {num_eval_cpus}\")\n",
        "    print(f\"  Total required: {total_required}\")\n",
        "    \n",
        "    if cluster_resources:\n",
        "        available_cpus = cluster_resources.get('CPU', 0)\n",
        "        if total_required > available_cpus:\n",
        "            print(f\"  ⚠️  WARNING: Required CPUs ({total_required}) > Available CPUs ({available_cpus})\")\n",
        "            print(f\"     This may cause training to hang. Consider reducing num_env_runners or setting evaluation_num_env_runners=0\")\n",
        "        else:\n",
        "            print(f\"  ✓ Resource requirements are within available resources\")\n",
        "    else:\n",
        "        print(f\"  (Could not verify against available resources)\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not check configuration: {e}\")\n",
        "    print(f\"  Config attributes: num_env_runners={config.num_env_runners}, evaluation_num_env_runners={config.evaluation_num_env_runners}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Algorithm\n",
        "\n",
        "**Expected time: 1-3 minutes**\n",
        "\n",
        "If this takes longer than 5 minutes, it may be stuck. Check:\n",
        "1. Ray cluster status (diagnostics cell above)\n",
        "2. No CPU resource warnings\n",
        "3. If stuck, restart Ray and rebuild\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Train the policy using RLlib's PPO algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building algorithm (this may take 1-3 minutes on first run)...\n",
            "Components being initialized:\n",
            "  - Neural network models\n",
            "  - Environment runners\n",
            "  - Learner group\n",
            "  - ALE environment (Pong ROM)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m Game console created:\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Cart Name: Video Olympics (1978) (Atari)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Display Format:  AUTO-DETECT ==> NTSC\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   ROM Size:        2048\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m Running ROM file...\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m Random seed is -439344301\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m [Powered by Stella]\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m Game console created:\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Cart Name: Video Olympics (1978) (Atari)\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Display Format:  AUTO-DETECT ==> NTSC\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   ROM Size:        2048\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m   Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m Running ROM file...\n",
            "\u001b[36m(SingleAgentEnvRunner pid=13830)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13830)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13827)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13827)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13829)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13829)\u001b[0m \n",
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
            "2025-11-15 02:54:55,934\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Algorithm built successfully in 4.4 seconds (0.1 minutes)\n",
            "Note: Could not access model directly: 'LearnerGroup' object has no attribute 'get_module'\n",
            "Algorithm is ready for training\n"
          ]
        }
      ],
      "source": [
        "from ray.rllib.algorithms.ppo import PPO\n",
        "import time\n",
        "\n",
        "# Build the algorithm (using new API method)\n",
        "print(\"Building algorithm (this may take 1-3 minutes on first run)...\")\n",
        "print(\"Components being initialized:\")\n",
        "print(\"  - Neural network models\")\n",
        "print(\"  - Environment runners\")\n",
        "print(\"  - Learner group\")\n",
        "print(\"  - ALE environment (Pong ROM)\")\n",
        "print()\n",
        "\n",
        "build_start = time.time()\n",
        "try:\n",
        "    algo = config.build_algo()\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✓ Algorithm built successfully in {build_time:.1f} seconds ({build_time/60:.1f} minutes)\")\n",
        "except Exception as e:\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✗ Algorithm build failed after {build_time:.1f} seconds\")\n",
        "    print(f\"Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Access the model - new API stack uses modules differently\n",
        "try:\n",
        "    # Try new API stack access\n",
        "    if hasattr(algo, 'learner_group'):\n",
        "        module = algo.learner_group.get_module()\n",
        "        print(f\"Policy module type: {type(module)}\")\n",
        "    elif hasattr(algo, 'get_policy'):\n",
        "        # Old API stack\n",
        "        policy = algo.get_policy()\n",
        "        print(f\"Policy network: {policy.model}\")\n",
        "    else:\n",
        "        print(\"Algorithm built, but model access method not found\")\n",
        "        print(f\"Algorithm type: {type(algo)}\")\n",
        "        print(f\"Available attributes: {[attr for attr in dir(algo) if not attr.startswith('_')]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not access model directly: {e}\")\n",
        "    print(\"Algorithm is ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 1200 iterations...\n",
            "============================================================\n",
            "Note: First iteration may take longer to initialize environments\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(SingleAgentEnvRunner pid=13828)\u001b[0m [2025-11-15 02:55:22,204 E 13828 19662449] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First iteration diagnostics:\n",
            "  Result type: <class 'dict'>\n",
            "  Result keys (first 20): ['timers', 'env_runners', 'learners', 'num_training_step_calls_per_iteration', 'num_env_steps_sampled_lifetime', 'fault_tolerance', 'env_runner_group', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore']\n",
            "  Env runners metrics type: <class 'dict'>\n",
            "  Env runners keys: ['env_to_module_connector', 'module_to_env_connector', 'num_agent_steps_sampled_lifetime', 'env_step_timer', 'num_env_steps_sampled_lifetime', 'sample', 'env_to_module_sum_episodes_length_in', 'env_reset_timer', 'weights_seq_no', 'num_module_steps_sampled', 'num_module_steps_sampled_lifetime', 'num_episodes', 'num_episodes_lifetime', 'rlmodule_inference_timer', 'env_to_module_sum_episodes_length_out', 'num_agent_steps_sampled', 'num_env_steps_sampled', 'num_env_steps_sampled_lifetime_throughput']\n",
            "    env_to_module_connector: {'timers': {'connectors': {'add_time_dim_to_batch_and_zero_pad': np.float64(4.573765405435052e-06), 'add_observations_from_episodes_to_batch': np.float64(1.0476607718608519e-05), 'batch_individual_items': np.float64(3.112026611527705e-05), 'add_states_from_episodes_to_batch': np.float64(1.5793782765045833e-06), 'numpy_to_tensor': np.float64(3.5481676947299094e-05)}}, 'connector_pipeline_timer': np.float64(0.0001392657127703719)}\n",
            "    module_to_env_connector: {'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(1.6725673967998178e-06), 'tensor_to_numpy': np.float64(6.313968717562007e-05), 'listify_data_for_vector_env': np.float64(3.054395920354988e-05), 'get_actions': np.float64(0.00019424188517607266), 'un_batch_to_individual_items': np.float64(1.8191478227818673e-05), 'normalize_and_clip_actions': np.float64(4.148348040272227e-05)}}, 'connector_pipeline_timer': np.float64(0.0004081228124456886)}\n",
            "    num_agent_steps_sampled_lifetime: {'default_agent': 2000.0}\n",
            "    env_step_timer: 0.0010819639731096987\n",
            "    num_env_steps_sampled_lifetime: 2000.0\n",
            "    sample: 1.032917958771577\n",
            "    env_to_module_sum_episodes_length_in: 318.0495862001117\n",
            "    env_reset_timer: 0.009029687993461266\n",
            "    weights_seq_no: 0.0\n",
            "    num_module_steps_sampled: {'default_policy': 2000.0}\n",
            "  Learners metrics keys: ['default_policy', '__all_modules__']\n",
            "\n",
            "  Searching for episode/reward metrics:\n",
            "\n",
            "  env_runners metrics:\n",
            "    num_agent_steps_sampled_lifetime: {'default_agent': 2000.0}\n",
            "    env_step_timer: 0.0010819639731096987\n",
            "    num_env_steps_sampled_lifetime: 2000.0\n",
            "    env_to_module_sum_episodes_length_in: 318.0495862001117\n",
            "    num_module_steps_sampled: {'default_policy': 2000.0}\n",
            "    num_module_steps_sampled_lifetime: {'default_policy': 2000.0}\n",
            "    num_episodes: 0.0\n",
            "    num_episodes_lifetime: 0.0\n",
            "    env_to_module_sum_episodes_length_out: 318.0495862001117\n",
            "    num_agent_steps_sampled: {'default_agent': 2000.0}\n",
            "    num_env_steps_sampled: 2000.0\n",
            "    num_env_steps_sampled_lifetime_throughput: nan\n",
            "\n",
            "  Total steps sampled: 2000.0\n",
            "\n",
            "Iteration 1/1200 | Mean Reward: 0.00 | Min: 0.00 | Max: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 34.4s\n",
            "  Available metrics keys: []\n",
            "Iteration 2/1200 | Mean Reward: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 33.0s\n",
            "Iteration 3/1200 | Mean Reward: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 32.6s\n",
            "Iteration 4/1200 | Mean Reward: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 32.9s\n",
            "Iteration 5/1200 | Mean Reward: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 32.8s\n",
            "Iteration 10/1200 | Mean Reward: 0.00 | Min: 0.00 | Max: 0.00 | Episodes: 0.0 | Mean Len: 0.0 | Time: 32.9s\n",
            "Iteration 20/1200 | Mean Reward: -20.25 | Min: -21.00 | Max: -18.00 | Episodes: 0.0 | Mean Len: 3452.9 | Time: 32.5s\n",
            "Iteration 30/1200 | Mean Reward: -20.27 | Min: -21.00 | Max: -18.00 | Episodes: 1.0 | Mean Len: 3440.5 | Time: 32.6s\n",
            "Iteration 40/1200 | Mean Reward: -20.28 | Min: -21.00 | Max: -18.00 | Episodes: 0.0 | Mean Len: 3423.0 | Time: 32.3s\n",
            "Iteration 50/1200 | Mean Reward: -20.29 | Min: -21.00 | Max: -18.00 | Episodes: 0.0 | Mean Len: 3409.5 | Time: 32.7s\n",
            "Iteration 60/1200 | Mean Reward: -20.40 | Min: -21.00 | Max: -18.00 | Episodes: 1.0 | Mean Len: 3414.7 | Time: 65.5s\n",
            "Iteration 70/1200 | Mean Reward: -20.57 | Min: -21.00 | Max: -20.00 | Episodes: 2.0 | Mean Len: 3401.6 | Time: 36.8s\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "training_history = {\n",
        "    \"episode_reward_mean\": [],\n",
        "    \"episode_reward_min\": [],\n",
        "    \"episode_reward_max\": [],\n",
        "    \"episode_len_mean\": [],\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {NUM_TRAIN_ITERATIONS} iterations...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Note: First iteration may take longer to initialize environments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(NUM_TRAIN_ITERATIONS):\n",
        "    iteration_start = time.time()\n",
        "    \n",
        "    # Train for one iteration\n",
        "    try:\n",
        "        result = algo.train()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR at iteration {i+1}: {e}\")\n",
        "        print(\"Training failed. Check resource constraints and Ray status.\")\n",
        "        break\n",
        "    \n",
        "    # Diagnostic check on first iteration: print all available metrics\n",
        "    if i == 0:\n",
        "        print(\"\\nFirst iteration diagnostics:\")\n",
        "        print(f\"  Result type: {type(result)}\")\n",
        "        print(f\"  Result keys (first 20): {list(result.keys())[:20]}\")\n",
        "        \n",
        "        # Check for environment runner metrics (new API - note: plural \"env_runners\")\n",
        "        if \"env_runners\" in result:\n",
        "            env_metrics = result[\"env_runners\"]\n",
        "            print(f\"  Env runners metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runners keys: {list(env_metrics.keys())[:20]}\")\n",
        "                # Print some values\n",
        "                for k in list(env_metrics.keys())[:10]:\n",
        "                    print(f\"    {k}: {env_metrics[k]}\")\n",
        "        elif \"env_runner\" in result:\n",
        "            env_metrics = result[\"env_runner\"]\n",
        "            print(f\"  Env runner (singular) metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runner keys: {list(env_metrics.keys())[:15]}\")\n",
        "        \n",
        "        # Check for learner metrics\n",
        "        if \"learners\" in result:\n",
        "            learner_metrics = result[\"learners\"]\n",
        "            print(f\"  Learners metrics keys: {list(learner_metrics.keys())[:15] if isinstance(learner_metrics, dict) else 'N/A'}\")\n",
        "        \n",
        "        # Look for episode metrics in different places\n",
        "        print(\"\\n  Searching for episode/reward metrics:\")\n",
        "        for key in result.keys():\n",
        "            if \"episode\" in key.lower() or \"reward\" in key.lower():\n",
        "                val = result[key]\n",
        "                print(f\"    {key}: {val}\")\n",
        "        \n",
        "        # Check env_runners dict for episode metrics\n",
        "        if \"env_runners\" in result and isinstance(result[\"env_runners\"], dict):\n",
        "            print(\"\\n  env_runners metrics:\")\n",
        "            for key in result[\"env_runners\"].keys():\n",
        "                if \"episode\" in key.lower() or \"reward\" in key.lower() or \"step\" in key.lower():\n",
        "                    val = result[\"env_runners\"][key]\n",
        "                    print(f\"    {key}: {val}\")\n",
        "            \n",
        "            # Also check num_env_steps_sampled to see if steps are being collected\n",
        "            if \"num_env_steps_sampled\" in result.get(\"env_runners\", {}):\n",
        "                print(f\"\\n  Total steps sampled: {result['env_runners']['num_env_steps_sampled']}\")\n",
        "        print()\n",
        "    \n",
        "    # Store metrics - new RLlib API uses different key names\n",
        "    # In env_runners: episode_return_* (not episode_reward_*)\n",
        "    # Also check agent_episode_return_mean for agent-specific rewards\n",
        "    env_runners_metrics = result.get(\"env_runners\", {})\n",
        "    env_runner_metrics = result.get(\"env_runner\", {})  # Also check singular for compatibility\n",
        "    \n",
        "    # Try env_runners first (new API), then env_runner, then top-level\n",
        "    if isinstance(env_runners_metrics, dict) and len(env_runners_metrics) > 0:\n",
        "        # New API uses \"episode_return\" not \"episode_reward\"\n",
        "        episode_reward_mean = env_runners_metrics.get(\"episode_return_mean\",\n",
        "                                                      env_runners_metrics.get(\"episode_reward_mean\",\n",
        "                                                      result.get(\"episode_return_mean\",\n",
        "                                                                 result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runners_metrics.get(\"episode_return_min\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_min\",\n",
        "                                                     result.get(\"episode_return_min\",\n",
        "                                                                result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runners_metrics.get(\"episode_return_max\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_max\",\n",
        "                                                     result.get(\"episode_return_max\",\n",
        "                                                                result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runners_metrics.get(\"episode_len_mean\",\n",
        "                                                   result.get(\"episode_len_mean\", 0))\n",
        "        \n",
        "        # If we got rewards from agent_episode_return_mean, use that (more accurate)\n",
        "        agent_returns = env_runners_metrics.get(\"agent_episode_return_mean\", {})\n",
        "        if isinstance(agent_returns, dict) and len(agent_returns) > 0:\n",
        "            # Use the first agent's return (typically \"default_agent\")\n",
        "            first_agent_return = list(agent_returns.values())[0]\n",
        "            if first_agent_return != 0 or episode_reward_mean == 0:\n",
        "                episode_reward_mean = first_agent_return\n",
        "    elif isinstance(env_runner_metrics, dict) and len(env_runner_metrics) > 0:\n",
        "        episode_reward_mean = env_runner_metrics.get(\"episode_return_mean\",\n",
        "                                                     env_runner_metrics.get(\"episode_reward_mean\",\n",
        "                                                     result.get(\"episode_return_mean\",\n",
        "                                                                result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runner_metrics.get(\"episode_return_min\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_min\",\n",
        "                                                    result.get(\"episode_return_min\",\n",
        "                                                               result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runner_metrics.get(\"episode_return_max\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_max\",\n",
        "                                                    result.get(\"episode_return_max\",\n",
        "                                                               result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runner_metrics.get(\"episode_len_mean\",\n",
        "                                                  result.get(\"episode_len_mean\", 0))\n",
        "    else:\n",
        "        # Fallback to top-level keys\n",
        "        episode_reward_mean = result.get(\"episode_return_mean\", result.get(\"episode_reward_mean\", 0))\n",
        "        episode_reward_min = result.get(\"episode_return_min\", result.get(\"episode_reward_min\", 0))\n",
        "        episode_reward_max = result.get(\"episode_return_max\", result.get(\"episode_reward_max\", 0))\n",
        "        episode_len_mean = result.get(\"episode_len_mean\", 0)\n",
        "    \n",
        "    training_history[\"episode_reward_mean\"].append(episode_reward_mean)\n",
        "    training_history[\"episode_reward_min\"].append(episode_reward_min)\n",
        "    training_history[\"episode_reward_max\"].append(episode_reward_max)\n",
        "    training_history[\"episode_len_mean\"].append(episode_len_mean)\n",
        "    \n",
        "    iteration_time = time.time() - iteration_start\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print progress (always print first iteration, then every 10)\n",
        "    if (i + 1) % 10 == 0 or i == 0:\n",
        "        # Use the same values we stored (already extracted from correct location)\n",
        "        mean_reward = episode_reward_mean\n",
        "        min_reward = episode_reward_min\n",
        "        max_reward = episode_reward_max\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        \n",
        "        # Check if we have other reward metrics\n",
        "        env_runner_metrics = result.get(\"env_runner\", {})\n",
        "        if isinstance(env_runner_metrics, dict):\n",
        "            actual_rewards = env_runner_metrics.get(\"episode_reward_mean\", mean_reward)\n",
        "        else:\n",
        "            actual_rewards = mean_reward\n",
        "        \n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Min: {min_reward:.2f} | Max: {max_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "        \n",
        "        # Diagnostic: Print all result keys on first iteration\n",
        "        if i == 0:\n",
        "            print(f\"  Available metrics keys: {[k for k in result.keys() if 'reward' in k.lower() or 'episode' in k.lower()][:10]}\")\n",
        "        \n",
        "    elif i < 5:  # Also print first 5 iterations to show it's working\n",
        "        mean_reward = episode_reward_mean\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Training complete! Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Final mean reward: {training_history['episode_reward_mean'][-1] if training_history['episode_reward_mean'] else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Episode rewards\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_history[\"episode_reward_mean\"], label=\"Mean Reward\", linewidth=2)\n",
        "plt.fill_between(\n",
        "    range(len(training_history[\"episode_reward_mean\"])),\n",
        "    training_history[\"episode_reward_min\"],\n",
        "    training_history[\"episode_reward_max\"],\n",
        "    alpha=0.3,\n",
        "    label=\"Min-Max Range\"\n",
        ")\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Reward\")\n",
        "plt.title(\"Training Progress - Episode Rewards\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Episode length\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_history[\"episode_len_mean\"], label=\"Mean Episode Length\", color=\"green\", linewidth=2)\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.title(\"Training Progress - Episode Length\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final statistics\n",
        "final_mean = training_history[\"episode_reward_mean\"][-1] if training_history[\"episode_reward_mean\"] else 0\n",
        "best_mean = max(training_history[\"episode_reward_mean\"]) if training_history[\"episode_reward_mean\"] else 0\n",
        "print(f\"\\nFinal mean reward: {final_mean:.2f}\")\n",
        "print(f\"Best mean reward: {best_mean:.2f}\")\n",
        "if \"Pong\" in GAME_ID:\n",
        "    print(f\"Pong theoretical max: 21 (winning 21-0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Run evaluation episodes to see how well the trained policy performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "eval_results = algo.evaluate()\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Mean episode reward: {eval_results.get('evaluation/episode_reward_mean', 'N/A'):.2f}\")\n",
        "print(f\"  Min episode reward: {eval_results.get('evaluation/episode_reward_min', 'N/A'):.2f}\")\n",
        "print(f\"  Max episode reward: {eval_results.get('evaluation/episode_reward_max', 'N/A'):.2f}\")\n",
        "print(f\"  Mean episode length: {eval_results.get('evaluation/episode_len_mean', 'N/A'):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual evaluation: run a few episodes and record video\n",
        "import imageio\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def evaluate_and_record(algo, env, num_episodes=3, save_video=True):\n",
        "    \"\"\"Run evaluation episodes and optionally save videos.\"\"\"\n",
        "    # Handle both old and new API stacks\n",
        "    def get_action(obs):\n",
        "        try:\n",
        "            # Try new API stack first (algo.compute_single_action)\n",
        "            if hasattr(algo, 'compute_single_action'):\n",
        "                result = algo.compute_single_action(obs, explore=False)\n",
        "                # Result might be action directly or tuple\n",
        "                return result[0] if isinstance(result, (list, tuple)) else result\n",
        "            # Fallback to old API stack\n",
        "            elif hasattr(algo, 'get_policy'):\n",
        "                policy = algo.get_policy()\n",
        "                return policy.compute_single_action(obs, explore=False)[0]\n",
        "            else:\n",
        "                raise AttributeError(\"No method found to compute actions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing action: {e}\")\n",
        "            # Return a random action as fallback\n",
        "            return env.action_space.sample()\n",
        "    \n",
        "    all_rewards = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        obs, info = env.reset(seed=1000 + ep)\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        frames = []\n",
        "        \n",
        "        while not done:\n",
        "            # Get action from policy (works with both API stacks)\n",
        "            action = get_action(obs)\n",
        "            \n",
        "            # Render frame\n",
        "            frame = (np.clip(obs, 0, 1) * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "            \n",
        "            # Add action overlay\n",
        "            img = Image.fromarray(frame)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            txt = f\"action={action}, reward={total_reward:.1f}\"\n",
        "            draw.rectangle([0, 0, 200, 20], fill=(0, 0, 0, 200))\n",
        "            draw.text((5, 3), txt, fill=(255, 255, 255))\n",
        "            frame = np.array(img)\n",
        "            frames.append(frame)\n",
        "            \n",
        "            # Step environment\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        \n",
        "        all_rewards.append(total_reward)\n",
        "        \n",
        "        # Save video\n",
        "        if save_video:\n",
        "            video_path = f\"rllib_eval_episode_{ep+1}.mp4\"\n",
        "            with imageio.get_writer(video_path, fps=30, macro_block_size=1) as writer:\n",
        "                for f in frames:\n",
        "                    writer.append_data(f)\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}, Saved to {video_path}\")\n",
        "        else:\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}\")\n",
        "    \n",
        "    print(f\"\\nEvaluation Summary:\")\n",
        "    print(f\"  Mean reward: {np.mean(all_rewards):.2f}\")\n",
        "    print(f\"  Std reward: {np.std(all_rewards):.2f}\")\n",
        "    print(f\"  Min reward: {np.min(all_rewards):.2f}\")\n",
        "    print(f\"  Max reward: {np.max(all_rewards):.2f}\")\n",
        "    \n",
        "    return all_rewards\n",
        "\n",
        "# Run evaluation\n",
        "eval_rewards = evaluate_and_record(algo, env, num_episodes=EVAL_EPISODES, save_video=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Load Model\n",
        "\n",
        "Save the trained model for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "checkpoint_path = algo.save(\"./rllib_pong_checkpoint\")\n",
        "print(f\"Model saved to: {checkpoint_path}\")\n",
        "\n",
        "# Example: Load the model later\n",
        "# from ray.rllib.algorithms.ppo import PPO\n",
        "# algo_loaded = PPO.from_checkpoint(checkpoint_path)\n",
        "# print(\"Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Google Colab Considerations\n",
        "\n",
        "- **GPU Runtime**: Enable GPU for faster training (Runtime > Change runtime type > GPU)\n",
        "- **Resource Limits**: Configuration uses 1 worker and reduced batch sizes to fit Colab's constraints\n",
        "- **Memory**: If you encounter OOM errors, reduce `train_batch_size` further\n",
        "- **Ray Shutdown**: Colab may require restarting the runtime if Ray doesn't shut down cleanly\n",
        "\n",
        "### Baseline Comparison\n",
        "\n",
        "This notebook provides a baseline for comparing against:\n",
        "- **V-JEPA2 encoder approach**: Uses pre-trained frozen encoder\n",
        "- **RSSM approach**: Uses learned dynamics model\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **No encoder**: Policy learns directly from raw visual observations\n",
        "- **RLlib framework**: Uses well-tested RL algorithms (PPO) with built-in optimizations\n",
        "- **Standard preprocessing**: 84x84 RGB frames, normalized to [0, 1]\n",
        "- **Colab-optimized**: Reduced workers and batch sizes for Colab's resource constraints\n",
        "\n",
        "### Configuration\n",
        "\n",
        "You can adjust training parameters via environment variables:\n",
        "- `NUM_TRAIN_ITERATIONS`: Number of training iterations (default: 100)\n",
        "- `NUM_EVAL_EPISODES`: Number of evaluation episodes (default: 5)\n",
        "- `ATARI_GAME`: Game environment (default: PongNoFrameskip-v4)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Compare training curves with V-JEPA2 and RSSM approaches\n",
        "- Experiment with different RLlib algorithms (IMPALA, A3C, etc.)\n",
        "- Tune hyperparameters for better performance\n",
        "- Add frame stacking for temporal information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "try:\n",
        "    env.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Shutdown Ray (recommended for Colab to free resources)\n",
        "# Note: In Colab, you may need to restart the runtime if Ray doesn't shut down cleanly\n",
        "try:\n",
        "    ray.shutdown()\n",
        "    print(\"Ray shut down successfully\")\n",
        "except:\n",
        "    print(\"Ray shutdown encountered an issue. You may need to restart the Colab runtime.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl-pong",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
