{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pong Policy Training Baseline with RLlib\n",
        "\n",
        "This notebook implements a baseline policy training pipeline using Ray RLlib, training directly on visual observations without any encoder. This serves as a comparison baseline for the V-JEPA2 and RSSM approaches.\n",
        "\n",
        "## Google Colab Setup\n",
        "\n",
        "**Important**: This notebook is optimized for Google Colab. Make sure to:\n",
        "1. Enable GPU runtime: `Runtime > Change runtime type > GPU`\n",
        "2. Run the installation cell first to install dependencies\n",
        "3. The configuration is optimized for Colab's limited resources (2 CPUs, 1 GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies for Google Colab\n",
        "# Note: Colab comes with gymnasium, but we need ray[rllib] and ale-py\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Register ALE environments\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# Configuration\n",
        "GAME_ID = os.environ.get(\"ATARI_GAME\", \"PongNoFrameskip-v4\")\n",
        "NUM_TRAIN_ITERATIONS = int(os.environ.get(\"NUM_TRAIN_ITERATIONS\", \"100\"))\n",
        "EVAL_EPISODES = int(os.environ.get(\"NUM_EVAL_EPISODES\", \"5\"))\n",
        "\n",
        "print(f\"Game: {GAME_ID}\")\n",
        "print(f\"Training iterations: {NUM_TRAIN_ITERATIONS}\")\n",
        "print(f\"Evaluation episodes: {EVAL_EPISODES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Create the Pong environment with standard preprocessing (resize to 84x84, normalize).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_obs(obs):\n",
        "    \"\"\"Transform observation to (C, H, W) format and normalize to [0, 1].\"\"\"\n",
        "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "    return obs_t\n",
        "\n",
        "# Create environment with wrappers\n",
        "env = gym.make(GAME_ID)\n",
        "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "\n",
        "env = gym.wrappers.TransformObservation(\n",
        "    env,\n",
        "    func=transform_obs,\n",
        "    observation_space=new_obs_space,\n",
        ")\n",
        "\n",
        "# Test environment\n",
        "obs, info = env.reset()\n",
        "assert obs.shape == (3, 84, 84), f\"Expected (3, 84, 84), got {obs.shape}\"\n",
        "print(f\"Environment initialized successfully\")\n",
        "print(f\"Observation shape: {obs.shape}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Number of actions: {env.action_space.n}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RLlib Training Setup\n",
        "\n",
        "Configure Ray RLlib to train a policy using PPO (Proximal Policy Optimization), which works well for Atari games.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "# Check if GPU is available\n",
        "import torch\n",
        "has_gpu = torch.cuda.is_available()\n",
        "print(f\"GPU available: {has_gpu}\")\n",
        "\n",
        "# Initialize Ray for Google Colab\n",
        "# Colab typically has 2 CPUs, so we use fewer workers\n",
        "if not ray.is_initialized():\n",
        "    # Colab-friendly initialization: use available CPUs, enable GPU if available\n",
        "    ray.init(\n",
        "        ignore_reinit_error=True,\n",
        "        num_cpus=2,  # Colab typically has 2 CPUs\n",
        "        num_gpus=1 if has_gpu else 0,\n",
        "        object_store_memory=2_000_000_000,  # 2GB object store\n",
        "    )\n",
        "    print(\"Ray initialized for Colab\")\n",
        "else:\n",
        "    print(\"Ray already initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a function to register and return the environment\n",
        "def env_creator(env_config):\n",
        "    \"\"\"Create and return the Pong environment.\"\"\"\n",
        "    import gymnasium as gym\n",
        "    import ale_py\n",
        "    from gymnasium import spaces\n",
        "    \n",
        "    gym.register_envs(ale_py)\n",
        "    \n",
        "    env = gym.make(GAME_ID)\n",
        "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "    \n",
        "    def transform_obs(obs):\n",
        "        obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
        "        return obs_t\n",
        "    \n",
        "    new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 84, 84), dtype=np.float32)\n",
        "    env = gym.wrappers.TransformObservation(\n",
        "        env,\n",
        "        func=transform_obs,\n",
        "        observation_space=new_obs_space,\n",
        "    )\n",
        "    \n",
        "    return env\n",
        "\n",
        "# Register the environment\n",
        "tune.register_env(\"pong_env\", env_creator)\n",
        "print(\"Environment registered with RLlib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure PPO algorithm for Colab\n",
        "# Colab-friendly settings: fewer workers, smaller batches, GPU if available\n",
        "num_env_runners = 1  # Colab has limited CPUs, use 1 env runner\n",
        "use_gpu = has_gpu  # Use GPU if available\n",
        "\n",
        "# Note: minibatch_size is automatically calculated by RLlib from train_batch_size\n",
        "# Updated to use new RLlib API (env_runners instead of rollouts)\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\"pong_env\")\n",
        "    .framework(\"torch\")\n",
        "    .training(\n",
        "        lr=3e-4,\n",
        "        train_batch_size=1000,  # Reduced further to see episode completion sooner\n",
        "        # minibatch_size will be auto-calculated (typically train_batch_size / 4)\n",
        "        num_epochs=30,  # Updated from num_sgd_iter (deprecated)\n",
        "        gamma=0.99,\n",
        "        lambda_=0.95,\n",
        "        clip_param=0.2,\n",
        "        entropy_coeff=0.01,\n",
        "        vf_loss_coeff=0.5,\n",
        "    )\n",
        "    .resources(\n",
        "        num_gpus=1 if use_gpu else 0,\n",
        "    )\n",
        "    .env_runners(\n",
        "        num_env_runners=num_env_runners,  # Updated from num_rollout_workers\n",
        "        num_envs_per_env_runner=1,  # Updated from num_envs_per_worker\n",
        "        num_cpus_per_env_runner=1,  # Updated from num_cpus_per_worker in resources\n",
        "    )\n",
        "    .evaluation(\n",
        "        evaluation_interval=10,\n",
        "        evaluation_duration=EVAL_EPISODES,\n",
        "        evaluation_num_env_runners=0,  # Set to 0 to disable separate eval workers (Colab resource constraint)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"PPO configuration (Colab-optimized):\")\n",
        "print(f\"  Learning rate: {config.lr}\")\n",
        "print(f\"  Training batch size: {config.train_batch_size}\")\n",
        "print(f\"  Number of epochs: {config.num_epochs}\")\n",
        "print(f\"  Number of env runners: {config.num_env_runners}\")\n",
        "print(f\"  Evaluation env runners: {config.evaluation_num_env_runners} (0 = use training workers)\")\n",
        "print(f\"  GPU enabled: {use_gpu}\")\n",
        "print(f\"  Gamma (discount): {config.gamma}\")\n",
        "print(\"  Note: Minibatch size is auto-calculated by RLlib\")\n",
        "print(\"  Note: Evaluation uses training workers to avoid CPU resource conflicts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostics\n",
        "\n",
        "Check Ray cluster status and algorithm readiness before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Ray cluster status\n",
        "import ray\n",
        "print(\"Ray cluster status:\")\n",
        "cluster_resources = {}\n",
        "available_resources = {}\n",
        "try:\n",
        "    cluster_resources = ray.cluster_resources()\n",
        "    available_resources = ray.available_resources()\n",
        "    print(f\"  Total CPUs: {cluster_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Available CPUs: {available_resources.get('CPU', 0)}\")\n",
        "    print(f\"  Total GPUs: {cluster_resources.get('GPU', 0)}\")\n",
        "    print(f\"  Available GPUs: {available_resources.get('GPU', 0)}\")\n",
        "    used_cpus = cluster_resources.get('CPU', 0) - available_resources.get('CPU', 0)\n",
        "    print(f\"  Used CPUs: {used_cpus:.1f}\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not get cluster resources: {e}\")\n",
        "\n",
        "print(\"\\nAlgorithm configuration check:\")\n",
        "try:\n",
        "    num_train_cpus = config.num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    num_eval_cpus = config.evaluation_num_env_runners * getattr(config, 'num_cpus_per_env_runner', 1)\n",
        "    total_required = num_train_cpus + num_eval_cpus\n",
        "    print(f\"  Required CPUs (training): {num_train_cpus}\")\n",
        "    print(f\"  Required CPUs (evaluation): {num_eval_cpus}\")\n",
        "    print(f\"  Total required: {total_required}\")\n",
        "    \n",
        "    if cluster_resources:\n",
        "        available_cpus = cluster_resources.get('CPU', 0)\n",
        "        if total_required > available_cpus:\n",
        "            print(f\"  ⚠️  WARNING: Required CPUs ({total_required}) > Available CPUs ({available_cpus})\")\n",
        "            print(f\"     This may cause training to hang. Consider reducing num_env_runners or setting evaluation_num_env_runners=0\")\n",
        "        else:\n",
        "            print(f\"  ✓ Resource requirements are within available resources\")\n",
        "    else:\n",
        "        print(f\"  (Could not verify against available resources)\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not check configuration: {e}\")\n",
        "    print(f\"  Config attributes: num_env_runners={config.num_env_runners}, evaluation_num_env_runners={config.evaluation_num_env_runners}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Algorithm\n",
        "\n",
        "**Expected time: 1-3 minutes**\n",
        "\n",
        "If this takes longer than 5 minutes, it may be stuck. Check:\n",
        "1. Ray cluster status (diagnostics cell above)\n",
        "2. No CPU resource warnings\n",
        "3. If stuck, restart Ray and rebuild\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Train the policy using RLlib's PPO algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo import PPO\n",
        "import time\n",
        "\n",
        "# Build the algorithm (using new API method)\n",
        "print(\"Building algorithm (this may take 1-3 minutes on first run)...\")\n",
        "print(\"Components being initialized:\")\n",
        "print(\"  - Neural network models\")\n",
        "print(\"  - Environment runners\")\n",
        "print(\"  - Learner group\")\n",
        "print(\"  - ALE environment (Pong ROM)\")\n",
        "print()\n",
        "\n",
        "build_start = time.time()\n",
        "try:\n",
        "    algo = config.build_algo()\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✓ Algorithm built successfully in {build_time:.1f} seconds ({build_time/60:.1f} minutes)\")\n",
        "except Exception as e:\n",
        "    build_time = time.time() - build_start\n",
        "    print(f\"\\n✗ Algorithm build failed after {build_time:.1f} seconds\")\n",
        "    print(f\"Error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Access the model - new API stack uses modules differently\n",
        "try:\n",
        "    # Try new API stack access\n",
        "    if hasattr(algo, 'learner_group'):\n",
        "        module = algo.learner_group.get_module()\n",
        "        print(f\"Policy module type: {type(module)}\")\n",
        "    elif hasattr(algo, 'get_policy'):\n",
        "        # Old API stack\n",
        "        policy = algo.get_policy()\n",
        "        print(f\"Policy network: {policy.model}\")\n",
        "    else:\n",
        "        print(\"Algorithm built, but model access method not found\")\n",
        "        print(f\"Algorithm type: {type(algo)}\")\n",
        "        print(f\"Available attributes: {[attr for attr in dir(algo) if not attr.startswith('_')]}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not access model directly: {e}\")\n",
        "    print(\"Algorithm is ready for training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "training_history = {\n",
        "    \"episode_reward_mean\": [],\n",
        "    \"episode_reward_min\": [],\n",
        "    \"episode_reward_max\": [],\n",
        "    \"episode_len_mean\": [],\n",
        "}\n",
        "\n",
        "print(f\"Starting training for {NUM_TRAIN_ITERATIONS} iterations...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Note: First iteration may take longer to initialize environments\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(NUM_TRAIN_ITERATIONS):\n",
        "    iteration_start = time.time()\n",
        "    \n",
        "    # Train for one iteration\n",
        "    try:\n",
        "        result = algo.train()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR at iteration {i+1}: {e}\")\n",
        "        print(\"Training failed. Check resource constraints and Ray status.\")\n",
        "        break\n",
        "    \n",
        "    # Diagnostic check on first iteration: print all available metrics\n",
        "    if i == 0:\n",
        "        print(\"\\nFirst iteration diagnostics:\")\n",
        "        print(f\"  Result type: {type(result)}\")\n",
        "        print(f\"  Result keys (first 20): {list(result.keys())[:20]}\")\n",
        "        \n",
        "        # Check for environment runner metrics (new API - note: plural \"env_runners\")\n",
        "        if \"env_runners\" in result:\n",
        "            env_metrics = result[\"env_runners\"]\n",
        "            print(f\"  Env runners metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runners keys: {list(env_metrics.keys())[:20]}\")\n",
        "                # Print some values\n",
        "                for k in list(env_metrics.keys())[:10]:\n",
        "                    print(f\"    {k}: {env_metrics[k]}\")\n",
        "        elif \"env_runner\" in result:\n",
        "            env_metrics = result[\"env_runner\"]\n",
        "            print(f\"  Env runner (singular) metrics type: {type(env_metrics)}\")\n",
        "            if isinstance(env_metrics, dict):\n",
        "                print(f\"  Env runner keys: {list(env_metrics.keys())[:15]}\")\n",
        "        \n",
        "        # Check for learner metrics\n",
        "        if \"learners\" in result:\n",
        "            learner_metrics = result[\"learners\"]\n",
        "            print(f\"  Learners metrics keys: {list(learner_metrics.keys())[:15] if isinstance(learner_metrics, dict) else 'N/A'}\")\n",
        "        \n",
        "        # Look for episode metrics in different places\n",
        "        print(\"\\n  Searching for episode/reward metrics:\")\n",
        "        for key in result.keys():\n",
        "            if \"episode\" in key.lower() or \"reward\" in key.lower():\n",
        "                val = result[key]\n",
        "                print(f\"    {key}: {val}\")\n",
        "        \n",
        "        # Check env_runners dict for episode metrics\n",
        "        if \"env_runners\" in result and isinstance(result[\"env_runners\"], dict):\n",
        "            print(\"\\n  env_runners metrics:\")\n",
        "            for key in result[\"env_runners\"].keys():\n",
        "                if \"episode\" in key.lower() or \"reward\" in key.lower() or \"step\" in key.lower():\n",
        "                    val = result[\"env_runners\"][key]\n",
        "                    print(f\"    {key}: {val}\")\n",
        "            \n",
        "            # Also check num_env_steps_sampled to see if steps are being collected\n",
        "            if \"num_env_steps_sampled\" in result.get(\"env_runners\", {}):\n",
        "                print(f\"\\n  Total steps sampled: {result['env_runners']['num_env_steps_sampled']}\")\n",
        "        print()\n",
        "    \n",
        "    # Store metrics - new RLlib API uses different key names\n",
        "    # In env_runners: episode_return_* (not episode_reward_*)\n",
        "    # Also check agent_episode_return_mean for agent-specific rewards\n",
        "    env_runners_metrics = result.get(\"env_runners\", {})\n",
        "    env_runner_metrics = result.get(\"env_runner\", {})  # Also check singular for compatibility\n",
        "    \n",
        "    # Try env_runners first (new API), then env_runner, then top-level\n",
        "    if isinstance(env_runners_metrics, dict) and len(env_runners_metrics) > 0:\n",
        "        # New API uses \"episode_return\" not \"episode_reward\"\n",
        "        episode_reward_mean = env_runners_metrics.get(\"episode_return_mean\",\n",
        "                                                      env_runners_metrics.get(\"episode_reward_mean\",\n",
        "                                                      result.get(\"episode_return_mean\",\n",
        "                                                                 result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runners_metrics.get(\"episode_return_min\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_min\",\n",
        "                                                     result.get(\"episode_return_min\",\n",
        "                                                                result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runners_metrics.get(\"episode_return_max\",\n",
        "                                                     env_runners_metrics.get(\"episode_reward_max\",\n",
        "                                                     result.get(\"episode_return_max\",\n",
        "                                                                result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runners_metrics.get(\"episode_len_mean\",\n",
        "                                                   result.get(\"episode_len_mean\", 0))\n",
        "        \n",
        "        # If we got rewards from agent_episode_return_mean, use that (more accurate)\n",
        "        agent_returns = env_runners_metrics.get(\"agent_episode_return_mean\", {})\n",
        "        if isinstance(agent_returns, dict) and len(agent_returns) > 0:\n",
        "            # Use the first agent's return (typically \"default_agent\")\n",
        "            first_agent_return = list(agent_returns.values())[0]\n",
        "            if first_agent_return != 0 or episode_reward_mean == 0:\n",
        "                episode_reward_mean = first_agent_return\n",
        "    elif isinstance(env_runner_metrics, dict) and len(env_runner_metrics) > 0:\n",
        "        episode_reward_mean = env_runner_metrics.get(\"episode_return_mean\",\n",
        "                                                     env_runner_metrics.get(\"episode_reward_mean\",\n",
        "                                                     result.get(\"episode_return_mean\",\n",
        "                                                                result.get(\"episode_reward_mean\", 0))))\n",
        "        episode_reward_min = env_runner_metrics.get(\"episode_return_min\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_min\",\n",
        "                                                    result.get(\"episode_return_min\",\n",
        "                                                               result.get(\"episode_reward_min\", 0))))\n",
        "        episode_reward_max = env_runner_metrics.get(\"episode_return_max\",\n",
        "                                                    env_runner_metrics.get(\"episode_reward_max\",\n",
        "                                                    result.get(\"episode_return_max\",\n",
        "                                                               result.get(\"episode_reward_max\", 0))))\n",
        "        episode_len_mean = env_runner_metrics.get(\"episode_len_mean\",\n",
        "                                                  result.get(\"episode_len_mean\", 0))\n",
        "    else:\n",
        "        # Fallback to top-level keys\n",
        "        episode_reward_mean = result.get(\"episode_return_mean\", result.get(\"episode_reward_mean\", 0))\n",
        "        episode_reward_min = result.get(\"episode_return_min\", result.get(\"episode_reward_min\", 0))\n",
        "        episode_reward_max = result.get(\"episode_return_max\", result.get(\"episode_reward_max\", 0))\n",
        "        episode_len_mean = result.get(\"episode_len_mean\", 0)\n",
        "    \n",
        "    training_history[\"episode_reward_mean\"].append(episode_reward_mean)\n",
        "    training_history[\"episode_reward_min\"].append(episode_reward_min)\n",
        "    training_history[\"episode_reward_max\"].append(episode_reward_max)\n",
        "    training_history[\"episode_len_mean\"].append(episode_len_mean)\n",
        "    \n",
        "    iteration_time = time.time() - iteration_start\n",
        "    elapsed_time = time.time() - start_time\n",
        "    \n",
        "    # Print progress (always print first iteration, then every 10)\n",
        "    if (i + 1) % 10 == 0 or i == 0:\n",
        "        # Use the same values we stored (already extracted from correct location)\n",
        "        mean_reward = episode_reward_mean\n",
        "        min_reward = episode_reward_min\n",
        "        max_reward = episode_reward_max\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        \n",
        "        # Check if we have other reward metrics\n",
        "        env_runner_metrics = result.get(\"env_runner\", {})\n",
        "        if isinstance(env_runner_metrics, dict):\n",
        "            actual_rewards = env_runner_metrics.get(\"episode_reward_mean\", mean_reward)\n",
        "        else:\n",
        "            actual_rewards = mean_reward\n",
        "        \n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Min: {min_reward:.2f} | Max: {max_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "        \n",
        "        # Diagnostic: Print all result keys on first iteration\n",
        "        if i == 0:\n",
        "            print(f\"  Available metrics keys: {[k for k in result.keys() if 'reward' in k.lower() or 'episode' in k.lower()][:10]}\")\n",
        "        \n",
        "    elif i < 5:  # Also print first 5 iterations to show it's working\n",
        "        mean_reward = episode_reward_mean\n",
        "        mean_len = episode_len_mean\n",
        "        \n",
        "        # Get episode count - new API uses \"num_episodes\" not \"episodes_this_iter\"\n",
        "        if isinstance(env_runners_metrics, dict):\n",
        "            num_episodes = env_runners_metrics.get(\"num_episodes\",\n",
        "                                                   env_runners_metrics.get(\"episodes_this_iter\",\n",
        "                                                   result.get(\"num_episodes\",\n",
        "                                                              result.get(\"episodes_this_iter\", 0))))\n",
        "        elif isinstance(env_runner_metrics, dict):\n",
        "            num_episodes = env_runner_metrics.get(\"num_episodes\",\n",
        "                                                  env_runner_metrics.get(\"episodes_this_iter\",\n",
        "                                                  result.get(\"num_episodes\",\n",
        "                                                             result.get(\"episodes_this_iter\", 0))))\n",
        "        else:\n",
        "            num_episodes = result.get(\"num_episodes\", result.get(\"episodes_this_iter\", 0))\n",
        "        print(f\"Iteration {i+1}/{NUM_TRAIN_ITERATIONS} | \"\n",
        "              f\"Mean Reward: {mean_reward:.2f} | \"\n",
        "              f\"Episodes: {num_episodes} | \"\n",
        "              f\"Mean Len: {mean_len:.1f} | \"\n",
        "              f\"Time: {iteration_time:.1f}s\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Training complete! Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Final mean reward: {training_history['episode_reward_mean'][-1] if training_history['episode_reward_mean'] else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Episode rewards\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_history[\"episode_reward_mean\"], label=\"Mean Reward\", linewidth=2)\n",
        "plt.fill_between(\n",
        "    range(len(training_history[\"episode_reward_mean\"])),\n",
        "    training_history[\"episode_reward_min\"],\n",
        "    training_history[\"episode_reward_max\"],\n",
        "    alpha=0.3,\n",
        "    label=\"Min-Max Range\"\n",
        ")\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Reward\")\n",
        "plt.title(\"Training Progress - Episode Rewards\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Episode length\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_history[\"episode_len_mean\"], label=\"Mean Episode Length\", color=\"green\", linewidth=2)\n",
        "plt.xlabel(\"Training Iteration\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.title(\"Training Progress - Episode Length\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final statistics\n",
        "final_mean = training_history[\"episode_reward_mean\"][-1] if training_history[\"episode_reward_mean\"] else 0\n",
        "best_mean = max(training_history[\"episode_reward_mean\"]) if training_history[\"episode_reward_mean\"] else 0\n",
        "print(f\"\\nFinal mean reward: {final_mean:.2f}\")\n",
        "print(f\"Best mean reward: {best_mean:.2f}\")\n",
        "if \"Pong\" in GAME_ID:\n",
        "    print(f\"Pong theoretical max: 21 (winning 21-0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Run evaluation episodes to see how well the trained policy performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation\n",
        "eval_results = algo.evaluate()\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Mean episode reward: {eval_results.get('evaluation/episode_reward_mean', 'N/A'):.2f}\")\n",
        "print(f\"  Min episode reward: {eval_results.get('evaluation/episode_reward_min', 'N/A'):.2f}\")\n",
        "print(f\"  Max episode reward: {eval_results.get('evaluation/episode_reward_max', 'N/A'):.2f}\")\n",
        "print(f\"  Mean episode length: {eval_results.get('evaluation/episode_len_mean', 'N/A'):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual evaluation: run a few episodes and record video\n",
        "import imageio\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def evaluate_and_record(algo, env, num_episodes=3, save_video=True):\n",
        "    \"\"\"Run evaluation episodes and optionally save videos.\"\"\"\n",
        "    # Handle both old and new API stacks\n",
        "    def get_action(obs):\n",
        "        try:\n",
        "            # Try new API stack first (algo.compute_single_action)\n",
        "            if hasattr(algo, 'compute_single_action'):\n",
        "                result = algo.compute_single_action(obs, explore=False)\n",
        "                # Result might be action directly or tuple\n",
        "                return result[0] if isinstance(result, (list, tuple)) else result\n",
        "            # Fallback to old API stack\n",
        "            elif hasattr(algo, 'get_policy'):\n",
        "                policy = algo.get_policy()\n",
        "                return policy.compute_single_action(obs, explore=False)[0]\n",
        "            else:\n",
        "                raise AttributeError(\"No method found to compute actions\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing action: {e}\")\n",
        "            # Return a random action as fallback\n",
        "            return env.action_space.sample()\n",
        "    \n",
        "    all_rewards = []\n",
        "    \n",
        "    for ep in range(num_episodes):\n",
        "        obs, info = env.reset(seed=1000 + ep)\n",
        "        done = False\n",
        "        total_reward = 0.0\n",
        "        frames = []\n",
        "        \n",
        "        while not done:\n",
        "            # Get action from policy (works with both API stacks)\n",
        "            action = get_action(obs)\n",
        "            \n",
        "            # Render frame\n",
        "            frame = (np.clip(obs, 0, 1) * 255).astype(np.uint8).transpose(1, 2, 0)\n",
        "            \n",
        "            # Add action overlay\n",
        "            img = Image.fromarray(frame)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            txt = f\"action={action}, reward={total_reward:.1f}\"\n",
        "            draw.rectangle([0, 0, 200, 20], fill=(0, 0, 0, 200))\n",
        "            draw.text((5, 3), txt, fill=(255, 255, 255))\n",
        "            frame = np.array(img)\n",
        "            frames.append(frame)\n",
        "            \n",
        "            # Step environment\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "        \n",
        "        all_rewards.append(total_reward)\n",
        "        \n",
        "        # Save video\n",
        "        if save_video:\n",
        "            video_path = f\"rllib_eval_episode_{ep+1}.mp4\"\n",
        "            with imageio.get_writer(video_path, fps=30, macro_block_size=1) as writer:\n",
        "                for f in frames:\n",
        "                    writer.append_data(f)\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}, Saved to {video_path}\")\n",
        "        else:\n",
        "            print(f\"Episode {ep+1}: Reward = {total_reward:.1f}\")\n",
        "    \n",
        "    print(f\"\\nEvaluation Summary:\")\n",
        "    print(f\"  Mean reward: {np.mean(all_rewards):.2f}\")\n",
        "    print(f\"  Std reward: {np.std(all_rewards):.2f}\")\n",
        "    print(f\"  Min reward: {np.min(all_rewards):.2f}\")\n",
        "    print(f\"  Max reward: {np.max(all_rewards):.2f}\")\n",
        "    \n",
        "    return all_rewards\n",
        "\n",
        "# Run evaluation\n",
        "eval_rewards = evaluate_and_record(algo, env, num_episodes=EVAL_EPISODES, save_video=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save and Load Model\n",
        "\n",
        "Save the trained model for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "checkpoint_path = algo.save(\"./rllib_pong_checkpoint\")\n",
        "print(f\"Model saved to: {checkpoint_path}\")\n",
        "\n",
        "# Example: Load the model later\n",
        "# from ray.rllib.algorithms.ppo import PPO\n",
        "# algo_loaded = PPO.from_checkpoint(checkpoint_path)\n",
        "# print(\"Model loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Google Colab Considerations\n",
        "\n",
        "- **GPU Runtime**: Enable GPU for faster training (Runtime > Change runtime type > GPU)\n",
        "- **Resource Limits**: Configuration uses 1 worker and reduced batch sizes to fit Colab's constraints\n",
        "- **Memory**: If you encounter OOM errors, reduce `train_batch_size` further\n",
        "- **Ray Shutdown**: Colab may require restarting the runtime if Ray doesn't shut down cleanly\n",
        "\n",
        "### Baseline Comparison\n",
        "\n",
        "This notebook provides a baseline for comparing against:\n",
        "- **V-JEPA2 encoder approach**: Uses pre-trained frozen encoder\n",
        "- **RSSM approach**: Uses learned dynamics model\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **No encoder**: Policy learns directly from raw visual observations\n",
        "- **RLlib framework**: Uses well-tested RL algorithms (PPO) with built-in optimizations\n",
        "- **Standard preprocessing**: 84x84 RGB frames, normalized to [0, 1]\n",
        "- **Colab-optimized**: Reduced workers and batch sizes for Colab's resource constraints\n",
        "\n",
        "### Configuration\n",
        "\n",
        "You can adjust training parameters via environment variables:\n",
        "- `NUM_TRAIN_ITERATIONS`: Number of training iterations (default: 100)\n",
        "- `NUM_EVAL_EPISODES`: Number of evaluation episodes (default: 5)\n",
        "- `ATARI_GAME`: Game environment (default: PongNoFrameskip-v4)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Compare training curves with V-JEPA2 and RSSM approaches\n",
        "- Experiment with different RLlib algorithms (IMPALA, A3C, etc.)\n",
        "- Tune hyperparameters for better performance\n",
        "- Add frame stacking for temporal information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "try:\n",
        "    env.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Shutdown Ray (recommended for Colab to free resources)\n",
        "# Note: In Colab, you may need to restart the runtime if Ray doesn't shut down cleanly\n",
        "try:\n",
        "    ray.shutdown()\n",
        "    print(\"Ray shut down successfully\")\n",
        "except:\n",
        "    print(\"Ray shutdown encountered an issue. You may need to restart the Colab runtime.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
