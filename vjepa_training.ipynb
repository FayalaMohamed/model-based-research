{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pong Policy Training Baseline with RLlib\n",
        "\n",
        "This notebook implements a baseline policy training pipeline using Ray RLlib, training directly on visual observations without any encoder. This serves as a comparison baseline for the V-JEPA2 and RSSM approaches.\n",
        "\n",
        "## Environment Notes\n",
        "\n",
        "- **Google Colab**: keep the install cell and default config (2 CPUs, 1 GPU)\n",
        "- **Local Apple Silicon (M1/M2)**: use the optimized configuration below (multi-core CPU, optional MPS acceleration)\n",
        "- Adjust the configuration variables if you run on a different machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Dependencies installed successfully!\n",
            "Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies for Google Colab\n",
        "# Note: Colab comes with gymnasium, but we need ray[rllib] and ale-py\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is -1654354918\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Update    1 | Approx return:  -0.26 | Running avg:  -0.26\n",
            "Update    2 | Approx return:  -1.20 | Running avg:  -0.27\n",
            "Update    3 | Approx return:  -1.51 | Running avg:  -0.29\n",
            "Update    4 | Approx return:  -1.67 | Running avg:  -0.30\n",
            "Update    5 | Approx return:  -1.89 | Running avg:  -0.32\n",
            "Update    6 | Approx return:  -2.02 | Running avg:  -0.33\n",
            "Update    7 | Approx return:  -2.18 | Running avg:  -0.35\n",
            "Update    8 | Approx return:  -2.25 | Running avg:  -0.37\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining finished. Model saved to\u001b[39m\u001b[33m\"\u001b[39m, MODEL_PATH)\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, avg_returns_history, running_history\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m model, avg_hist, run_hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    205\u001b[39m running_history = []       \u001b[38;5;66;03m# NEW: smoothed version\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, MAX_UPDATES + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     batch = \u001b[43mcollect_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS_PER_UPDATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# logging reward info from rollout\u001b[39;00m\n\u001b[32m    211\u001b[39m     approx_return = batch.returns.mean().item()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mcollect_rollout\u001b[39m\u001b[34m(env, model, steps_per_update)\u001b[39m\n\u001b[32m    100\u001b[39m action = dist.sample()\n\u001b[32m    101\u001b[39m log_prob = dist.log_prob(action)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m next_obs, reward, terminated, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[32m    105\u001b[39m next_obs = np.array(next_obs, dtype=np.float32)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/env.py:311\u001b[39m, in \u001b[36mAtariEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    309\u001b[39m reward = \u001b[32m0.0\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     reward += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43male\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m is_terminal = \u001b[38;5;28mself\u001b[39m.ale.game_over(with_truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    314\u001b[39m is_truncated = \u001b[38;5;28mself\u001b[39m.ale.game_truncated()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import ale_py\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt  # NEW\n",
        "\n",
        "# register ALE envs (gymnasium)\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# ==========================\n",
        "#  Hyperparameters\n",
        "# ==========================\n",
        "ENV_ID = \"ALE/Pong-v5\"   # note: we use obs_type=\"ram\" below\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "GAMMA = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "CLIP_EPS = 0.2\n",
        "LR = 2.5e-4\n",
        "VALUE_COEF = 0.5\n",
        "ENTROPY_COEF = 0.01\n",
        "\n",
        "STEPS_PER_UPDATE = 2048     # how many env steps before each PPO update\n",
        "PPO_EPOCHS = 4              # how many passes over the collected data\n",
        "MINI_BATCH_SIZE = 256\n",
        "\n",
        "MAX_UPDATES = 20000           # you can bump this later if you want\n",
        "MODEL_PATH = \"ppo_pong_ram.pt\"  # NEW: file to save model\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  Actor-Critic Network\n",
        "# ==========================\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Linear(hidden_size, act_dim)\n",
        "        self.value_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.shared(x)\n",
        "        logits = self.policy_head(x)\n",
        "        value = self.value_head(x).squeeze(-1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        return dist, value\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RolloutBatch:\n",
        "    obs: torch.Tensor       # [N, obs_dim]\n",
        "    actions: torch.Tensor   # [N]\n",
        "    log_probs: torch.Tensor # [N]\n",
        "    returns: torch.Tensor   # [N]\n",
        "    advantages: torch.Tensor# [N]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  GAE / advantage computation\n",
        "# ==========================\n",
        "def compute_gae(rewards, values, dones, gamma=GAMMA, lam=GAE_LAMBDA):\n",
        "    \"\"\"\n",
        "    rewards, values, dones are 1D numpy arrays of length T\n",
        "    values has an extra value for V(s_{T}) at the end (so len(values) = T+1).\n",
        "    \"\"\"\n",
        "    T = len(rewards)\n",
        "    advantages = np.zeros(T, dtype=np.float32)\n",
        "    gae = 0.0\n",
        "    for t in reversed(range(T)):\n",
        "        mask = 1.0 - float(dones[t])\n",
        "        delta = rewards[t] + gamma * values[t + 1] * mask - values[t]\n",
        "        gae = delta + gamma * lam * mask * gae\n",
        "        advantages[t] = gae\n",
        "    returns = advantages + values[:-1]\n",
        "    return advantages, returns\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  Collect rollout\n",
        "# ==========================\n",
        "def collect_rollout(env, model, steps_per_update):\n",
        "    obs_buf, act_buf, logp_buf = [], [], []\n",
        "    rew_buf, done_buf, val_buf = [], [], []\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    obs = np.array(obs, dtype=np.float32)\n",
        "\n",
        "    for _ in range(steps_per_update):\n",
        "        obs_tensor = torch.from_numpy(obs).unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            dist, value = model(obs_tensor)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "        next_obs = np.array(next_obs, dtype=np.float32)\n",
        "\n",
        "        obs_buf.append(obs)\n",
        "        act_buf.append(action.item())\n",
        "        logp_buf.append(log_prob.item())\n",
        "        rew_buf.append(reward)\n",
        "        done_buf.append(done)\n",
        "        val_buf.append(value.item())\n",
        "\n",
        "        obs = next_obs\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "            obs = np.array(obs, dtype=np.float32)\n",
        "\n",
        "    # final value for GAE bootstrap\n",
        "    obs_tensor = torch.from_numpy(obs).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        _, last_value = model(obs_tensor)\n",
        "    val_buf.append(last_value.item())\n",
        "\n",
        "    # convert to numpy\n",
        "    rewards = np.array(rew_buf, dtype=np.float32)\n",
        "    dones = np.array(done_buf, dtype=np.bool_)\n",
        "    values = np.array(val_buf, dtype=np.float32)\n",
        "\n",
        "    advantages, returns = compute_gae(rewards, values, dones)\n",
        "\n",
        "    # normalize advantages\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # to tensors\n",
        "    obs_tensor = torch.tensor(np.array(obs_buf), dtype=torch.float32, device=DEVICE)\n",
        "    actions_tensor = torch.tensor(np.array(act_buf), dtype=torch.long, device=DEVICE)\n",
        "    logp_tensor = torch.tensor(np.array(logp_buf), dtype=torch.float32, device=DEVICE)\n",
        "    adv_tensor = torch.tensor(advantages, dtype=torch.float32, device=DEVICE)\n",
        "    ret_tensor = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "    return RolloutBatch(\n",
        "        obs=obs_tensor,\n",
        "        actions=actions_tensor,\n",
        "        log_probs=logp_tensor,\n",
        "        returns=ret_tensor,\n",
        "        advantages=adv_tensor,\n",
        "    )\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  PPO Update\n",
        "# ==========================\n",
        "def ppo_update(model, optimizer, batch: RolloutBatch):\n",
        "    N = batch.obs.size(0)\n",
        "    idxs = np.arange(N)\n",
        "\n",
        "    for _ in range(PPO_EPOCHS):\n",
        "        np.random.shuffle(idxs)\n",
        "        for start in range(0, N, MINI_BATCH_SIZE):\n",
        "            end = start + MINI_BATCH_SIZE\n",
        "            mb_idx = idxs[start:end]\n",
        "\n",
        "            obs = batch.obs[mb_idx]\n",
        "            actions = batch.actions[mb_idx]\n",
        "            old_log_probs = batch.log_probs[mb_idx]\n",
        "            returns = batch.returns[mb_idx]\n",
        "            advantages = batch.advantages[mb_idx]\n",
        "\n",
        "            dist, values = model(obs)\n",
        "            new_log_probs = dist.log_prob(actions)\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            # ratio = π_new / π_old\n",
        "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "            # clipped surrogate objective\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1.0 - CLIP_EPS, 1.0 + CLIP_EPS) * advantages\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            value_loss = (returns - values).pow(2).mean()\n",
        "\n",
        "            loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  Training (with reward logging)\n",
        "# ==========================\n",
        "def train():\n",
        "    # note render_mode=None for training\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\")\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    model = ActorCritic(obs_dim, act_dim, hidden_size=128).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    running_reward = None\n",
        "    avg_returns_history = []   # NEW: per-update average \"return\" proxy\n",
        "    running_history = []       # NEW: smoothed version\n",
        "\n",
        "    for update in range(1, MAX_UPDATES + 1):\n",
        "        batch = collect_rollout(env, model, STEPS_PER_UPDATE)\n",
        "\n",
        "        # logging reward info from rollout\n",
        "        approx_return = batch.returns.mean().item()\n",
        "        if running_reward is None:\n",
        "            running_reward = approx_return\n",
        "        else:\n",
        "            running_reward = 0.99 * running_reward + 0.01 * approx_return\n",
        "\n",
        "        avg_returns_history.append(approx_return)\n",
        "        running_history.append(running_reward)\n",
        "\n",
        "        print(\n",
        "            f\"Update {update:4d} | \"\n",
        "            f\"Approx return: {approx_return:6.2f} | \"\n",
        "            f\"Running avg: {running_reward:6.2f}\"\n",
        "        )\n",
        "\n",
        "        ppo_update(model, optimizer, batch)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # save model\n",
        "    torch.save(model.state_dict(), MODEL_PATH)\n",
        "    print(\"Training finished. Model saved to\", MODEL_PATH)\n",
        "\n",
        "    return model, avg_returns_history, running_history\n",
        "\n",
        "model, avg_hist, run_hist = train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'avg_hist' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: return = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m     env.close()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m plot_rewards(\u001b[43mavg_hist\u001b[49m, run_hist)\n",
            "\u001b[31mNameError\u001b[39m: name 'avg_hist' is not defined"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Plot rewards\n",
        "# ==========================\n",
        "# ==========================\n",
        "#  Plot rewards (skip initial burn-in)\n",
        "# ==========================\n",
        "def plot_rewards(avg_returns_history, running_history, skip=300):\n",
        "    \"\"\"\n",
        "    Plot rewards but skip the first `skip` updates (burn-in period).\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    n = len(avg_returns_history)\n",
        "    skip = min(skip, n - 1) if n > 1 else 0  # safety\n",
        "\n",
        "    # x-axis = update index (1-based), starting after burn-in\n",
        "    xs = np.arange(skip, n)\n",
        "\n",
        "    plt.plot(xs, avg_returns_history[skip:], label=\"Per-update avg return\")\n",
        "    plt.plot(xs, running_history[skip:], label=\"Smoothed return\")\n",
        "\n",
        "    plt.xlabel(\"Update\")\n",
        "    plt.ylabel(\"Return (approx)\")\n",
        "    plt.title(f\"PPO on Pong (GT)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  Watch trained agent\n",
        "# ==========================\n",
        "def watch_agent(model=None, n_episodes=3):\n",
        "    \"\"\"\n",
        "    Renders the agent playing Pong.\n",
        "    - If `model` is None, loads weights from MODEL_PATH.\n",
        "    - Uses render_mode='human' (window) – good for local runs.\n",
        "      In a notebook, switch to render_mode='rgb_array' and draw frames manually.\n",
        "    \"\"\"\n",
        "    # for local desktop: render_mode=\"human\"\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=\"human\")\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    if model is None:\n",
        "        model = ActorCritic(obs_dim, act_dim, hidden_size=128).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        obs = np.array(obs, dtype=np.float32)\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            obs_tensor = torch.from_numpy(obs).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                dist, _ = model(obs_tensor)\n",
        "                action = dist.probs.argmax(dim=-1)  # greedy for demo\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "            obs = np.array(next_obs, dtype=np.float32)\n",
        "\n",
        "        print(f\"Episode {ep + 1}: return = {ep_return}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "plot_rewards(avg_hist, run_hist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is -1569516431\n",
            "/var/folders/7k/gy0ltxvs51sf5w1stk7h06vw0000gn/T/ipykernel_82457/789450303.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: return = 8.0\n",
            "Episode 2: return = 12.0\n",
            "Episode 3: return = 16.0\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Watch trained agent\n",
        "# ==========================\n",
        "def watch_agent(model=None, n_episodes=3):\n",
        "    \"\"\"\n",
        "    Renders the agent playing Pong.\n",
        "    - If `model` is None, loads weights from MODEL_PATH.\n",
        "    - Uses render_mode='human' (window) – good for local runs.\n",
        "      In a notebook, switch to render_mode='rgb_array' and draw frames manually.\n",
        "    \"\"\"\n",
        "    # for local desktop: render_mode=\"human\"\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=\"human\")\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    if model is None:\n",
        "        model = ActorCritic(obs_dim, act_dim, hidden_size=128).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        obs = np.array(obs, dtype=np.float32)\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "\n",
        "        while not done:\n",
        "            obs_tensor = torch.from_numpy(obs).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                dist, _ = model(obs_tensor)\n",
        "                action = dist.probs.argmax(dim=-1)  # greedy for demo\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "            obs = np.array(next_obs, dtype=np.float32)\n",
        "\n",
        "        print(f\"Episode {ep + 1}: return = {ep_return}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "watch_agent(None, n_episodes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is -408401050\n",
            "/var/folders/7k/gy0ltxvs51sf5w1stk7h06vw0000gn/T/ipykernel_82457/1765489412.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: return = -21.0\n",
            "Episode 2: return = -21.0\n",
            "Episode 3: return = -21.0\n",
            "Videos saved to: videos/\n"
          ]
        }
      ],
      "source": [
        "import imageio.v2 as imageio\n",
        "\n",
        "def watch_agent_and_save(model=None, n_episodes=3, video_prefix=\"pong_ep\", fps=30):\n",
        "    \"\"\"\n",
        "    Runs the trained agent, saves each episode as an MP4.\n",
        "    Assumes obs preprocessing matches training (adjust /255.0 if needed).\n",
        "    \"\"\"\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
        "\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    if model is None:\n",
        "        model = ActorCritic(obs_dim, act_dim, hidden_size=128).to(DEVICE)\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        # IMPORTANT: match training preprocessing here\n",
        "        obs = np.array(obs, dtype=np.float32)           # or /255.0 if you trained that way\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "        frames = []\n",
        "\n",
        "        while not done:\n",
        "            # render current frame\n",
        "            frame = env.render()        # H x W x 3 RGB\n",
        "            frames.append(frame)\n",
        "\n",
        "            obs_tensor = torch.from_numpy(obs).unsqueeze(0).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                dist, _ = model(obs_tensor)\n",
        "                action = dist.probs.argmax(dim=-1)  # greedy\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "            obs = np.array(next_obs, dtype=np.float32)  # or /255.0\n",
        "\n",
        "        print(f\"Episode {ep + 1}: return = {ep_return}\")\n",
        "\n",
        "        # save video\n",
        "        filename = f\"{video_prefix}{ep}.mp4\"\n",
        "        imageio.mimsave(filename, frames, fps=fps)\n",
        "        print(f\"Saved {filename}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "watch_agent(None, n_episodes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl-pong",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
