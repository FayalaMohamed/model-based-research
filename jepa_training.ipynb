{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lioAspSqb8Mi"
      },
      "source": [
        "# Pong Policy Training Baseline with Stable-Baselines3\n",
        "\n",
        "This notebook implements a baseline policy training pipeline using Stable-Baselines3 (SB3) with vectorized environments, training directly on RAM observations. This serves as a comparison baseline for the V-JEPA2 and RSSM approaches.\n",
        "\n",
        "## Key Improvements\n",
        "\n",
        "- **Vectorized Environments**: Uses parallel environments for faster data collection\n",
        "- **Optimized PPO**: Uses SB3's highly optimized PPO implementation\n",
        "- **Better GPU Utilization**: Efficient batching and tensor operations\n",
        "\n",
        "## Environment Notes\n",
        "\n",
        "- **Google Colab**: Use GPU runtime for faster training\n",
        "- **Local Apple Silicon (M1/M2)**: Will use CPU (MPS support in SB3 is limited)\n",
        "- Adjust `n_envs` based on your CPU cores (4-8 is typical)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XbcPU7Wb8Mj",
        "outputId": "6d8201d4-8ed9-4e63-ab5b-631bf6b8475e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Running on Google Colab - TensorBoard extension loaded\n",
            "Dependencies installed successfully!\n",
            "Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted at /content/drive/MyDrive/rl-pong\n",
            "CUDA available: NVIDIA A100-SXM4-40GB\n",
            "CUDA memory: 42.5 GB\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Install dependencies (run once per runtime)\n",
        "# ==========================\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py tensorboard \"transformers>=4.44.0\"\n",
        "\n",
        "# For Colab: Install tensorboard extension for inline viewing\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    %load_ext tensorboard\n",
        "    print(\"Running on Google Colab - TensorBoard extension loaded\")\n",
        "except Exception:\n",
        "    print(\"Running locally - TensorBoard can be viewed with: tensorboard --logdir <log_dir>\")\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n",
        "\n",
        "# ==========================\n",
        "#  Imports and setup\n",
        "# ==========================\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import ale_py\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "import ray\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import ResizeObservation, TransformObservation\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Colab-specific: Mount Google Drive (optional, for saving models)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    USE_DRIVE = True\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/rl-pong'\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "    print(f\"Google Drive mounted at {DRIVE_PATH}\")\n",
        "except Exception:\n",
        "    USE_DRIVE = False\n",
        "    print(\"Google Drive not available (running locally)\")\n",
        "\n",
        "# GPU optimization settings\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miPG-Ic9b8Mj",
        "outputId": "9d6b6b6d-6e5f-4bd1-a76a-5e0ec7b1164b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Running on Google Colab - TensorBoard extension loaded\n",
            "Dependencies installed successfully!\n",
            "Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted at /content/drive/MyDrive/rl-pong\n",
            "CUDA available: NVIDIA A100-SXM4-40GB\n",
            "CUDA memory: 42.5 GB\n",
            "Models will be saved to: /content/drive/MyDrive/rl-pong/ppo_pong_vjepa_latent_rllib\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 03:50:26,002\tINFO worker.py:2023 -- Started a local Ray instance.\n",
            "2025-11-24 03:50:27,741\tWARNING algorithm_config.py:5087 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment 'ALE/Pong-v5' (V-JEPA latent) will be used with RLlib\n",
            "Ray initialized for RLlib\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:644: UserWarning: \u001b[33mWARN: Overriding environment rllib-single-agent-env-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VJEPA] Loading V-JEPA 2 encoder on cpu from HF repo: facebook/vjepa2-vitl-fpc64-256\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 03:50:28,646\tWARNING algorithm_config.py:5087 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
            "2025-11-24 03:50:28,904\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RLlib PPO configured (new API stack, MLP on V-JEPA latents)\n",
            "Total timesteps target: 500,000\n",
            "Env runners: 0, envs/runner: 1\n",
            "Steps per update per env: 2048\n",
            "Train batch size per update: 2048\n",
            "\n",
            "Starting training...\n",
            "============================================================\n",
            "Training for 244 updates (~500,000 timesteps)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-24 03:50:53,608 E 30904 30904] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[33m(raylet)\u001b[0m [2025-11-24 03:50:55,947 E 31017 31017] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
            "\u001b[36m(pid=31069)\u001b[0m [2025-11-24 03:50:57,720 E 31069 31177] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Update    1/244 | Reward:  -21.00 | Length:  764.0 | Timesteps: 2,048\n",
            "Update    2/244 | Reward:  -20.75 | Length:  859.8 | Timesteps: 4,096\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Install dependencies (run once per runtime)\n",
        "# ==========================\n",
        "%pip install -q \"ray[rllib]\" \"gymnasium[atari]\" ale-py tensorboard \"transformers>=4.44.0\"\n",
        "\n",
        "# For Colab: Install tensorboard extension for inline viewing\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    %load_ext tensorboard\n",
        "    print(\"Running on Google Colab - TensorBoard extension loaded\")\n",
        "except Exception:\n",
        "    print(\"Running locally - TensorBoard can be viewed with: tensorboard --logdir <log_dir>\")\n",
        "\n",
        "print(\"Dependencies installed successfully!\")\n",
        "print(\"Note: Make sure you're using a GPU runtime (Runtime > Change runtime type > GPU) for faster training\")\n",
        "\n",
        "# ==========================\n",
        "#  Imports and setup\n",
        "# ==========================\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import ale_py\n",
        "import ray\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "from gymnasium import spaces\n",
        "from gymnasium.wrappers import ResizeObservation\n",
        "\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "\n",
        "# Colab-specific: Mount Google Drive (optional, for saving models)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    USE_DRIVE = True\n",
        "    DRIVE_PATH = '/content/drive/MyDrive/rl-pong'\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "    print(f\"Google Drive mounted at {DRIVE_PATH}\")\n",
        "except Exception:\n",
        "    USE_DRIVE = False\n",
        "    print(\"Google Drive not available (running locally)\")\n",
        "\n",
        "# GPU info (for PPO / policy net)\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "\n",
        "# Register ALE envs\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# ==========================\n",
        "#  Hyperparameters\n",
        "# ==========================\n",
        "ENV_ID = \"ALE/Pong-v5\"\n",
        "\n",
        "HF_REPO = \"facebook/vjepa2-vitl-fpc64-256\"\n",
        "IMG_SIZE = 256\n",
        "LATENT_DIM = 1024    # V-JEPA 2 ViT-L feature dim\n",
        "\n",
        "DEVICE_PPO = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# PPO hyperparams\n",
        "GAMMA = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "CLIP_RANGE = 0.2\n",
        "LR = 2.5e-4\n",
        "VF_COEF = 0.5\n",
        "ENT_COEF = 0.01\n",
        "\n",
        "# Training / parallelism\n",
        "STEPS_PER_UPDATE = 2048        # per env per update\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "NUM_ENV_RUNNERS = 0            # number of RLlib env runner processes\n",
        "NUM_ENVS_PER_RUNNER = 1        # envs per runner (vectorized within a process)\n",
        "\n",
        "TOTAL_TIMESTEPS = 500_000    # start smaller; bump once working\n",
        "MODEL_PATH = \"ppo_pong_vjepa_latent_rllib\"\n",
        "EVAL_INTERVAL = 0              # disable evaluation to keep it simple\n",
        "\n",
        "if USE_DRIVE:\n",
        "    MODEL_PATH = os.path.join(DRIVE_PATH, MODEL_PATH)\n",
        "    os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "    print(f\"Models will be saved to: {MODEL_PATH}\")\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  Global encoder cache (per process)\n",
        "# ==========================\n",
        "_GLOBAL_VJEPA_ENCODER = None\n",
        "\n",
        "def get_vjepa_encoder():\n",
        "    \"\"\"\n",
        "    Returns a (per-process) singleton V-JEPA encoder on CPU.\n",
        "    This avoids re-loading the huge HF model for every env instance.\n",
        "    \"\"\"\n",
        "    global _GLOBAL_VJEPA_ENCODER\n",
        "    if _GLOBAL_VJEPA_ENCODER is None:\n",
        "        _GLOBAL_VJEPA_ENCODER = FrozenVJEPAEncoder(latent_dim=LATENT_DIM, device=\"cpu\")\n",
        "    return _GLOBAL_VJEPA_ENCODER\n",
        "\n",
        "\n",
        "# ==========================\n",
        "#  V-JEPA encoder (CPU)\n",
        "# ==========================\n",
        "class FrozenVJEPAEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Frozen V-JEPA 2 encoder using Hugging Face.\n",
        "\n",
        "    Expects:\n",
        "      x: (B, C, H, W) float32 in [0, 1]\n",
        "    Returns:\n",
        "      z: (B, LATENT_DIM) pooled V-JEPA 2 features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim: int, hf_repo: str = HF_REPO, device: str = \"cpu\"):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        # IMPORTANT: For envs we keep this on CPU to avoid Ray GPU issues\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.hf_repo = hf_repo\n",
        "\n",
        "        print(f\"[VJEPA] Loading V-JEPA 2 encoder on {self.device} from HF repo: {hf_repo}\")\n",
        "        self.processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "        self.model = AutoModel.from_pretrained(hf_repo).to(self.device)\n",
        "\n",
        "        self.model.eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W), float32 in [0,1]\n",
        "        returns: (B, latent_dim)\n",
        "        \"\"\"\n",
        "        # [0,1] -> uint8 on CPU\n",
        "        x = (x.clamp(0.0, 1.0) * 255.0).to(torch.uint8)  # (B, C, H, W)\n",
        "        x_np = x.permute(0, 2, 3, 1).cpu().numpy()       # (B, H, W, C)\n",
        "        images = [img for img in x_np]\n",
        "\n",
        "        processed = self.processor(images, return_tensors=\"pt\")\n",
        "        pixel_values = processed[\"pixel_values_videos\"].to(self.device)  # (B, T, C, H, W)\n",
        "\n",
        "        # Repeat frames along time dimension for JEPA (simple hack)\n",
        "        pixel_values = pixel_values.repeat(1, 16, 1, 1, 1)\n",
        "\n",
        "        features = self.model.get_vision_features(pixel_values)  # (B, num_patches, D)\n",
        "        z = features.mean(dim=1)  # (B, D)\n",
        "        return z\n",
        "\n",
        "# ==========================\n",
        "#  Pong env that outputs V-JEPA latents\n",
        "# ==========================\n",
        "class PongVJEPAEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Pong env with observations = V-JEPA latent vector (1024-D).\n",
        "\n",
        "    Pipeline per step:\n",
        "      - step ALE/Pong-v5 (obs_type='rgb')\n",
        "      - Resize to (IMG_SIZE, IMG_SIZE)\n",
        "      - Run FrozenVJEPAEncoder on the frame (CPU)\n",
        "      - Return z ∈ ℝ^{LATENT_DIM}\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        super().__init__()\n",
        "        gym.register_envs(ale_py)\n",
        "\n",
        "        # Base Pong env with RGB observations\n",
        "        base_env = gym.make(ENV_ID, obs_type=\"rgb\")\n",
        "\n",
        "        # Resize to IMG_SIZE x IMG_SIZE\n",
        "        self._env = ResizeObservation(base_env, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        # V-JEPA encoder (CPU)\n",
        "        # self.encoder = FrozenVJEPAEncoder(latent_dim=LATENT_DIM, device=\"cpu\")\n",
        "        self.encoder = get_vjepa_encoder()\n",
        "\n",
        "\n",
        "        # Latent observation space\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(LATENT_DIM,),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.action_space = self._env.action_space\n",
        "\n",
        "    def _encode_obs(self, obs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        obs: (H, W, C) uint8 [0,255] -> z: (LATENT_DIM,) float32\n",
        "        \"\"\"\n",
        "        x = torch.from_numpy(obs).permute(2, 0, 1).unsqueeze(0).float() / 255.0  # (1, C, H, W)\n",
        "        with torch.no_grad():\n",
        "            z = self.encoder(x)  # (1, D)\n",
        "        return z.squeeze(0).cpu().numpy().astype(np.float32)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        obs, info = self._env.reset(seed=seed, options=options)\n",
        "        z = self._encode_obs(obs)\n",
        "        return z, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self._env.step(action)\n",
        "        z = self._encode_obs(obs)\n",
        "        return z, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        return self._env.render()\n",
        "\n",
        "    def close(self):\n",
        "        self._env.close()\n",
        "\n",
        "# ==========================\n",
        "#  Initialize Ray\n",
        "# ==========================\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, num_cpus=NUM_ENV_RUNNERS + 1)\n",
        "print(f\"Environment '{ENV_ID}' (V-JEPA latent) will be used with RLlib\")\n",
        "print(\"Ray initialized for RLlib\")\n",
        "\n",
        "# ==========================\n",
        "#  PPO config (new API stack, default MLP on latents)\n",
        "# ==========================\n",
        "TRAIN_BATCH_SIZE = STEPS_PER_UPDATE\n",
        "\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .framework(\"torch\")\n",
        "    .environment(PongVJEPAEnv)\n",
        "    .env_runners(\n",
        "        num_env_runners=NUM_ENV_RUNNERS,\n",
        "        num_envs_per_env_runner=NUM_ENVS_PER_RUNNER,\n",
        "        rollout_fragment_length=STEPS_PER_UPDATE,\n",
        "        sample_timeout_s=600,            # <-- give it 10 minutes if needed\n",
        "    )\n",
        "    .training(\n",
        "        lr=LR,\n",
        "        gamma=GAMMA,\n",
        "        lambda_=GAE_LAMBDA,\n",
        "        clip_param=CLIP_RANGE,\n",
        "        vf_loss_coeff=VF_COEF,\n",
        "        entropy_coeff=ENT_COEF,\n",
        "        train_batch_size=TRAIN_BATCH_SIZE,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        model={\n",
        "            \"fcnet_hiddens\": [256, 256],\n",
        "            \"fcnet_activation\": \"relu\",\n",
        "            \"vf_share_layers\": True,\n",
        "        },\n",
        "    )\n",
        "    .resources(\n",
        "        num_gpus=1 if torch.cuda.is_available() else 0,\n",
        "    )\n",
        ")\n",
        "\n",
        "algo = config.build_algo()\n",
        "\n",
        "\n",
        "print(\"RLlib PPO configured (new API stack, MLP on V-JEPA latents)\")\n",
        "print(f\"Total timesteps target: {TOTAL_TIMESTEPS:,}\")\n",
        "print(f\"Env runners: {NUM_ENV_RUNNERS}, envs/runner: {NUM_ENVS_PER_RUNNER}\")\n",
        "print(f\"Steps per update per env: {STEPS_PER_UPDATE}\")\n",
        "print(f\"Train batch size per update: {TRAIN_BATCH_SIZE}\")\n",
        "\n",
        "# ==========================\n",
        "#  Training Loop\n",
        "# ==========================\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = []\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "\n",
        "NUM_UPDATES = TOTAL_TIMESTEPS // TRAIN_BATCH_SIZE\n",
        "print(f\"Training for {NUM_UPDATES} updates (~{TOTAL_TIMESTEPS:,} timesteps)\")\n",
        "\n",
        "for i in range(NUM_UPDATES):\n",
        "    result = algo.train()\n",
        "    results.append(result)\n",
        "\n",
        "    env_metrics = result.get(\"env_runners\", {}) or {}\n",
        "\n",
        "    reward = env_metrics.get(\n",
        "        \"episode_return_mean\",\n",
        "        result.get(\"episode_reward_mean\", 0.0),\n",
        "    )\n",
        "    length = env_metrics.get(\n",
        "        \"episode_len_mean\",\n",
        "        result.get(\"episode_len_mean\", 0.0),\n",
        "    )\n",
        "    timesteps = env_metrics.get(\n",
        "        \"num_env_steps_sampled_lifetime\",\n",
        "        result.get(\"timesteps_total\", 0),\n",
        "    )\n",
        "\n",
        "    episode_rewards.append(reward)\n",
        "    episode_lengths.append(length)\n",
        "\n",
        "    print(\n",
        "        f\"Update {i+1:4d}/{NUM_UPDATES} | \"\n",
        "        f\"Reward: {reward:7.2f} | \"\n",
        "        f\"Length: {length:6.1f} | \"\n",
        "        f\"Timesteps: {timesteps:,}\"\n",
        "    )\n",
        "\n",
        "    # Optional: checkpoint every N updates\n",
        "    if (i + 1) % 50 == 0:\n",
        "        algo.save(MODEL_PATH)\n",
        "\n",
        "# Save final model\n",
        "checkpoint_path = algo.save(MODEL_PATH)\n",
        "print(f\"\\nTraining finished. Model saved to {checkpoint_path}\")\n",
        "\n",
        "# ==========================\n",
        "#  Final Metrics + Simple Plot\n",
        "# ==========================\n",
        "print(\"\\nFinal metrics (last 100 updates):\")\n",
        "if episode_rewards:\n",
        "    import numpy as np\n",
        "    print(f\"  Mean reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
        "    print(f\"  Best reward: {max(episode_rewards):.2f}\")\n",
        "\n",
        "    window = min(20, len(episode_rewards))\n",
        "    ma = np.convolve(\n",
        "        episode_rewards, np.ones(window) / window, mode=\"valid\"\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(episode_rewards, label=\"Episode return mean (per update)\")\n",
        "    plt.plot(range(window - 1, window - 1 + len(ma)), ma, label=f\"{window}-update MA\")\n",
        "    plt.xlabel(\"Update\")\n",
        "    plt.ylabel(\"Return (env score)\")\n",
        "    plt.title(\"PPO on Pong with V-JEPA latents (RLlib)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nTo view training progress with TensorBoard, run:\")\n",
        "print(f\"  tensorboard --logdir {MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIuT5TZeb8Mk",
        "outputId": "b6e77431-b26b-4f6a-9a44-907a412b6ac1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'avg_hist' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: return = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m     env.close()\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m plot_rewards(\u001b[43mavg_hist\u001b[49m, run_hist)\n",
            "\u001b[31mNameError\u001b[39m: name 'avg_hist' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_learning_curves(results):\n",
        "    \"\"\"Extract timesteps, mean episode return, and mean episode length from RLlib results.\"\"\"\n",
        "    timesteps = []\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "\n",
        "    for r in results:\n",
        "        env_metrics = r.get(\"env_runners\", {}) or {}\n",
        "\n",
        "        t = env_metrics.get(\n",
        "            \"num_env_steps_sampled_lifetime\",\n",
        "            r.get(\"timesteps_total\", 0),\n",
        "        )\n",
        "        rew = env_metrics.get(\n",
        "            \"episode_return_mean\",\n",
        "            r.get(\"episode_reward_mean\", np.nan),\n",
        "        )\n",
        "        leng = env_metrics.get(\n",
        "            \"episode_len_mean\",\n",
        "            r.get(\"episode_len_mean\", np.nan),\n",
        "        )\n",
        "\n",
        "        timesteps.append(t)\n",
        "        rewards.append(rew)\n",
        "        lengths.append(leng)\n",
        "\n",
        "    return np.array(timesteps), np.array(rewards), np.array(lengths)\n",
        "\n",
        "\n",
        "def plot_training(results, smooth_window=10):\n",
        "    \"\"\"\n",
        "    Plot reward (and optionally episode length) vs timesteps.\n",
        "    `smooth_window` applies a moving average to the reward curve.\n",
        "    \"\"\"\n",
        "    ts, rew, leng = extract_learning_curves(results)\n",
        "\n",
        "    # Simple moving average smoothing for rewards\n",
        "    if smooth_window > 1 and len(rew) >= smooth_window:\n",
        "        kernel = np.ones(smooth_window) / smooth_window\n",
        "        rew_smooth = np.convolve(rew, kernel, mode=\"valid\")\n",
        "        ts_smooth = ts[smooth_window - 1 :]\n",
        "    else:\n",
        "        rew_smooth = rew\n",
        "        ts_smooth = ts\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "    # Reward curve\n",
        "    ax1.plot(ts_smooth, rew_smooth, label=\"Mean episode reward\")\n",
        "    ax1.set_xlabel(\"Timesteps\")\n",
        "    ax1.set_ylabel(\"Reward\")\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Optional: overlay episode length on secondary axis\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(ts, leng, alpha=0.3, linestyle=\"--\", label=\"Episode length\")\n",
        "    ax2.set_ylabel(\"Episode length\")\n",
        "\n",
        "    # Combine legends\n",
        "    lines, labels = ax1.get_legend_handles_labels()\n",
        "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "    ax1.legend(lines + lines2, labels + labels2, loc=\"lower right\")\n",
        "\n",
        "    plt.title(\"PPO Training on Pong (RAM)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLDq5M-b8Mk",
        "outputId": "acf33e18-d423-46a3-ff32-a2202e81889b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "/var/folders/7k/gy0ltxvs51sf5w1stk7h06vw0000gn/T/ipykernel_82457/789450303.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is 281464273\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: return = 14.0\n",
            "Episode 2: return = 3.0\n",
            "Episode 3: return = 16.0\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Watch Trained Agent (RLlib)\n",
        "# ==========================\n",
        "def watch_agent_rllib(checkpoint_path=None, n_episodes=3, render_mode=\"human\"):\n",
        "    \"\"\"\n",
        "    Renders the agent playing Pong using RLlib model.\n",
        "    - If `checkpoint_path` is None, loads from MODEL_PATH.\n",
        "    - render_mode: 'human' for window, 'rgb_array' for frames\n",
        "    \"\"\"\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = MODEL_PATH\n",
        "\n",
        "    # Load algorithm from checkpoint\n",
        "    algo = config.build()\n",
        "    algo.restore(checkpoint_path)\n",
        "    print(f\"Loaded model from {checkpoint_path}\")\n",
        "\n",
        "    # Create environment\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=render_mode)\n",
        "\n",
        "    returns = []\n",
        "    for ep in range(n_episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            # RLlib: compute_action returns action\n",
        "            action = algo.compute_single_action(obs, explore=False)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "            steps += 1\n",
        "\n",
        "        returns.append(ep_return)\n",
        "        print(f\"Episode {ep + 1}: return = {ep_return:.1f}, length = {steps}\")\n",
        "\n",
        "    env.close()\n",
        "    print(f\"\\nMean return over {n_episodes} episodes: {np.mean(returns):.2f} ± {np.std(returns):.2f}\")\n",
        "    return returns\n",
        "\n",
        "# Evaluate the trained model\n",
        "# Uncomment to run:\n",
        "# watch_agent_rllib(n_episodes=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOU1c4E-b8Mk",
        "outputId": "5d8fe8ab-fab5-44f9-90e7-d7fb085f736a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.11.0+unknown)\n",
            "[Powered by Stella]\n",
            "Game console created:\n",
            "  ROM file:  /Users/lavan/miniconda3/envs/rl-pong/lib/python3.11/site-packages/ale_py/roms/pong.bin\n",
            "  Cart Name: Video Olympics (1978) (Atari)\n",
            "  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n",
            "  Display Format:  AUTO-DETECT ==> NTSC\n",
            "  ROM Size:        2048\n",
            "  Bankswitch Type: AUTO-DETECT ==> 2K\n",
            "\n",
            "Running ROM file...\n",
            "Random seed is -408401050\n",
            "/var/folders/7k/gy0ltxvs51sf5w1stk7h06vw0000gn/T/ipykernel_82457/1765489412.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: return = -21.0\n",
            "Episode 2: return = -21.0\n",
            "Episode 3: return = -21.0\n",
            "Videos saved to: videos/\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "#  Save Agent Videos (RLlib)\n",
        "# ==========================\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "def watch_agent_and_save_rllib(checkpoint_path=None, n_episodes=3, video_prefix=\"pong_ep\", fps=30):\n",
        "    \"\"\"\n",
        "    Runs the trained agent and saves each episode as an MP4 using RLlib model.\n",
        "    \"\"\"\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = MODEL_PATH\n",
        "\n",
        "    # Load algorithm from checkpoint\n",
        "    algo = config.build()\n",
        "    algo.restore(checkpoint_path)\n",
        "    print(f\"Loaded model from {checkpoint_path}\")\n",
        "\n",
        "    # Create environment with rgb_array rendering\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "        frames = []\n",
        "\n",
        "        while not done:\n",
        "            # Render current frame\n",
        "            frame = env.render()\n",
        "            frames.append(frame)\n",
        "\n",
        "            # Get action from RLlib algorithm\n",
        "            action = algo.compute_single_action(obs, explore=False)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "\n",
        "        # Save video\n",
        "        filename = f\"{video_prefix}_{ep}.mp4\"\n",
        "        imageio.mimsave(filename, frames, fps=fps)\n",
        "        print(f\"Episode {ep + 1}: return = {ep_return:.1f}, saved to {filename}\")\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Videos saved with prefix: {video_prefix}\")\n",
        "\n",
        "# Uncomment to save videos:\n",
        "# watch_agent_and_save_rllib(n_episodes=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZprRTXueb8Mk"
      },
      "source": [
        "## TensorBoard Integration (Colab)\n",
        "\n",
        "View training progress in real-time using TensorBoard inline in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYyQ22hib8Mk"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "#  Start TensorBoard (Colab)\n",
        "# ==========================\n",
        "# This will display TensorBoard inline in Colab\n",
        "# Run this cell after training starts to view progress\n",
        "\n",
        "log_dir = f\"{MODEL_PATH}_logs/\"\n",
        "\n",
        "try:\n",
        "    from google.colab import output\n",
        "    # Start TensorBoard in background\n",
        "    %tensorboard --logdir {log_dir} --port 6006\n",
        "except:\n",
        "    print(f\"TensorBoard logs available at: {log_dir}\")\n",
        "    print(\"Run: tensorboard --logdir\", log_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GLOFa4db8Mk"
      },
      "source": [
        "## Visualization and Evaluation Tools\n",
        "\n",
        "Enhanced visualization functions for viewing agent performance and comparing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6HIjNlHb8Mk"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "#  Enhanced Evaluation & Visualization\n",
        "# ==========================\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_agent_comprehensive_rllib(checkpoint_path=None, n_episodes=10, render_video=True):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation: runs episodes and optionally saves video.\n",
        "    Returns stats and optionally displays video in Colab.\n",
        "    \"\"\"\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_path = MODEL_PATH\n",
        "\n",
        "    # Load algorithm from checkpoint\n",
        "    algo = config.build()\n",
        "    algo.restore(checkpoint_path)\n",
        "    print(f\"Loaded model from {checkpoint_path}\")\n",
        "\n",
        "    env = gym.make(ENV_ID, obs_type=\"ram\", render_mode=\"rgb_array\")\n",
        "\n",
        "    returns = []\n",
        "    lengths = []\n",
        "    videos = [] if render_video else None\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "        steps = 0\n",
        "        frames = [] if render_video else None\n",
        "\n",
        "        while not done:\n",
        "            if render_video:\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "\n",
        "            action = algo.compute_single_action(obs, explore=False)\n",
        "            obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ep_return += reward\n",
        "            steps += 1\n",
        "\n",
        "        returns.append(ep_return)\n",
        "        lengths.append(steps)\n",
        "        if render_video and frames:\n",
        "            videos.append(frames)\n",
        "\n",
        "        print(f\"Episode {ep+1:2d}: return = {ep_return:6.1f}, length = {steps:4d}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Print statistics\n",
        "    returns = np.array(returns)\n",
        "    lengths = np.array(lengths)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluation Results ({n_episodes} episodes):\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Mean return:  {returns.mean():7.2f} ± {returns.std():6.2f}\")\n",
        "    print(f\"Max return:   {returns.max():7.2f}\")\n",
        "    print(f\"Min return:   {returns.min():7.2f}\")\n",
        "    print(f\"Mean length:  {lengths.mean():7.1f} ± {lengths.std():6.1f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Display video in Colab (first episode)\n",
        "    if render_video and videos and len(videos) > 0:\n",
        "        print(\"\\nDisplaying first episode video...\")\n",
        "        display_video_colab(videos[0], fps=30)\n",
        "\n",
        "    return {\n",
        "        'returns': returns,\n",
        "        'lengths': lengths,\n",
        "        'mean_return': returns.mean(),\n",
        "        'std_return': returns.std(),\n",
        "        'videos': videos\n",
        "    }\n",
        "\n",
        "def display_video_colab(frames, fps=30):\n",
        "    \"\"\"\n",
        "    Display video inline in Colab notebook.\n",
        "    \"\"\"\n",
        "    import imageio\n",
        "    import tempfile\n",
        "\n",
        "    # Save to temporary file\n",
        "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    imageio.mimsave(tmp_path, frames, fps=fps)\n",
        "\n",
        "    # Read and encode\n",
        "    with open(tmp_path, 'rb') as f:\n",
        "        video_data = f.read()\n",
        "\n",
        "    video_base64 = base64.b64encode(video_data).decode('utf-8')\n",
        "    video_html = f'''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    '''\n",
        "    display(HTML(video_html))\n",
        "\n",
        "    # Cleanup\n",
        "    os.unlink(tmp_path)\n",
        "\n",
        "def compare_models_rllib(checkpoint_paths, model_names=None, n_episodes=10):\n",
        "    \"\"\"\n",
        "    Compare multiple trained models side-by-side using RLlib.\n",
        "    \"\"\"\n",
        "    if model_names is None:\n",
        "        model_names = [f\"Model {i+1}\" for i in range(len(checkpoint_paths))]\n",
        "\n",
        "    results = {}\n",
        "    for path, name in zip(checkpoint_paths, model_names):\n",
        "        print(f\"\\nEvaluating {name}...\")\n",
        "        algo = config.build()\n",
        "        algo.restore(path)\n",
        "        env = gym.make(ENV_ID, obs_type=\"ram\")\n",
        "\n",
        "        returns = []\n",
        "        for _ in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            ep_return = 0.0\n",
        "\n",
        "            while not done:\n",
        "                action = algo.compute_single_action(obs, explore=False)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                ep_return += reward\n",
        "\n",
        "            returns.append(ep_return)\n",
        "\n",
        "        env.close()\n",
        "        results[name] = {\n",
        "            'mean': np.mean(returns),\n",
        "            'std': np.std(returns),\n",
        "            'max': np.max(returns),\n",
        "            'returns': returns\n",
        "        }\n",
        "        print(f\"  Mean return: {results[name]['mean']:.2f} ± {results[name]['std']:.2f}\")\n",
        "\n",
        "    # Plot comparison\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    names = list(results.keys())\n",
        "    means = [results[n]['mean'] for n in names]\n",
        "    stds = [results[n]['std'] for n in names]\n",
        "\n",
        "    x = np.arange(len(names))\n",
        "    ax.bar(x, means, yerr=stds, capsize=5, alpha=0.7)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Mean Episode Return')\n",
        "    ax.set_title('Model Comparison')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test evaluation (uncomment after training):\n",
        "# results = evaluate_agent_comprehensive_rllib(n_episodes=5, render_video=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0BMQaZb8Mk"
      },
      "source": [
        "## Training Progress Dashboard\n",
        "\n",
        "Plot comprehensive training metrics from TensorBoard or monitor logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JROWRmczb8Mk"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "#  Training Progress Dashboard (RLlib)\n",
        "# ==========================\n",
        "def plot_comprehensive_training_metrics_rllib(results=None):\n",
        "    \"\"\"\n",
        "    Plot comprehensive training metrics from RLlib results.\n",
        "    \"\"\"\n",
        "    if results is None or len(results) == 0:\n",
        "        print(\"No training results available. Run training first.\")\n",
        "        return\n",
        "\n",
        "    # Extract all available metrics\n",
        "    rewards = [r.get(\"episode_reward_mean\", 0) for r in results]\n",
        "    lengths = [r.get(\"episode_len_mean\", 0) for r in results]\n",
        "    timesteps = [r.get(\"timesteps_total\", 0) for r in results]\n",
        "    policy_loss = [r.get(\"info\", {}).get(\"learner\", {}).get(\"default_policy\", {}).get(\"policy_loss\", 0) for r in results]\n",
        "    value_loss = [r.get(\"info\", {}).get(\"learner\", {}).get(\"default_policy\", {}).get(\"vf_loss\", 0) for r in results]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Episode rewards\n",
        "    axes[0, 0].plot(rewards)\n",
        "    axes[0, 0].set_title('Episode Return')\n",
        "    axes[0, 0].set_xlabel('Training Iteration')\n",
        "    axes[0, 0].set_ylabel('Mean Return')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Episode length\n",
        "    axes[0, 1].plot(lengths)\n",
        "    axes[0, 1].set_title('Episode Length')\n",
        "    axes[0, 1].set_xlabel('Training Iteration')\n",
        "    axes[0, 1].set_ylabel('Mean Length')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Policy loss (if available)\n",
        "    if any(policy_loss):\n",
        "        axes[1, 0].plot([p for p in policy_loss if p != 0])\n",
        "        axes[1, 0].set_title('Policy Loss')\n",
        "        axes[1, 0].set_xlabel('Training Iteration')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Value loss (if available)\n",
        "    if any(value_loss):\n",
        "        axes[1, 1].plot([v for v in value_loss if v != 0])\n",
        "        axes[1, 1].set_title('Value Loss')\n",
        "        axes[1, 1].set_xlabel('Training Iteration')\n",
        "        axes[1, 1].set_ylabel('Loss')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Plotting {len(results)} training iterations\")\n",
        "\n",
        "# Uncomment after training to see comprehensive plots:\n",
        "# plot_comprehensive_training_metrics_rllib(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qztXh1zXb8Mk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}