{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7764c9",
   "metadata": {},
   "source": [
    "# Gradient-based policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import ale_py\n",
    "import imageio\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", frameskip=1, repeat_action_probability=0.0, full_action_space=True)\n",
    "env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "env = gym.wrappers.ResizeObservation(env, (64, 64))\n",
    "\n",
    "\n",
    "def transform_obs(obs):\n",
    "    obs_t = np.transpose(obs, (2, 0, 1)).astype(np.float32) / 255.0\n",
    "    return obs_t\n",
    "\n",
    "new_obs_space = spaces.Box(low=0.0, high=1.0, shape=(3, 64, 64), dtype=np.float32)\n",
    "\n",
    "env = gym.wrappers.TransformObservation(\n",
    "    env,\n",
    "    func=transform_obs,\n",
    "    observation_space=new_obs_space,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8ff5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "assert obs.shape == (3, 64, 64), f\"Expected (3, 64, 64), got {obs.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4313439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "local_repo_path = os.path.abspath(os.path.join(os.getcwd(), 'world-models'))\n",
    "local_repo_path = os.path.abspath(os.path.join(os.getcwd(), '../world-models-fork'))\n",
    "if local_repo_path not in sys.path:\n",
    "    sys.path.append(local_repo_path)\n",
    "from models.rssm import RSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ef317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSSM encoder loaded.\n"
     ]
    }
   ],
   "source": [
    "from models.models import EncoderCNN, DecoderCNN, RewardModel\n",
    "from models.dynamics import DynamicsModel\n",
    "\n",
    "# Dimensions must match training\n",
    "action_dim = 18 \n",
    "hidden_size = 1024\n",
    "state_size = 32\n",
    "embedding_dim = 1024\n",
    "image_shape = (3, 64, 64)\n",
    "\n",
    "encoder = EncoderCNN(3, embedding_dim, (image_shape[1], image_shape[2])).to(device)\n",
    "decoder = DecoderCNN(hidden_size, state_size, embedding_dim, True, image_shape).to(device)\n",
    "reward_model = RewardModel(hidden_size, state_size).to(device)\n",
    "dynamics = DynamicsModel(hidden_size, action_dim, state_size, embedding_dim).to(device)\n",
    "\n",
    "rssm = RSSM(encoder, decoder, reward_model, dynamics, hidden_size, state_size, action_dim, embedding_dim, device)\n",
    "#checkpoint = torch.load(\"checkpoints/rssm/rssm_checkpoint_epoch_296.pth\", map_location=device)\n",
    "checkpoint = torch.load(\"checkpoints_less_diverse/rssm_best.pth\", map_location=device)\n",
    "rssm.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "rssm.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "rssm.reward_model.load_state_dict(checkpoint[\"reward_model\"])\n",
    "rssm.dynamics.load_state_dict(checkpoint[\"dynamics\"])\n",
    "rssm.eval()\n",
    "print(\"RSSM encoder loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d27ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, rssm):\n",
    "        super().__init__()\n",
    "        self.encoder = rssm.encoder\n",
    "    def forward(self, obs):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(obs_t)\n",
    "        return z.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPolicy(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50101fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RSSMFeatureExtractor(rssm).to(device)\n",
    "policy = LinearPolicy(embedding_dim, action_dim).to(device) # env.action_space.n\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, encoder, policy, rssm, gamma=0.99, seed=None, save_video=True):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        env.reset(seed=seed)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    log_probs, rewards = [], []\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    raw_frames, recon_frames = [], []\n",
    "\n",
    "    h = torch.zeros(1, hidden_size, device=device)\n",
    "    s = torch.zeros(1, state_size, device=device)\n",
    "    prev_action = torch.zeros(1, action_dim, device=device)\n",
    "\n",
    "    while not done:\n",
    "        frame = (obs * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "        raw_frames.append(frame)\n",
    "\n",
    "        z = encoder(obs)\n",
    "        logits = policy(z)\n",
    "        valid = env.action_space.n\n",
    "        logits[valid:] = -1e9\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_probs.append(dist.log_prob(action))\n",
    "\n",
    "        one_hot = torch.nn.functional.one_hot(\n",
    "            torch.tensor([action.item()], device=device),\n",
    "            rssm.action_dim\n",
    "        ).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h, s = rssm.step(h, s, one_hot, z.unsqueeze(0))\n",
    "            decoded = rssm.decoder(h, s)\n",
    "            recon = decoded[0].permute(1, 2, 0).cpu().numpy()\n",
    "            recon = np.clip(recon, 0.0, 1.0)\n",
    "            recon_frames.append((recon * 255).astype(np.uint8))\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        rewards.append(reward)\n",
    "        total_reward += reward\n",
    "        prev_action = one_hot\n",
    "\n",
    "    if save_video:\n",
    "        imageio.mimsave(\"rssm_raw.gif\", raw_frames, fps=15)\n",
    "        imageio.mimsave(\"rssm_recon.gif\", recon_frames, fps=15)\n",
    "        paired = [np.hstack((f, r)) for f, r in zip(raw_frames, recon_frames)]\n",
    "        imageio.mimsave(\"rssm_compare.gif\", paired, fps=15)\n",
    "        print(\"Saved: rssm_raw.gif, rssm_recon.gif, rssm_compare.gif\")\n",
    "\n",
    "    returns, G = [], 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "    loss = -torch.sum(torch.stack(log_probs) * returns)\n",
    "    return loss, float(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe37ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10 | Reward: 80.0\n",
      "Episode 2/10 | Reward: 110.0\n",
      "Episode 3/10 | Reward: 100.0\n",
      "Episode 4/10 | Reward: 120.0\n",
      "Episode 5/10 | Reward: 140.0\n",
      "Episode 6/10 | Reward: 100.0\n",
      "Episode 7/10 | Reward: 150.0\n",
      "Episode 8/10 | Reward: 140.0\n",
      "Episode 9/10 | Reward: 100.0\n",
      "Episode 10/10 | Reward: 120.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "reward_history = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    policy.train()\n",
    "    loss, total_reward = run_episode(env, encoder, policy, rssm, seed=ep)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    reward_history.append(float(total_reward))\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Episode {ep+1}/{num_episodes} | Reward: {total_reward:.1f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reward_history), type(reward_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"RSSM-based Linear Policy on Pong\")\n",
    "plt.savefig(\"reward_plot.png\")\n",
    "print(\"Saved to reward_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c14975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad37f43",
   "metadata": {},
   "source": [
    "# Satble baseline library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5204e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from gymnasium import spaces\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bce712",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoints_less_diverse/rssm_best.pth\", map_location=device)\n",
    "\n",
    "action_dim = 18\n",
    "hidden_size = 1024\n",
    "state_size = 32\n",
    "embedding_dim = 1024\n",
    "image_shape = (3, 64, 64)\n",
    "\n",
    "encoder_cnn = EncoderCNN(3, embedding_dim, image_shape[1:]).to(device)\n",
    "decoder = DecoderCNN(hidden_size, state_size, embedding_dim, True, image_shape).to(device)\n",
    "reward_model = RewardModel(hidden_size, state_size).to(device)\n",
    "dynamics = DynamicsModel(hidden_size, action_dim, state_size, embedding_dim).to(device)\n",
    "\n",
    "rssm = RSSM(encoder_cnn, decoder, reward_model, dynamics,\n",
    "            hidden_size, state_size, action_dim, embedding_dim, device)\n",
    "\n",
    "rssm.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "rssm.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSMEncodingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, encoder, embedding_dim, device):\n",
    "        super().__init__(env)\n",
    "        self.encoder = encoder\n",
    "        self.device = device\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(embedding_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(obs)\n",
    "\n",
    "        return z.squeeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ef36a",
   "metadata": {},
   "source": [
    "## Stack latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0709e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from gymnasium import spaces\n",
    "\n",
    "class RSSMFrameStackWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, encoder, embedding_dim, device, n_frames=4, use_encoding=True):\n",
    "        super().__init__(env)\n",
    "        self.encoder = encoder\n",
    "        self.device = device\n",
    "        self.n_frames = n_frames\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_encoding = use_encoding\n",
    "\n",
    "        self.buffer = deque(maxlen=n_frames)\n",
    "\n",
    "        if self.use_encoding:\n",
    "            obs_shape = (embedding_dim * n_frames,)\n",
    "            low, high = -np.inf, np.inf\n",
    "        else:\n",
    "            h, w, c = env.observation_space.shape\n",
    "            obs_shape = (n_frames * c, h, w)\n",
    "            low, high = 0.0, 1.0\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low, high=high, shape=obs_shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        z = self.encode_obs(obs)\n",
    "\n",
    "        self.buffer.clear()\n",
    "        for _ in range(self.n_frames):\n",
    "            self.buffer.append(z)\n",
    "\n",
    "        return self.get_stacked(), info\n",
    "\n",
    "    def observation(self, obs):\n",
    "        z = self.encode_obs(obs)\n",
    "        self.buffer.append(z)\n",
    "        return self.get_stacked()\n",
    "\n",
    "    def encode_obs(self, obs):\n",
    "        # Remove extra frame dimension if present\n",
    "        if obs.ndim == 4:\n",
    "            obs = obs[0]\n",
    "\n",
    "        if self.use_encoding:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\\\n",
    "                          .permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "            with torch.no_grad():\n",
    "                z = self.encoder(obs_t)\n",
    "            return z.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            return np.transpose(obs, (2,0,1)).astype(np.float32) / 255.0\n",
    "\n",
    "    def get_stacked(self):\n",
    "        return np.concatenate(list(self.buffer), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd89b5",
   "metadata": {},
   "source": [
    "#### PPO using encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "env = gym.wrappers.ResizeObservation(env, (64, 64))\n",
    "env = RSSMFrameStackWrapper(env, rssm.encoder, embedding_dim, device, n_frames=6)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e40b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RewardPlotCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals.get(\"infos\", []):\n",
    "            if \"episode\" in info:\n",
    "                self.episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RewardPlotCallback()\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    verbose=0,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=3_000_000, callback=callback)\n",
    "model.save(\"ppo_rssm_latent_policy_stack_latents_3million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74907a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training reward\")\n",
    "plt.savefig(\"ppo_rssm_reward_plot_stack_latents_3million.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cebf2",
   "metadata": {},
   "source": [
    "#### PPO using encodings - approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoded_env(n_stack=4):\n",
    "    def _make():\n",
    "        env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "\n",
    "        env = gym.wrappers.AtariPreprocessing(\n",
    "            env,\n",
    "            screen_size=64,\n",
    "            grayscale_obs=False,\n",
    "            scale_obs=False\n",
    "        )\n",
    "\n",
    "        env = RSSMFrameStackWrapper(\n",
    "            env,\n",
    "            encoder=encoder,\n",
    "            embedding_dim=embedding_dim,\n",
    "            device=device,\n",
    "            n_frames=n_stack,\n",
    "            use_encoding=True\n",
    "        )\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _make\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac908046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardRecorderCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        for info in self.locals[\"infos\"]:\n",
    "            if \"episode\" in info:\n",
    "                self.episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 8\n",
    "latent_stack = 4\n",
    "\n",
    "env = DummyVecEnv([make_encoded_env(latent_stack) for _ in range(n_envs)])\n",
    "\n",
    "callback = RewardRecorderCallback()\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.1,\n",
    "    ent_coef=0.01,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=3_000_000,\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "model.save(\"ppo_alien_latent_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training Rewards (PPO on RSSM Latents)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"alien_training_rewards_latent_2.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae46a8",
   "metadata": {},
   "source": [
    "#### PPO using direct observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93263f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "env = gym.wrappers.ResizeObservation(env, (64, 64))\n",
    "env = RSSMFrameStackWrapper(env, rssm.encoder, embedding_dim, device, n_frames=6, use_encoding=False)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_callback = RewardPlotCallback()\n",
    "\n",
    "obs_model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    verbose=0,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "obs_model.learn(total_timesteps=1_000_000, callback=obs_callback)\n",
    "obs_model.save(\"ppo_rssm_latent_policy_stack_latents_using_observations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc827396",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(obs_callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training reward\")\n",
    "plt.savefig(\"ppo_rssm_reward_plot_stack_latents_using_observations.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1988c62",
   "metadata": {},
   "source": [
    "#### PPO using direct observation (one observation, cnn policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "env = gym.wrappers.ResizeObservation(env, (64, 64))\n",
    "env = RSSMFrameStackWrapper(env, rssm.encoder, embedding_dim, device, n_frames=1, use_encoding=False)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_obs_callback = RewardPlotCallback()\n",
    "\n",
    "one_obs_model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    verbose=0,\n",
    "    device=\"cpu\",\n",
    "    policy_kwargs=dict(normalize_images=False)\n",
    ")\n",
    "\n",
    "one_obs_model.learn(total_timesteps=1_000_000, callback=one_obs_callback)\n",
    "one_obs_model.save(\"ppo_rssm_latent_policy_stack_latents_using_one_observation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(one_obs_callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training reward\")\n",
    "plt.savefig(\"ppo_rssm_reward_plot_stack_latents_using_one_observation.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3601d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardRecorderCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Vectorized env -> info is list of info dicts\n",
    "        for info in self.locals[\"infos\"]:\n",
    "            if \"episode\" in info:\n",
    "                ep_reward = info[\"episode\"][\"r\"]\n",
    "                self.episode_rewards.append(ep_reward)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"AlienNoFrameskip-v4\", n_envs=8, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "reward_callback = RewardRecorderCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef880f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.1,\n",
    "    ent_coef=0.01,\n",
    "    device=\"cuda\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6259bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(\n",
    "    total_timesteps=3_000_000,\n",
    "    callback=reward_callback\n",
    ")\n",
    "\n",
    "model.save(\"ppo_alien\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ffba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(reward_callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training Rewards for PPO on Alien\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"alien_training_rewards.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f8f76",
   "metadata": {},
   "source": [
    "## Use [z,h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSSMZPlusHWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, rssm, embedding_dim, hidden_size, state_size, action_dim, device):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.rssm = rssm\n",
    "        self.encoder = rssm.encoder\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.state_size = state_size\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.h = None  # deterministic RNN hidden state\n",
    "        self.s = None  # stochastic state\n",
    "\n",
    "        # PPO receives concat[z, h]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(embedding_dim + hidden_size,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        self.h = torch.zeros(1, self.hidden_size, device=self.device)\n",
    "        self.s = torch.zeros(1, self.state_size, device=self.device)\n",
    "\n",
    "        z = self.encode_obs(obs)\n",
    "\n",
    "        return self.concat_zh(z, self.h), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        z = self.encode_obs(obs)\n",
    "\n",
    "        a_onehot = self.one_hot(action)\n",
    "\n",
    "        actions = a_onehot.unsqueeze(0).unsqueeze(1)\n",
    "        actions = torch.cat([actions, actions], dim=1).to(self.device)\n",
    "\n",
    "        obs_seq = z.unsqueeze(0).unsqueeze(1)\n",
    "        obs_seq = torch.cat([obs_seq, obs_seq], dim=1).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h_seq, prior_states, post_states, _, _, _, _ = self.rssm.dynamics(\n",
    "                self.h,\n",
    "                self.s,\n",
    "                actions,\n",
    "                obs_seq\n",
    "            )\n",
    "\n",
    "        self.h = h_seq[:, -1]\n",
    "        self.s = post_states[:, -1]\n",
    "\n",
    "        return self.concat_zh(z, self.h), reward, terminated, truncated, info\n",
    "\n",
    "    def encode_obs(self, obs):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\\\n",
    "                      .permute(2, 0, 1).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(obs_t)\n",
    "        return z.squeeze(0).cpu()\n",
    "\n",
    "    def one_hot(self, action):\n",
    "        v = torch.zeros(self.action_dim, device=self.device)\n",
    "        v[action] = 1.0\n",
    "        return v\n",
    "\n",
    "    def concat_zh(self, z, h):\n",
    "        z = z if isinstance(z, torch.Tensor) else torch.tensor(z)\n",
    "        return torch.cat([z, h.squeeze(0).cpu()], dim=-1).numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"AlienNoFrameskip-v4\")\n",
    "env = gym.wrappers.ResizeObservation(env, (64, 64))\n",
    "\n",
    "env = RSSMZPlusHWrapper(\n",
    "    env,\n",
    "    rssm=rssm,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    state_size=state_size,\n",
    "    action_dim=action_dim,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ee9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RewardPlotCallback()\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    verbose=0,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=1_000, callback=callback)\n",
    "model.save(\"ppo_rssm_latent_policy_z_h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084337cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(callback.episode_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training reward\")\n",
    "plt.savefig(\"ppo_rssm_reward_plot_z_h.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
